#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç Flash Attention –¥–ª—è dots.ocr
–ü—Ä–æ–≤–µ—Ä—è–µ–º PyTorch SDPA Flash Attention backend
"""

import time
import torch
from PIL import Image
from models.model_loader import ModelLoader
from utils.logger import logger

def test_flash_attention():
    """–¢–µ—Å—Ç Flash Attention —Å dots.ocr"""
    print("üöÄ –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ FLASH ATTENTION")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º PyTorch SDPA Flash Attention
    print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
    
    # –¢–µ—Å—Ç PyTorch SDPA Flash Attention
    print("\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º PyTorch SDPA Flash Attention...")
    try:
        with torch.backends.cuda.sdp_kernel(enable_flash=True):
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä
            test_tensor = torch.randn(1, 1, 10, 64, device='cuda', dtype=torch.bfloat16)
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º SDPA
            start_time = time.time()
            result = torch.nn.functional.scaled_dot_product_attention(
                test_tensor, test_tensor, test_tensor
            )
            sdpa_time = time.time() - start_time
            
            print(f"‚úÖ PyTorch SDPA Flash Attention –†–ê–ë–û–¢–ê–ï–¢!")
            print(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {sdpa_time:.4f}s")
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç shape: {result.shape}")
            
    except Exception as e:
        print(f"‚ùå PyTorch SDPA Flash –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        return False
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º dots.ocr
    print("\nüì• –ó–∞–≥—Ä—É–∂–∞–µ–º dots.ocr —Å Flash Attention...")
    start_load = time.time()
    
    try:
        model = ModelLoader.load_model('dots_ocr')
        load_time = time.time() - start_load
        print(f"‚úÖ dots.ocr –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {load_time:.2f}s")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return False
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    print("\nüñºÔ∏è –¢–µ—Å—Ç–∏—Ä—É–µ–º OCR —Å Flash Attention...")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image_path = "test_document.png"
        if not os.path.exists(image_path):
            print(f"‚ö†Ô∏è –§–∞–π–ª {image_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            from PIL import ImageDraw, ImageFont
            img = Image.new('RGB', (800, 600), color='white')
            draw = ImageDraw.Draw(img)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
            try:
                font = ImageFont.truetype("arial.ttf", 24)
            except:
                font = ImageFont.load_default()
            
            draw.text((50, 50), "–¢–ï–°–¢–û–í–´–ô –î–û–ö–£–ú–ï–ù–¢", fill='black', font=font)
            draw.text((50, 100), "Flash Attention Test", fill='black', font=font)
            draw.text((50, 150), "–ù–æ–º–µ—Ä: 123456789", fill='black', font=font)
            draw.text((50, 200), "–î–∞—Ç–∞: 19.01.2026", fill='black', font=font)
            
            img.save(image_path)
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        
        image = Image.open(image_path)
        
        # OCR —Ç–µ—Å—Ç
        print("üîç –ó–∞–ø—É—Å–∫–∞–µ–º OCR...")
        start_ocr = time.time()
        
        ocr_result = model.extract_text_only(image)
        ocr_time = time.time() - start_ocr
        
        print(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {ocr_time:.2f}s")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç OCR ({len(ocr_result)} —Å–∏–º–≤–æ–ª–æ–≤):")
        print(f"   {ocr_result[:200]}...")
        
        # Layout –∞–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç
        print("\nüìã –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ layout...")
        start_layout = time.time()
        
        layout_result = model.parse_document(image, return_json=True)
        layout_time = time.time() - start_layout
        
        print(f"‚úÖ Layout –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {layout_time:.2f}s")
        
        if isinstance(layout_result, dict):
            if 'raw_text' in layout_result:
                print(f"üìä Layout —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Ç–µ–∫—Å—Ç): {len(layout_result['raw_text'])} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                print(f"üìä Layout —Ä–µ–∑—É–ª—å—Ç–∞—Ç (JSON): {len(str(layout_result))} —Å–∏–º–≤–æ–ª–æ–≤")
        else:
            print(f"üìä Layout —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {type(layout_result)}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "=" * 50)
        print("üéØ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ FLASH ATTENTION:")
        print(f"‚úÖ PyTorch SDPA Flash: –†–ê–ë–û–¢–ê–ï–¢")
        print(f"‚úÖ dots.ocr –∑–∞–≥—Ä—É–∑–∫–∞: {load_time:.2f}s")
        print(f"‚úÖ OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞: {ocr_time:.2f}s")
        print(f"‚úÖ Layout –∞–Ω–∞–ª–∏–∑: {layout_time:.2f}s")
        print(f"‚úÖ –û–±—â–µ–µ –≤—Ä–µ–º—è: {load_time + ocr_time + layout_time:.2f}s")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024**3
            print(f"‚úÖ GPU –ø–∞–º—è—Ç—å: {memory_used:.2f}GB")
        
        print("\nüöÄ FLASH ATTENTION –ü–û–õ–ù–û–°–¢–¨–Æ –§–£–ù–ö–¶–ò–û–ù–ê–õ–ï–ù!")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # –û—á–∏—Å—Ç–∫–∞
        if 'model' in locals():
            model.unload()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

if __name__ == "__main__":
    import os
    success = test_flash_attention()
    
    if success:
        print("\nüéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("Flash Attention —á–µ—Ä–µ–∑ PyTorch SDPA —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ!")
    else:
        print("\n‚ùå –¢–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã")
        exit(1)