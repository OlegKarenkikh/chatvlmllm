#!/usr/bin/env python3
"""–¢–µ—Å—Ç dots.ocr –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É."""

import sys
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import torch

# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent))


def test_dots_official():
    """–¢–µ—Å—Ç –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º—É –ø—Ä–∏–º–µ—Ä—É."""
    print("üß™ –¢–ï–°–¢ DOTS.OCR –ü–û –û–§–ò–¶–ò–ê–õ–¨–ù–û–ú–£ –ü–†–ò–ú–ï–†–£")
    print("=" * 50)
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer, AutoImageProcessor
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.new('RGB', (400, 200), color='white')
        draw = ImageDraw.Draw(image)
        draw.text((50, 50), "TEST DOCUMENT", fill='black')
        draw.text((50, 100), "Line 1: Important information", fill='black')
        draw.text((50, 130), "Line 2: More data here", fill='black')
        
        print("‚úÖ –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ")
        
        model_path = "rednote-hilab/dots.ocr"
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        try:
            image_processor = AutoImageProcessor.from_pretrained(model_path, trust_remote_code=True)
            print("‚úÖ Image processor –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ image processor: {e}")
            # –ü–æ–ø—Ä–æ–±—É–µ–º Qwen2VLImageProcessor
            from transformers import Qwen2VLImageProcessor
            image_processor = Qwen2VLImageProcessor.from_pretrained(model_path)
            print("‚úÖ Qwen2VL Image processor –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="eager"  # –û—Ç–∫–ª—é—á–∞–µ–º Flash Attention
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print("\nüîç –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü—Ä–æ–º–ø—Ç
        prompt = "Please output the layout information from the PDF image, including bbox, category, and text."
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        try:
            image_inputs = image_processor(image, return_tensors="pt")
            print(f"‚úÖ Image inputs: {list(image_inputs.keys())}")
            
            for key, value in image_inputs.items():
                if torch.is_tensor(value):
                    print(f"   {key}: {value.shape}")
                else:
                    print(f"   {key}: {type(value)}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return False
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text_inputs = tokenizer(prompt, return_tensors="pt")
        print(f"‚úÖ Text inputs: {list(text_inputs.keys())}")
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        inputs = {
            **text_inputs,
            **image_inputs
        }
        
        # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = next(model.parameters()).device
        inputs = {k: v.to(device) if torch.is_tensor(v) else v for k, v in inputs.items()}
        
        print(f"‚úÖ –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        print("\nüöÄ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è...")
        
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=1000,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!")
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç ({len(generated_text)} —Å–∏–º–≤–æ–ª–æ–≤):")
            print("-" * 40)
            print(generated_text[:500] + "..." if len(generated_text) > 500 else generated_text)
            print("-" * 40)
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_dots_official()
    sys.exit(0 if success else 1)