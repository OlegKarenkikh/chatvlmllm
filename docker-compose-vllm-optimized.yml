version: '3.8'

services:
  # DotsOCR - основная OCR модель (оптимизированная для экономии памяти)
  dots-ocr:
    image: vllm/vllm-openai:latest
    container_name: dots-ocr-memory-optimized
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:rw
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface:rw
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 6G  # Ограничение системной памяти
    shm_size: 4g  # Уменьшено для экономии
    command: >
      --model rednote-hilab/dots.ocr
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 1024
      --gpu-memory-utilization 0.4
      --dtype bfloat16
      --enforce-eager
      --disable-log-requests
      --enable-prefix-caching
      --max-num-batched-tokens 512
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - dots-ocr
      - single-model

  # Qwen3-VL 2B - улучшенная VLM модель (оптимизированная)
  qwen3-vl-2b:
    image: vllm/vllm-openai:latest
    container_name: qwen3-vl-2b-memory-optimized
    restart: unless-stopped
    ports:
      - "8004:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:rw
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface:rw
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 8G  # Ограничение системной памяти
    shm_size: 4g  # Уменьшено для экономии
    command: >
      --model Qwen/Qwen3-VL-2B-Instruct
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 2048
      --gpu-memory-utilization 0.5
      --dtype bfloat16
      --disable-log-requests
      --enable-prefix-caching
      --max-num-batched-tokens 1024
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - qwen3-vl
      - single-model

  # Qwen2-VL 2B - легкая VLM модель (экономичная версия)
  qwen2-vl-2b:
    image: vllm/vllm-openai:latest
    container_name: qwen2-vl-2b-memory-optimized
    restart: unless-stopped
    ports:
      - "8001:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:rw
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface:rw
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          memory: 7G
    shm_size: 4g
    command: >
      --model Qwen/Qwen2-VL-2B-Instruct
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 2048
      --gpu-memory-utilization 0.45
      --dtype bfloat16
      --disable-log-requests
      --enable-prefix-caching
      --max-num-batched-tokens 1024
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - qwen2-vl
      - backup-model

# Именованные тома для кеша
volumes:
  huggingface-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/huggingface

# Сети для изоляции
networks:
  vllm-network:
    driver: bridge