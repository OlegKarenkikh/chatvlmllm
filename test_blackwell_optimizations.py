#!/usr/bin/env python3
"""
–¢–ï–°–¢ BLACKWELL –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ô
"""

import torch
import time
from transformers import AutoModelForImageTextToText, AutoProcessor

def test_blackwell_optimizations():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Blackwell."""
    print("üß™ –¢–ï–°–¢ BLACKWELL –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ô")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    compute_cap = torch.cuda.get_device_capability(0)
    print(f"GPU: {gpu_name}")
    print(f"Compute Capability: {compute_cap}")
    
    # –í–∫–ª—é—á–∞–µ–º Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.enable_flash_sdp(True)
    
    print("‚úÖ Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
    
    # –¢–µ—Å—Ç bfloat16
    try:
        print("\nüîç –¢–µ—Å—Ç bfloat16 –æ–ø–µ—Ä–∞—Ü–∏–π...")
        start = time.time()
        
        x = torch.randn(1024, 1024, device='cuda', dtype=torch.bfloat16)
        y = torch.randn(1024, 1024, device='cuda', dtype=torch.bfloat16)
        
        for _ in range(100):
            z = torch.matmul(x, y)
        
        torch.cuda.synchronize()
        elapsed = time.time() - start
        
        print(f"‚úÖ bfloat16 –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: {elapsed:.3f}s")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ bfloat16: {e}")
        return False
    
    # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    try:
        print("\nüîç –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Å Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏...")
        start = time.time()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º eager attention (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å Blackwell)
        model = AutoModelForImageTextToText.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            torch_dtype=torch.bfloat16,
            attn_implementation="eager",  # –ù–ï flash_attention_2
            device_map="auto",
            trust_remote_code=True
        )
        
        load_time = time.time() - start
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º dtype –º–æ–¥–µ–ª–∏
        first_param = next(model.parameters())
        print(f"‚úÖ Dtype –º–æ–¥–µ–ª–∏: {first_param.dtype}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

if __name__ == "__main__":
    success = test_blackwell_optimizations()
    print(f"\n{'‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´' if success else '‚ùå –ï–°–¢–¨ –ü–†–û–ë–õ–ï–ú–´'}")
