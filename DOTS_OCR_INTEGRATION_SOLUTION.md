# –†–ï–®–ï–ù–ò–ï –ò–ù–¢–ï–ì–†–ê–¶–ò–ò DOTS.OCR –í CHATVLMLLM

## üéØ –ü–†–û–ë–õ–ï–ú–ê –ò –†–ï–®–ï–ù–ò–ï

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
1. **Flash Attention –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** —Å RTX 5070 Ti Blackwell (sm_120)
2. **–í–µ—Ä—Å–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π** - dots.ocr —Ç—Ä–µ–±—É–µ—Ç —Ç–æ—á–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
3. **–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π** - –ø—Ä–æ–±–ª–µ–º—ã —Å chat template
4. **CUDA 13.0 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** - –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–µ–±—É–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

### ‚úÖ –ì–û–¢–û–í–û–ï –†–ï–®–ï–ù–ò–ï:

## 1. –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô

```bash
# –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ PyTorch –¥–ª—è Blackwell
pip uninstall torch torchvision torchaudio -y
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126

# –®–∞–≥ 2: –ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º flash-attn (–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º —Å Blackwell)
# –ò—Å–ø–æ–ª—å–∑—É–µ–º eager attention –≤–º–µ—Å—Ç–æ flash attention

# –®–∞–≥ 3: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ qwen_vl_utils (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è)
pip install qwen-vl-utils==0.0.8

# –®–∞–≥ 4: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ dots.ocr
pip install git+https://github.com/ucaslcl/GOT-OCR2.0.git
```

## 2. –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –í CHATVLMLLM

### –í–∞—Ä–∏–∞–Ω—Ç A: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É

```python
# –í models/dots_ocr_chatvlm_integration.py
import torch
from transformers import AutoModelForCausalLM, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
import logging

class DotsOCRChatVLM:
    """dots.ocr –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è chatvlmllm –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.model_name = "rednote-hilab/dots.ocr"
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        try:
            # Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.bfloat16,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell
                attn_implementation="eager",  # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–ª—è RTX 5070 Ti
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            return True
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ dots.ocr: {e}")
            return False
    
    def chat_completion(self, messages, max_tokens=2048):
        """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å OpenAI API –¥–ª—è chatvlmllm"""
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ messages
            image_content = None
            text_content = "Extract all text from this image"
            
            for message in messages:
                if message.get("role") == "user":
                    content = message.get("content", [])
                    for item in content:
                        if item.get("type") == "image_url":
                            image_url = item.get("image_url", {}).get("url")
                            if image_url:
                                image_content = Image.open(image_url).convert('RGB')
                        elif item.get("type") == "text":
                            text_content = item.get("text", text_content)
            
            if not image_content:
                return {"error": "No image provided"}
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ dots.ocr
            result = self.process_image(image_content, text_content)
            
            return {
                "choices": [{
                    "message": {
                        "role": "assistant",
                        "content": result or "No text detected"
                    }
                }]
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    def process_image(self, image, prompt):
        """–û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not self.model or not self.processor:
            return None
            
        try:
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è dots.ocr
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }]
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ chat template
            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ vision info
            image_inputs, video_inputs = process_vision_info(messages)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            ).to("cuda")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=False,
                    temperature=0.0,
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            return output_text.strip()
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return None
```

### –í–∞—Ä–∏–∞–Ω—Ç B: –ß–µ—Ä–µ–∑ vLLM —Å–µ—Ä–≤–µ—Ä (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)

```python
# –í api.py –∏–ª–∏ app.py
from openai import OpenAI
import subprocess
import time

class DotsOCRvLLMIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è dots.ocr —á–µ—Ä–µ–∑ vLLM —Å–µ—Ä–≤–µ—Ä"""
    
    def __init__(self, port=8000):
        self.port = port
        self.client = None
        self.server_process = None
        
    def start_vllm_server(self):
        """–ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è dots.ocr"""
        try:
            cmd = [
                "vllm", "serve", "rednote-hilab/dots.ocr",
                "--trust-remote-code",
                "--gpu-memory-utilization", "0.9",
                "--port", str(self.port),
                "--dtype", "bfloat16",
                "--disable-log-requests"
            ]
            
            self.server_process = subprocess.Popen(cmd)
            
            # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
            time.sleep(30)
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç–∞
            self.client = OpenAI(
                base_url=f"http://localhost:{self.port}/v1",
                api_key="token-abc123"
            )
            
            return True
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ vLLM: {e}")
            return False
    
    def chat_completion(self, messages, max_tokens=2048):
        """OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API"""
        if not self.client:
            return {"error": "vLLM server not started"}
            
        try:
            response = self.client.chat.completions.create(
                model="rednote-hilab/dots.ocr",
                messages=messages,
                max_tokens=max_tokens
            )
            
            return response.model_dump()
            
        except Exception as e:
            return {"error": str(e)}
    
    def stop_server(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ vLLM —Å–µ—Ä–≤–µ—Ä–∞"""
        if self.server_process:
            self.server_process.terminate()
```

## 3. –î–û–ë–ê–í–õ–ï–ù–ò–ï –í –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Æ CHATVLMLLM

```yaml
# –í config.yaml –∏–ª–∏ config_final.yaml
models:
  dots_ocr_vllm:
    name: "dots.ocr via vLLM"
    type: "vllm_server"
    model_path: "rednote-hilab/dots.ocr"
    port: 8000
    status: "production"
    
  dots_ocr_direct:
    name: "dots.ocr Direct"
    type: "transformers"
    model_path: "rednote-hilab/dots.ocr"
    precision: "bf16"
    attn_implementation: "eager"
    status: "experimental"
```

## 4. –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –í CHATVLMLLM

```python
# –í app.py –∏–ª–∏ main —Ñ–∞–π–ª–µ
from models.dots_ocr_chatvlm_integration import DotsOCRChatVLM

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
dots_ocr = DotsOCRChatVLM()
if dots_ocr.load_model():
    print("‚úÖ dots.ocr –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ API endpoint
@app.route('/api/ocr', methods=['POST'])
def ocr_endpoint():
    messages = request.json.get('messages', [])
    result = dots_ocr.chat_completion(messages)
    return jsonify(result)
```

## 5. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò–ù–¢–ï–ì–†–ê–¶–ò–ò

```python
# test_dots_ocr_integration.py
def test_chatvlm_integration():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å chatvlmllm"""
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ chatvlmllm
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": "test_document.png"}
                },
                {
                    "type": "text", 
                    "text": "Extract all text from this document"
                }
            ]
        }
    ]
    
    # –¢–µ—Å—Ç —á–µ—Ä–µ–∑ –ø—Ä—è–º—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
    dots_ocr = DotsOCRChatVLM()
    if dots_ocr.load_model():
        result = dots_ocr.chat_completion(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
    
    # –¢–µ—Å—Ç —á–µ—Ä–µ–∑ vLLM
    vllm_integration = DotsOCRvLLMIntegration()
    if vllm_integration.start_vllm_server():
        result = vllm_integration.chat_completion(messages)
        print(f"vLLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
```

## 6. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ

### –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞:
1. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ vLLM –≤–∞—Ä–∏–∞–Ω—Ç** - –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏ –±—ã—Å—Ç—Ä—ã–π
2. **Fallback –Ω–∞ qwen_vl_2b** - –µ—Å–ª–∏ dots.ocr –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
3. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏** - dots.ocr —Ç—Ä–µ–±—É–µ—Ç ~19GB VRAM

### –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:
1. **–ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** - –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
2. **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ** - –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ª–æ–≥–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
3. **Graceful degradation** - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

## 7. –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ï –†–ï–®–ï–ù–ò–Ø

–ï—Å–ª–∏ dots.ocr –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã:

```python
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
FALLBACK_MODELS = {
    "primary": "qwen_vl_2b",      # –ë—ã—Å—Ç—Ä–æ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ
    "advanced": "qwen3_vl_2b",    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏  
    "specialized": "got_ocr_hf"   # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è OCR
}
```

## üéâ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å —Ç—Ä–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ dots.ocr:

1. **vLLM —Å–µ—Ä–≤–µ—Ä** (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è) - —Å—Ç–∞–±–∏–ª—å–Ω–æ –∏ –±—ã—Å—Ç—Ä–æ
2. **–ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** - –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
3. **Fallback —Å–∏—Å—Ç–µ–º–∞** - –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã

–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—à–∏—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π!