#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ vLLM —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ HuggingFace
"""

import os
import json
import requests
from pathlib import Path
from typing import Dict, Any, Optional

class ModelAnalyzer:
    def __init__(self):
        self.cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        self.models_info = {}
        
    def get_cached_models(self) -> Dict[str, Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üîç –ê–ù–ê–õ–ò–ó –ö–ï–®–ò–†–û–í–ê–ù–ù–´–• –ú–û–î–ï–õ–ï–ô")
        print("=" * 40)
        
        if not self.cache_dir.exists():
            print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞ HuggingFace –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return {}
        
        model_dirs = [d for d in self.cache_dir.iterdir() if d.is_dir() and d.name.startswith('models--')]
        
        models = {}
        for model_dir in model_dirs:
            model_name = model_dir.name.replace('models--', '').replace('--', '/')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
            snapshots_dir = model_dir / "snapshots"
            if not snapshots_dir.exists():
                continue
                
            snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
            if not snapshot_dirs:
                continue
            
            latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
            
            # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
            config_path = latest_snapshot / "config.json"
            if config_path.exists():
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    
                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
                    size_gb = self.get_model_size(model_dir)
                    
                    models[model_name] = {
                        'path': str(model_dir),
                        'size_gb': round(size_gb, 2),
                        'config': config,
                        'model_type': config.get('model_type', 'unknown'),
                        'architectures': config.get('architectures', []),
                        'max_position_embeddings': config.get('max_position_embeddings'),
                        'hidden_size': config.get('hidden_size'),
                        'vocab_size': config.get('vocab_size')
                    }
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–∞ {model_name}: {e}")
        
        return models
    
    def get_model_size(self, model_path: Path) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –≤ –ì–ë"""
        total_size = 0
        for root, dirs, files in os.walk(model_path):
            for file in files:
                file_path = os.path.join(root, file)
                if os.path.exists(file_path):
                    total_size += os.path.getsize(file_path)
        return total_size / (1024**3)
    
    def get_huggingface_info(self, model_name: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏ —Å HuggingFace"""
        try:
            url = f"https://huggingface.co/api/models/{model_name}"
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                return response.json()
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ {model_name}: {e}")
        return None
    
    def analyze_vllm_compatibility(self, model_name: str, config: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å vLLM"""
        compatibility = {
            'vllm_supported': False,
            'recommended_params': {},
            'issues': [],
            'category': 'unknown'
        }
        
        model_type = config.get('model_type', '').lower()
        architectures = config.get('architectures', [])
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–æ–¥–µ–ª–∏
        if any(keyword in model_name.lower() for keyword in ['ocr', 'got', 'dots']):
            compatibility['category'] = 'ocr'
        elif any(keyword in model_name.lower() for keyword in ['vision', 'vlm', 'qwen', 'phi']):
            compatibility['category'] = 'vlm'
        else:
            compatibility['category'] = 'other'
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ vLLM
        supported_architectures = [
            'qwen2vlforconditionalgeneration',
            'qwen3vlforconditionalgeneration', 
            'qwen2_5_vlforconditionalgeneration',
            'phi3vforcausallm',
            'dotsocrcausallm',
            'gotqwenforcausallm',
            'gotocrforconditionalgeneration',
            'deepseekocrcausallm'
        ]
        
        arch_lower = [arch.lower() for arch in architectures]
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è dots.ocr (–∏–∑–≤–µ—Å—Ç–Ω–æ —á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)
        if 'dots.ocr' in model_name:
            compatibility['vllm_supported'] = True
        elif any(arch in supported_architectures for arch in arch_lower):
            compatibility['vllm_supported'] = True
        
        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        max_pos = config.get('max_position_embeddings', 2048)
        
        if compatibility['category'] == 'ocr':
            compatibility['recommended_params'] = {
                'max_model_len': min(max_pos, 2048),
                'gpu_memory_utilization': 0.8,
                'trust_remote_code': True,
                'enforce_eager': True,
                'port_offset': 0
            }
        elif compatibility['category'] == 'vlm':
            compatibility['recommended_params'] = {
                'max_model_len': min(max_pos, 4096),
                'gpu_memory_utilization': 0.7,
                'trust_remote_code': True,
                'enforce_eager': False,
                'port_offset': 10
            }
        else:
            compatibility['recommended_params'] = {
                'max_model_len': min(max_pos, 2048),
                'gpu_memory_utilization': 0.6,
                'trust_remote_code': True,
                'enforce_eager': True,
                'port_offset': 20
            }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
        if config.get('vocab_size', 0) > 100000:
            compatibility['issues'].append('Large vocabulary size may require more memory')
        
        if config.get('hidden_size', 0) > 4096:
            compatibility['issues'].append('Large hidden size may require more GPU memory')
        
        return compatibility
    
    def generate_vllm_configs(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è vLLM"""
        print("\nüìù –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô VLLM")
        print("=" * 40)
        
        models = self.get_cached_models()
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        priority_models = [
            'rednote-hilab/dots.ocr',
            'stepfun-ai/GOT-OCR2_0', 
            'ucaslcl/GOT-OCR2_0',
            'stepfun-ai/GOT-OCR-2.0-hf',
            'deepseek-ai/deepseek-ocr',
            'Qwen/Qwen3-VL-2B-Instruct',
            'Qwen/Qwen2-VL-2B-Instruct',
            'Qwen/Qwen2-VL-7B-Instruct',
            'Qwen/Qwen2.5-VL-7B-Instruct',
            'microsoft/Phi-3.5-vision-instruct'
        ]
        
        configs = {}
        port = 8000
        
        for model_name in priority_models:
            if model_name in models:
                model_info = models[model_name]
                compatibility = self.analyze_vllm_compatibility(model_name, model_info['config'])
                
                if compatibility['vllm_supported']:
                    config = {
                        'model_name': model_name,
                        'container_name': model_name.replace('/', '-').replace('.', '-').lower() + '-vllm',
                        'port': port,
                        'size_gb': model_info['size_gb'],
                        'category': compatibility['category'],
                        'vllm_params': compatibility['recommended_params'],
                        'issues': compatibility['issues'],
                        'priority': self.get_model_priority(model_name, compatibility['category'])
                    }
                    
                    configs[model_name] = config
                    port += 1
                    
                    print(f"‚úÖ {model_name}")
                    print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {compatibility['category']}")
                    print(f"   –†–∞–∑–º–µ—Ä: {model_info['size_gb']} –ì–ë")
                    print(f"   –ü–æ—Ä—Ç: {port-1}")
                    if compatibility['issues']:
                        print(f"   –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(compatibility['issues'])}")
                    print()
                else:
                    print(f"‚ùå {model_name} - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è vLLM")
            else:
                print(f"‚ö†Ô∏è {model_name} - –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ")
        
        return configs
    
    def get_model_priority(self, model_name: str, category: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –º–æ–¥–µ–ª–∏"""
        if 'dots.ocr' in model_name:
            return 1  # –í—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        elif category == 'ocr':
            return 2  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è OCR
        elif 'Qwen3' in model_name:
            return 3  # –ù–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        elif category == 'vlm':
            return 4  # VLM –º–æ–¥–µ–ª–∏
        else:
            return 5  # –û—Å—Ç–∞–ª—å–Ω—ã–µ
    
    def save_configs(self, configs: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
        with open('vllm_models_config.json', 'w', encoding='utf-8') as f:
            json.dump(configs, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ vllm_models_config.json")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        with open('models_summary.txt', 'w', encoding='utf-8') as f:
            f.write("–ê–ù–ê–õ–ò–ó –ú–û–î–ï–õ–ï–ô –î–õ–Ø VLLM\n")
            f.write("=" * 30 + "\n\n")
            
            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            categories = {}
            for model_name, config in configs.items():
                cat = config['category']
                if cat not in categories:
                    categories[cat] = []
                categories[cat].append((model_name, config))
            
            for category, models in categories.items():
                f.write(f"{category.upper()} –ú–û–î–ï–õ–ò:\n")
                f.write("-" * 20 + "\n")
                
                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
                models.sort(key=lambda x: x[1]['priority'])
                
                for model_name, config in models:
                    f.write(f"‚Ä¢ {model_name}\n")
                    f.write(f"  –†–∞–∑–º–µ—Ä: {config['size_gb']} –ì–ë\n")
                    f.write(f"  –ü–æ—Ä—Ç: {config['port']}\n")
                    f.write(f"  Max tokens: {config['vllm_params']['max_model_len']}\n")
                    if config['issues']:
                        f.write(f"  –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(config['issues'])}\n")
                    f.write("\n")
                f.write("\n")
        
        print(f"üìÑ –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ models_summary.txt")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    analyzer = ModelAnalyzer()
    configs = analyzer.generate_vllm_configs()
    
    if configs:
        analyzer.save_configs(configs)
        print(f"\n‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(configs)} –º–æ–¥–µ–ª–µ–π –¥–ª—è vLLM")
    else:
        print("\n‚ùå –ü–æ–¥—Ö–æ–¥—è—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

if __name__ == "__main__":
    main()