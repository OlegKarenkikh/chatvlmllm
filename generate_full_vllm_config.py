#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ vLLM –¥–ª—è –≤—Å–µ—Ö –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""

import json
import os
from pathlib import Path

def generate_full_config():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    
    # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    models_config = {
        # OCR –º–æ–¥–µ–ª–∏ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        "rednote-hilab/dots.ocr": {
            "model_name": "rednote-hilab/dots.ocr",
            "container_name": "dots-ocr-fixed",
            "port": 8000,
            "size_gb": 5.67,
            "category": "ocr",
            "vllm_params": {
                "max_model_len": 1024,
                "gpu_memory_utilization": 0.85,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": [],
            "priority": 1,
            "status": "tested_working"
        },
        
        "deepseek-ai/deepseek-ocr": {
            "model_name": "deepseek-ai/deepseek-ocr",
            "container_name": "deepseek-ocr-vllm",
            "port": 8001,
            "size_gb": 0.01,
            "category": "ocr",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.6,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": ["Very small size - may be incomplete"],
            "priority": 2,
            "status": "needs_testing"
        },
        
        "stepfun-ai/GOT-OCR-2.0-hf": {
            "model_name": "stepfun-ai/GOT-OCR-2.0-hf",
            "container_name": "got-ocr-2-0-hf-vllm",
            "port": 8002,
            "size_gb": 1.06,
            "category": "ocr",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.7,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": ["May require additional dependencies"],
            "priority": 2,
            "status": "needs_testing"
        },
        
        "stepfun-ai/GOT-OCR2_0": {
            "model_name": "stepfun-ai/GOT-OCR2_0",
            "container_name": "stepfun-got-ocr2-0-vllm",
            "port": 8003,
            "size_gb": 1.34,
            "category": "ocr",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.7,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": ["Requires verovio package"],
            "priority": 2,
            "status": "known_incompatible"
        },
        
        "ucaslcl/GOT-OCR2_0": {
            "model_name": "ucaslcl/GOT-OCR2_0",
            "container_name": "ucaslcl-got-ocr2-0-vllm",
            "port": 8004,
            "size_gb": 2.67,
            "category": "ocr",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.7,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": ["Requires verovio package"],
            "priority": 2,
            "status": "known_incompatible"
        },
        
        # VLM –º–æ–¥–µ–ª–∏ (—Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        "Qwen/Qwen3-VL-2B-Instruct": {
            "model_name": "Qwen/Qwen3-VL-2B-Instruct",
            "container_name": "qwen3-vl-2b-instruct-vllm",
            "port": 8010,
            "size_gb": 3.97,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.7,
                "trust_remote_code": True,
                "enforce_eager": False
            },
            "issues": [],
            "priority": 3,
            "status": "tested_working"
        },
        
        "Qwen/Qwen2-VL-2B-Instruct": {
            "model_name": "Qwen/Qwen2-VL-2B-Instruct",
            "container_name": "qwen2-vl-2b-instruct-vllm",
            "port": 8011,
            "size_gb": 4.13,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.7,
                "trust_remote_code": True,
                "enforce_eager": False
            },
            "issues": [],
            "priority": 3,
            "status": "needs_testing"
        },
        
        "Qwen/Qwen2.5-VL-7B-Instruct": {
            "model_name": "Qwen/Qwen2.5-VL-7B-Instruct",
            "container_name": "qwen2-5-vl-7b-instruct-vllm",
            "port": 8012,
            "size_gb": 0.66,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.7,
                "trust_remote_code": True,
                "enforce_eager": False
            },
            "issues": ["Small size - may be incomplete"],
            "priority": 3,
            "status": "needs_testing"
        },
        
        "Qwen/Qwen2-VL-7B-Instruct": {
            "model_name": "Qwen/Qwen2-VL-7B-Instruct",
            "container_name": "qwen2-vl-7b-instruct-vllm",
            "port": 8013,
            "size_gb": 7.61,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.6,
                "trust_remote_code": True,
                "enforce_eager": False
            },
            "issues": ["Large model - high memory usage"],
            "priority": 4,
            "status": "needs_testing"
        },
        
        "microsoft/Phi-3.5-vision-instruct": {
            "model_name": "microsoft/Phi-3.5-vision-instruct",
            "container_name": "phi-3-5-vision-instruct-vllm",
            "port": 8014,
            "size_gb": 7.73,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.6,
                "trust_remote_code": True,
                "enforce_eager": False
            },
            "issues": ["Large model - high memory usage"],
            "priority": 4,
            "status": "needs_testing"
        },
        
        "datalab-to/chandra": {
            "model_name": "datalab-to/chandra",
            "container_name": "datalab-chandra-vllm",
            "port": 8015,
            "size_gb": 0.42,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.6,
                "trust_remote_code": True,
                "enforce_eager": False
            },
            "issues": [],
            "priority": 3,
            "status": "needs_testing"
        },
        
        # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        "deepseek-ai/deepseek-vl-1.3b-chat": {
            "model_name": "deepseek-ai/deepseek-vl-1.3b-chat",
            "container_name": "deepseek-vl-1-3b-chat-vllm",
            "port": 8020,
            "size_gb": 0.0,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.5,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": ["Zero size - likely incomplete download"],
            "priority": 5,
            "status": "likely_broken"
        },
        
        "h2oai/h2ovl-mississippi-2b": {
            "model_name": "h2oai/h2ovl-mississippi-2b",
            "container_name": "h2ovl-mississippi-2b-vllm",
            "port": 8021,
            "size_gb": 4.01,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.6,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": ["Custom architecture - may not be supported"],
            "priority": 5,
            "status": "needs_testing"
        },
        
        "h2oai/h2ovl-mississippi-800m": {
            "model_name": "h2oai/h2ovl-mississippi-800m",
            "container_name": "h2ovl-mississippi-800m-vllm",
            "port": 8022,
            "size_gb": 1.54,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.5,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": ["Custom architecture - may not be supported"],
            "priority": 5,
            "status": "needs_testing"
        },
        
        "vikhyatk/moondream2": {
            "model_name": "vikhyatk/moondream2",
            "container_name": "moondream2-vllm",
            "port": 8023,
            "size_gb": 3.59,
            "category": "vlm",
            "vllm_params": {
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.6,
                "trust_remote_code": True,
                "enforce_eager": True
            },
            "issues": ["Custom architecture - may not be supported"],
            "priority": 5,
            "status": "needs_testing"
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    with open('full_vllm_models_config.json', 'w', encoding='utf-8') as f:
        json.dump(models_config, f, ensure_ascii=False, indent=2)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏
    summary = {
        "total_models": len(models_config),
        "by_category": {},
        "by_status": {},
        "by_priority": {},
        "total_size_gb": 0
    }
    
    for model_name, config in models_config.items():
        # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category = config["category"]
        if category not in summary["by_category"]:
            summary["by_category"][category] = 0
        summary["by_category"][category] += 1
        
        # –ü–æ —Å—Ç–∞—Ç—É—Å—É
        status = config["status"]
        if status not in summary["by_status"]:
            summary["by_status"][status] = 0
        summary["by_status"][status] += 1
        
        # –ü–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        priority = config["priority"]
        if priority not in summary["by_priority"]:
            summary["by_priority"][priority] = 0
        summary["by_priority"][priority] += 1
        
        # –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä
        summary["total_size_gb"] += config["size_gb"]
    
    summary["total_size_gb"] = round(summary["total_size_gb"], 2)
    
    with open('full_vllm_models_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("üìä –ü–û–õ–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø vLLM –°–û–ó–î–ê–ù–ê")
    print("=" * 40)
    print(f"–í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {summary['total_models']}")
    print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {summary['total_size_gb']} –ì–ë")
    print()
    
    print("üìÇ –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for category, count in summary["by_category"].items():
        print(f"   {category}: {count} –º–æ–¥–µ–ª–µ–π")
    
    print()
    print("üîç –ü–æ —Å—Ç–∞—Ç—É—Å—É:")
    for status, count in summary["by_status"].items():
        print(f"   {status}: {count} –º–æ–¥–µ–ª–µ–π")
    
    print()
    print("‚≠ê –ü–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É:")
    for priority in sorted(summary["by_priority"].keys()):
        count = summary["by_priority"][priority]
        print(f"   –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç {priority}: {count} –º–æ–¥–µ–ª–µ–π")
    
    print()
    print("üíæ –§–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã:")
    print("   ‚Ä¢ full_vllm_models_config.json - –ø–æ–ª–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
    print("   ‚Ä¢ full_vllm_models_summary.json - —Å–≤–æ–¥–∫–∞")
    
    return models_config, summary

if __name__ == "__main__":
    generate_full_config()