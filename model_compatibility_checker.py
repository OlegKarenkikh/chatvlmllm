#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å Transformers –∏ vLLM
–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∂–∏–º–∞
"""

import json
import requests
from pathlib import Path

class ModelCompatibilityChecker:
    def __init__(self):
        # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.compatibility_matrix = {
            # OCR –º–æ–¥–µ–ª–∏
            "rednote-hilab/dots.ocr": {
                "transformers": {
                    "supported": True,
                    "architecture": "DotsOCRForCausalLM", 
                    "memory_8bit_gb": 3.5,
                    "memory_fp16_gb": 5.67,
                    "issues": [],
                    "tested": True
                },
                "vllm": {
                    "supported": True,
                    "architecture": "DotsOCRForCausalLM",
                    "memory_required_gb": 8.0,
                    "max_model_len": 2048,
                    "issues": [],
                    "tested": True
                }
            },
            
            "stepfun-ai/GOT-OCR-2.0-hf": {
                "transformers": {
                    "supported": True,
                    "architecture": "GOTQwenForCausalLM",
                    "memory_8bit_gb": 0.8,
                    "memory_fp16_gb": 1.06,
                    "issues": ["Requires specific prompt format"],
                    "tested": False
                },
                "vllm": {
                    "supported": False,  # vLLM –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç GOT-OCR –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
                    "architecture": "GOTQwenForCausalLM",
                    "memory_required_gb": 3.0,
                    "max_model_len": 2048,
                    "issues": ["Custom architecture not supported by vLLM"],
                    "tested": False
                }
            },
            
            # VLM –º–æ–¥–µ–ª–∏
            "Qwen/Qwen2-VL-2B-Instruct": {
                "transformers": {
                    "supported": True,
                    "architecture": "Qwen2VLForConditionalGeneration",
                    "memory_8bit_gb": 2.5,
                    "memory_fp16_gb": 4.13,
                    "issues": [],
                    "tested": False
                },
                "vllm": {
                    "supported": True,
                    "architecture": "Qwen2VLForConditionalGeneration", 
                    "memory_required_gb": 6.0,
                    "max_model_len": 4096,
                    "issues": [],
                    "tested": False
                }
            },
            
            "Qwen/Qwen2-VL-7B-Instruct": {
                "transformers": {
                    "supported": True,
                    "architecture": "Qwen2VLForConditionalGeneration",
                    "memory_8bit_gb": 4.5,
                    "memory_fp16_gb": 7.61,
                    "issues": ["Large model - slow on limited hardware"],
                    "tested": False
                },
                "vllm": {
                    "supported": True,
                    "architecture": "Qwen2VLForConditionalGeneration",
                    "memory_required_gb": 12.0,
                    "max_model_len": 4096,
                    "issues": ["Requires high-end GPU"],
                    "tested": False
                }
            },
            
            "microsoft/Phi-3.5-vision-instruct": {
                "transformers": {
                    "supported": True,
                    "architecture": "Phi3VForCausalLM",
                    "memory_8bit_gb": 4.5,
                    "memory_fp16_gb": 7.73,
                    "issues": [],
                    "tested": False
                },
                "vllm": {
                    "supported": True,
                    "architecture": "Phi3VForCausalLM",
                    "memory_required_gb": 10.0,
                    "max_model_len": 4096,
                    "issues": ["May require specific vLLM version"],
                    "tested": False
                }
            },
            
            "vikhyatk/moondream2": {
                "transformers": {
                    "supported": True,
                    "architecture": "MoondreamForConditionalGeneration",
                    "memory_8bit_gb": 2.0,
                    "memory_fp16_gb": 3.59,
                    "issues": ["Custom architecture - may need special handling"],
                    "tested": False
                },
                "vllm": {
                    "supported": False,  # vLLM –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Moondream –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
                    "architecture": "MoondreamForConditionalGeneration",
                    "memory_required_gb": 5.0,
                    "max_model_len": 2048,
                    "issues": ["Custom architecture not supported by vLLM"],
                    "tested": False
                }
            },
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è Transformers
            "microsoft/Phi-3-vision-128k-instruct": {
                "transformers": {
                    "supported": True,
                    "architecture": "Phi3VForCausalLM",
                    "memory_8bit_gb": 4.0,
                    "memory_fp16_gb": 7.0,
                    "issues": [],
                    "tested": False
                },
                "vllm": {
                    "supported": False,  # –î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–µ–Ω
                    "architecture": "Phi3VForCausalLM",
                    "memory_required_gb": 12.0,
                    "max_model_len": 8192,
                    "issues": ["Long context may cause memory issues"],
                    "tested": False
                }
            },
            
            "OpenGVLab/InternVL2-2B": {
                "transformers": {
                    "supported": True,
                    "architecture": "InternVLChatModel",
                    "memory_8bit_gb": 2.0,
                    "memory_fp16_gb": 3.8,
                    "issues": ["May require specific transformers version"],
                    "tested": False
                },
                "vllm": {
                    "supported": False,  # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
                    "architecture": "InternVLChatModel",
                    "memory_required_gb": 5.0,
                    "max_model_len": 2048,
                    "issues": ["Custom architecture not supported by vLLM"],
                    "tested": False
                }
            }
        }
    
    def get_transformers_compatible_models(self):
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å Transformers"""
        compatible = {}
        for model_name, compat in self.compatibility_matrix.items():
            if compat["transformers"]["supported"]:
                compatible[model_name] = {
                    "name": self._get_display_name(model_name),
                    "type": self._get_model_type(model_name),
                    "architecture": compat["transformers"]["architecture"],
                    "memory_8bit_gb": compat["transformers"]["memory_8bit_gb"],
                    "memory_fp16_gb": compat["transformers"]["memory_fp16_gb"],
                    "max_memory_gb": compat["transformers"]["memory_fp16_gb"] + 1.0,
                    "default_prompt": self._get_default_prompt(model_name),
                    "issues": compat["transformers"]["issues"],
                    "tested": compat["transformers"]["tested"],
                    "priority": self._get_priority(model_name)
                }
        return compatible
    
    def get_vllm_compatible_models(self):
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å vLLM"""
        compatible = {}
        for model_name, compat in self.compatibility_matrix.items():
            if compat["vllm"]["supported"]:
                compatible[model_name] = {
                    "name": self._get_display_name(model_name),
                    "type": self._get_model_type(model_name),
                    "architecture": compat["vllm"]["architecture"],
                    "container_name": self._get_container_name(model_name),
                    "port": self._get_port(model_name),
                    "size_gb": compat["transformers"]["memory_fp16_gb"],
                    "memory_required_gb": compat["vllm"]["memory_required_gb"],
                    "vllm_params": self._get_vllm_params(model_name, compat["vllm"]),
                    "issues": compat["vllm"]["issues"],
                    "tested": compat["vllm"]["tested"],
                    "priority": self._get_priority(model_name)
                }
        return compatible
    
    def _get_display_name(self, model_name):
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏"""
        name_map = {
            "rednote-hilab/dots.ocr": "DotsOCR",
            "stepfun-ai/GOT-OCR-2.0-hf": "GOT-OCR 2.0",
            "Qwen/Qwen2-VL-2B-Instruct": "Qwen2-VL 2B",
            "Qwen/Qwen2-VL-7B-Instruct": "Qwen2-VL 7B",
            "microsoft/Phi-3.5-vision-instruct": "Phi-3.5 Vision",
            "microsoft/Phi-3-vision-128k-instruct": "Phi-3 Vision 128K",
            "vikhyatk/moondream2": "Moondream2",
            "OpenGVLab/InternVL2-2B": "InternVL2 2B"
        }
        return name_map.get(model_name, model_name.split('/')[-1])
    
    def _get_model_type(self, model_name):
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –º–æ–¥–µ–ª–∏"""
        if "ocr" in model_name.lower() or "dots" in model_name.lower():
            return "ocr"
        else:
            return "vlm"
    
    def _get_container_name(self, model_name):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –¥–ª—è vLLM"""
        name = model_name.replace('/', '-').replace('.', '-').lower()
        return f"{name}-vllm"
    
    def _get_port(self, model_name):
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Ä—Ç –¥–ª—è –º–æ–¥–µ–ª–∏"""
        port_map = {
            "rednote-hilab/dots.ocr": 8000,
            "Qwen/Qwen2-VL-2B-Instruct": 8001,
            "stepfun-ai/GOT-OCR-2.0-hf": 8002,
            "Qwen/Qwen2-VL-7B-Instruct": 8003,
            "microsoft/Phi-3.5-vision-instruct": 8004,
            "microsoft/Phi-3-vision-128k-instruct": 8005,
            "vikhyatk/moondream2": 8006,
            "OpenGVLab/InternVL2-2B": 8007
        }
        return port_map.get(model_name, 8000)
    
    def _get_default_prompt(self, model_name):
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        if "ocr" in model_name.lower() or "dots" in model_name.lower():
            if "got" in model_name.lower():
                return "OCR:"
            else:
                return "Extract all text from this image"
        else:
            return "Describe what you see in this image"
    
    def _get_priority(self, model_name):
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –º–æ–¥–µ–ª–∏"""
        priority_map = {
            "rednote-hilab/dots.ocr": 1,
            "Qwen/Qwen2-VL-2B-Instruct": 2,
            "stepfun-ai/GOT-OCR-2.0-hf": 3,
            "microsoft/Phi-3.5-vision-instruct": 4,
            "Qwen/Qwen2-VL-7B-Instruct": 5,
            "vikhyatk/moondream2": 6,
            "microsoft/Phi-3-vision-128k-instruct": 7,
            "OpenGVLab/InternVL2-2B": 8
        }
        return priority_map.get(model_name, 9)
    
    def _get_vllm_params(self, model_name, vllm_compat):
        """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã vLLM –¥–ª—è –º–æ–¥–µ–ª–∏"""
        base_params = {
            "trust_remote_code": True,
            "dtype": "bfloat16",
            "disable_log_requests": True
        }
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        if "dots" in model_name.lower():
            base_params.update({
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.7,
                "enforce_eager": True
            })
        elif "qwen2-vl-2b" in model_name.lower():
            base_params.update({
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.6,
                "enforce_eager": False
            })
        elif "qwen2-vl-7b" in model_name.lower():
            base_params.update({
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5,
                "enforce_eager": False
            })
        elif "phi-3" in model_name.lower():
            base_params.update({
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.6,
                "enforce_eager": True
            })
        else:
            base_params.update({
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.6,
                "enforce_eager": True
            })
        
        return base_params
    
    def generate_corrected_configs(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        transformers_models = self.get_transformers_compatible_models()
        vllm_models = self.get_vllm_compatible_models()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è integrated_model_launcher.py
        integrated_config = {
            "transformers": transformers_models,
            "vllm": vllm_models
        }
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è vLLM (working_models_config.json)
        vllm_only_config = vllm_models
        
        return integrated_config, vllm_only_config, transformers_models
    
    def save_corrected_configs(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
        integrated_config, vllm_config, transformers_config = self.generate_corrected_configs()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è vLLM
        with open("corrected_vllm_models_config.json", "w", encoding="utf-8") as f:
            json.dump(vllm_config, f, indent=2, ensure_ascii=False)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è Transformers
        with open("corrected_transformers_models_config.json", "w", encoding="utf-8") as f:
            json.dump(transformers_config, f, indent=2, ensure_ascii=False)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        with open("corrected_integrated_models_config.json", "w", encoding="utf-8") as f:
            json.dump(integrated_config, f, indent=2, ensure_ascii=False)
        
        return integrated_config, vllm_config, transformers_config
    
    def print_compatibility_report(self):
        """–í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        print("üîç –û–¢–ß–ï–¢ –û –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò –ú–û–î–ï–õ–ï–ô")
        print("=" * 50)
        
        transformers_models = self.get_transformers_compatible_models()
        vllm_models = self.get_vllm_compatible_models()
        
        print(f"\nü§ñ TRANSFORMERS –†–ï–ñ–ò–ú ({len(transformers_models)} –º–æ–¥–µ–ª–µ–π):")
        print("-" * 30)
        for model_name, config in sorted(transformers_models.items(), key=lambda x: x[1]['priority']):
            status = "‚úÖ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ" if config['tested'] else "‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
            issues = f" ({', '.join(config['issues'])})" if config['issues'] else ""
            print(f"{config['priority']}. {config['name']} - {config['memory_8bit_gb']} GB - {status}{issues}")
        
        print(f"\nüöÄ vLLM –†–ï–ñ–ò–ú ({len(vllm_models)} –º–æ–¥–µ–ª–µ–π):")
        print("-" * 30)
        for model_name, config in sorted(vllm_models.items(), key=lambda x: x[1]['priority']):
            status = "‚úÖ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ" if config['tested'] else "‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
            issues = f" ({', '.join(config['issues'])})" if config['issues'] else ""
            print(f"{config['priority']}. {config['name']} - {config['memory_required_gb']} GB - {status}{issues}")
        
        print(f"\n‚ùå –ù–ï–°–û–í–ú–ï–°–¢–ò–ú–´–ï –° vLLM:")
        print("-" * 30)
        for model_name, compat in self.compatibility_matrix.items():
            if not compat["vllm"]["supported"]:
                display_name = self._get_display_name(model_name)
                issues = ', '.join(compat["vllm"]["issues"])
                print(f"‚Ä¢ {display_name}: {issues}")
        
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print("-" * 15)
        print("‚Ä¢ –î–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π GPU –ø–∞–º—è—Ç–∏ (< 6 GB): –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Transformers —Ä–µ–∂–∏–º")
        print("‚Ä¢ –î–ª—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (> 8 GB): –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ vLLM —Ä–µ–∂–∏–º")
        print("‚Ä¢ GOT-OCR –∏ Moondream2: –¢–æ–ª—å–∫–æ Transformers —Ä–µ–∂–∏–º")
        print("‚Ä¢ DotsOCR: –õ—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è OCR –≤ –æ–±–æ–∏—Ö —Ä–µ–∂–∏–º–∞—Ö")
        print("‚Ä¢ Qwen2-VL-2B: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è VLM –¥–ª—è –æ–±–æ–∏—Ö —Ä–µ–∂–∏–º–æ–≤")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    checker = ModelCompatibilityChecker()
    
    print("üîß –ü–†–û–í–ï–†–ö–ê –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò –ú–û–î–ï–õ–ï–ô")
    print("=" * 40)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
    print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π...")
    integrated_config, vllm_config, transformers_config = checker.save_corrected_configs()
    
    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print("   ‚Ä¢ corrected_vllm_models_config.json")
    print("   ‚Ä¢ corrected_transformers_models_config.json") 
    print("   ‚Ä¢ corrected_integrated_models_config.json")
    
    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    checker.print_compatibility_report()
    
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   ‚Ä¢ Transformers —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö: {len(transformers_config)}")
    print(f"   ‚Ä¢ vLLM —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö: {len(vllm_config)}")
    print(f"   ‚Ä¢ –û–±—â–∏–π –ø—É–ª –º–æ–¥–µ–ª–µ–π: {len(checker.compatibility_matrix)}")

if __name__ == "__main__":
    main()