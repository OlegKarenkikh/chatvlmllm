#!/usr/bin/env python3
"""
–ú–Ω–æ–≥–æ–º–æ–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –Ω–∞ –±–∞–∑–µ Transformers —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
"""

import torch
import base64
import io
import json
import time
import threading
from pathlib import Path
from PIL import Image
from transformers import AutoModelForCausalLM, AutoProcessor
from flask import Flask, request, jsonify
import gc

class MultiModelTransformersServer:
    def __init__(self):
        self.models = {}  # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.processors = {}  # –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.supported_models = {
            "rednote-hilab/dots.ocr": {
                "name": "DotsOCR",
                "type": "ocr",
                "memory_8bit_gb": 3.5,
                "max_memory_gb": 6.0,
                "default_prompt": "Extract all text from this image"
            },
            "stepfun-ai/GOT-OCR-2.0-hf": {
                "name": "GOT-OCR 2.0",
                "type": "ocr", 
                "memory_8bit_gb": 0.8,
                "max_memory_gb": 2.0,
                "default_prompt": "OCR:"
            },
            "Qwen/Qwen2-VL-2B-Instruct": {
                "name": "Qwen2-VL 2B",
                "type": "vlm",
                "memory_8bit_gb": 2.5,
                "max_memory_gb": 4.0,
                "default_prompt": "Describe what you see in this image"
            },
            "microsoft/Phi-3.5-vision-instruct": {
                "name": "Phi-3.5 Vision",
                "type": "vlm",
                "memory_8bit_gb": 4.5,
                "max_memory_gb": 8.0,
                "default_prompt": "What is in this image?"
            }
        }
        
        self.loading_status = {}  # –°—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
        
    def get_gpu_memory_info(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU –ø–∞–º—è—Ç–∏"""
        if not torch.cuda.is_available():
            return {"available": False}
        
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        cached = torch.cuda.memory_reserved(0) / 1024**3
        free = total - cached
        
        return {
            "available": True,
            "total_gb": total,
            "allocated_gb": allocated,
            "cached_gb": cached,
            "free_gb": free
        }
    
    def can_load_model(self, model_name):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        if model_name not in self.supported_models:
            return False, "–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"
        
        model_config = self.supported_models[model_name]
        gpu_info = self.get_gpu_memory_info()
        
        if not gpu_info["available"]:
            return True, "CPU —Ä–µ–∂–∏–º"  # –ú–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ CPU
        
        required_memory = model_config["memory_8bit_gb"]
        available_memory = gpu_info["free_gb"]
        
        if available_memory < required_memory:
            return False, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏: –Ω—É–∂–Ω–æ {required_memory:.1f} GB, –¥–æ—Å—Ç—É–ø–Ω–æ {available_memory:.1f} GB"
        
        return True, f"–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏: {available_memory:.1f} GB –¥–æ—Å—Ç—É–ø–Ω–æ"
    
    def load_model(self, model_name):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        if model_name in self.models:
            return True, "–ú–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        
        can_load, reason = self.can_load_model(model_name)
        if not can_load:
            return False, reason
        
        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        self.loading_status[model_name] = "loading"
        
        try:
            model_config = self.supported_models[model_name]
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            print(f"üìù –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–ª—è {model_config['name']}...")
            processor = AutoProcessor.from_pretrained(
                model_name,
                trust_remote_code=True,
                cache_dir=self.cache_dir
            )
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            gpu_info = self.get_gpu_memory_info()
            load_params = {
                "trust_remote_code": True,
                "cache_dir": self.cache_dir,
                "low_cpu_mem_usage": True
            }
            
            if gpu_info["available"] and gpu_info["free_gb"] >= model_config["memory_8bit_gb"]:
                # GPU –∑–∞–≥—Ä—É–∑–∫–∞ —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
                print(f"üéÆ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ GPU —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π...")
                load_params.update({
                    "torch_dtype": torch.bfloat16,
                    "device_map": "auto",
                    "load_in_8bit": True,
                    "max_memory": {0: f"{model_config['max_memory_gb']}GB"}
                })
            else:
                # CPU –∑–∞–≥—Ä—É–∑–∫–∞
                print(f"üíª –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU...")
                load_params.update({
                    "torch_dtype": torch.float32,
                    "device_map": "cpu"
                })
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            print(f"üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_config['name']}...")
            model = AutoModelForCausalLM.from_pretrained(model_name, **load_params)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à–µ
            self.models[model_name] = model
            self.processors[model_name] = processor
            self.loading_status[model_name] = "loaded"
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            if gpu_info["available"]:
                new_gpu_info = self.get_gpu_memory_info()
                memory_used = new_gpu_info["allocated_gb"] - gpu_info["allocated_gb"]
                print(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ GPU –ø–∞–º—è—Ç–∏: {memory_used:.2f} GB")
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_config['name']} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True, f"–ú–æ–¥–µ–ª—å {model_config['name']} –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
            
        except Exception as e:
            self.loading_status[model_name] = "error"
            error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_name}: {str(e)}"
            print(f"‚ùå {error_msg}")
            return False, error_msg
    
    def unload_model(self, model_name):
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏"""
        if model_name not in self.models:
            return False, "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        
        try:
            print(f"üóëÔ∏è –í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –ø–∞–º—è—Ç–∏
            del self.models[model_name]
            del self.processors[model_name]
            
            if model_name in self.loading_status:
                del self.loading_status[model_name]
            
            # –û—á–∏—Å—Ç–∫–∞ GPU –∫–µ—à–∞
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
            gc.collect()
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
            return True, f"–ú–æ–¥–µ–ª—å {model_name} –≤—ã–≥—Ä—É–∂–µ–Ω–∞"
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ {model_name}: {str(e)}"
            print(f"‚ùå {error_msg}")
            return False, error_msg
    
    def process_image(self, model_name, image_data, prompt=None):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
        if model_name not in self.models:
            return {"error": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}
        
        if not prompt:
            prompt = self.supported_models[model_name]["default_prompt"]
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if image_data.startswith('data:image'):
                image_data = image_data.split(',')[1]
            
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes))
            
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            print(f"üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image.size} –º–æ–¥–µ–ª—å—é {model_name}")
            
            model = self.models[model_name]
            processor = self.processors[model_name]
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image", "image": image}
                    ]
                }
            ]
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
            text = processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–∞
            image_inputs, video_inputs = processor.process_vision_info(messages)
            inputs = processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            
            # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs = inputs.to(self.device)
            
            print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
            start_time = time.time()
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    do_sample=False,
                    temperature=0.1,
                    pad_token_id=processor.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            generation_time = time.time() - start_time
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {generation_time:.1f} —Å–µ–∫")
            
            return {
                "success": True,
                "text": output_text.strip(),
                "model": model_name,
                "generation_time": generation_time,
                "method": "transformers_8bit"
            }
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            print(f"‚ùå {error_msg}")
            return {"error": error_msg}
    
    def get_loaded_models(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return [
            {
                "id": model_name,
                "name": self.supported_models[model_name]["name"],
                "type": self.supported_models[model_name]["type"],
                "status": "loaded"
            }
            for model_name in self.models.keys()
        ]
    
    def get_available_models(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        models = []
        for model_name, config in self.supported_models.items():
            can_load, reason = self.can_load_model(model_name)
            status = "loaded" if model_name in self.models else ("available" if can_load else "unavailable")
            
            models.append({
                "id": model_name,
                "name": config["name"],
                "type": config["type"],
                "memory_8bit_gb": config["memory_8bit_gb"],
                "status": status,
                "reason": reason if not can_load else None
            })
        
        return models

# Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = Flask(__name__)
server = MultiModelTransformersServer()

@app.route('/health')
def health():
    gpu_info = server.get_gpu_memory_info()
    return jsonify({
        "status": "healthy",
        "loaded_models": len(server.models),
        "gpu_available": gpu_info["available"],
        "gpu_memory": gpu_info if gpu_info["available"] else None
    })

@app.route('/v1/models')
def models():
    loaded_models = server.get_loaded_models()
    return jsonify({
        "data": [
            {
                "id": model["id"],
                "object": "model",
                "created": int(time.time()),
                "owned_by": "transformers_multi"
            }
            for model in loaded_models
        ]
    })

@app.route('/models/available')
def available_models():
    return jsonify({
        "models": server.get_available_models()
    })

@app.route('/models/load', methods=['POST'])
def load_model():
    data = request.json
    model_name = data.get('model')
    
    if not model_name:
        return jsonify({"error": "Model name required"}), 400
    
    success, message = server.load_model(model_name)
    
    if success:
        return jsonify({"success": True, "message": message})
    else:
        return jsonify({"error": message}), 400

@app.route('/models/unload', methods=['POST'])
def unload_model():
    data = request.json
    model_name = data.get('model')
    
    if not model_name:
        return jsonify({"error": "Model name required"}), 400
    
    success, message = server.unload_model(model_name)
    
    if success:
        return jsonify({"success": True, "message": message})
    else:
        return jsonify({"error": message}), 400

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    try:
        data = request.json
        model_name = data.get('model')
        messages = data.get('messages', [])
        
        if not model_name:
            return jsonify({"error": "Model name required"}), 400
        
        if not messages:
            return jsonify({"error": "Messages required"}), 400
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–∞
        user_message = messages[-1]
        content = user_message.get('content', [])
        
        text_prompt = None
        image_data = None
        
        for item in content:
            if item.get('type') == 'text':
                text_prompt = item.get('text')
            elif item.get('type') == 'image_url':
                image_data = item.get('image_url', {}).get('url')
        
        if not image_data:
            return jsonify({"error": "Image required"}), 400
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        result = server.process_image(model_name, image_data, text_prompt)
        
        if "error" in result:
            return jsonify({"error": result["error"]}), 500
        
        # –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ OpenAI API
        response = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model_name,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": result["text"]
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 100,
                "completion_tokens": len(result["text"].split()),
                "total_tokens": 100 + len(result["text"].split())
            },
            "generation_time": result.get("generation_time", 0)
        }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ú–ù–û–ì–û–ú–û–î–ï–õ–¨–ù–´–ô TRANSFORMERS –°–ï–†–í–ï–†")
    print("=" * 45)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
    gpu_info = server.get_gpu_memory_info()
    if gpu_info["available"]:
        print(f"‚úÖ GPU –¥–æ—Å—Ç—É–ø–Ω–∞: {gpu_info['total_gb']:.1f} GB")
        print(f"üíæ –°–≤–æ–±–æ–¥–Ω–æ: {gpu_info['free_gb']:.1f} GB")
    else:
        print("‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU")
    
    print(f"üìÅ –ö–µ—à –º–æ–¥–µ–ª–µ–π: {server.cache_dir}")
    print(f"ü§ñ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π: {len(server.supported_models)}")
    
    # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    default_model = "rednote-hilab/dots.ocr"
    print(f"\nüîÑ –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {default_model}")
    
    def load_default_model():
        success, message = server.load_model(default_model)
        if success:
            print(f"‚úÖ {message}")
        else:
            print(f"‚ö†Ô∏è {message}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
    load_thread = threading.Thread(target=load_default_model)
    load_thread.daemon = True
    load_thread.start()
    
    # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
    print("\nüåê –ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞...")
    print("üì° Endpoints:")
    print("   ‚Ä¢ Health: http://localhost:8000/health")
    print("   ‚Ä¢ Models: http://localhost:8000/v1/models")
    print("   ‚Ä¢ Available: http://localhost:8000/models/available")
    print("   ‚Ä¢ Load: POST http://localhost:8000/models/load")
    print("   ‚Ä¢ Unload: POST http://localhost:8000/models/unload")
    print("   ‚Ä¢ Chat: http://localhost:8000/v1/chat/completions")
    
    app.run(host='0.0.0.0', port=8000, debug=False)

if __name__ == "__main__":
    main()