# –ê–ù–ê–õ–ò–ó –ü–†–û–ë–õ–ï–ú–´: DOTS.OCR –ò FLASH ATTENTION 2 –ù–ê RTX 5070 TI BLACKWELL

## üéØ –ö–û–†–ï–ù–¨ –ü–†–û–ë–õ–ï–ú–´

–í—ã –∞–±—Å–æ–ª—é—Ç–Ω–æ –ø—Ä–∞–≤—ã! **dots.ocr –∂–µ—Å—Ç–∫–æ –∑–∞–≤—è–∑–∞–Ω–∞ –Ω–∞ Flash Attention 2**, –∏ —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–∏—á–∏–Ω–∞ –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º –Ω–∞ RTX 5070 Ti Blackwell (sm_120).

## üîç –¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê

### 1. Flash Attention 2 –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤ dots.ocr:

```python
# –í –∫–æ–¥–µ dots.ocr –º–æ–¥–µ–ª–∏ –∂–µ—Å—Ç–∫–æ –ø—Ä–æ–ø–∏—Å–∞–Ω–æ:
attn_implementation="flash_attention_2"  # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!

# –ü—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ flash-attn –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç fallback:
"flash attention not available! fallback to eager implementation"
```

### 2. –ü—Ä–æ–±–ª–µ–º–∞ —Å RTX 5070 Ti Blackwell (sm_120):

- **Flash Attention 2** –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É **sm_120** (Blackwell)
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: `sm_80/90/100/101/110` (–¥–æ Ada Lovelace)
- **sm_120** (Blackwell) –ø–æ—è–≤–∏–ª–∞—Å—å –≤ 2025 –≥–æ–¥—É, flash-attn –µ—â–µ –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω

### 3. –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ dots.ocr –±–µ–∑ flash-attn:

```
Loading weights: ‚úÖ –£—Å–ø–µ—à–Ω–æ
Model initialization: ‚úÖ –£—Å–ø–µ—à–Ω–æ  
Flash attention check: ‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ ‚Üí fallback to eager
Text generation: ‚ùå –ü–£–°–¢–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´
```

## üìã –î–û–ö–ê–ó–ê–¢–ï–õ–¨–°–¢–í–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ò

### –ò–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ dots.ocr:

1. **–ñ–µ—Å—Ç–∫–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤ requirements**:
   ```
   flash-attn==2.8.0.post2  # –ñ–ï–°–¢–ö–û –ó–ê–ö–û–î–ò–†–û–í–ê–ù–û
   ```

2. **–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –∫–æ–¥–µ –º–æ–¥–µ–ª–∏**:
   ```python
   if not hasattr(model.config, 'attn_implementation'):
       model.config.attn_implementation = "flash_attention_2"
   ```

3. **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —Ä–æ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**:
   - Flash Attention 2 –Ω–µ —Ç–æ–ª—å–∫–æ —É—Å–∫–æ—Ä—è–µ—Ç, –Ω–æ –∏ **–≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**
   - –ë–µ–∑ –Ω–µ–≥–æ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∏–ª–∏ –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

## üõ†Ô∏è –ü–û–ß–ï–ú–£ EAGER ATTENTION –ù–ï –†–ê–ë–û–¢–ê–ï–¢

### –ü—Ä–æ–±–ª–µ–º–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã dots.ocr:

1. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥ Flash Attention**:
   - –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∞—Å—å —Å flash-attn
   - –í–µ—Å–∞ –∏ bias –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏–∫—É Flash Attention

2. **–†–∞–∑–ª–∏—á–∏—è –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö**:
   ```python
   # Flash Attention 2 (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
   attention = flash_attn_func(q, k, v, dropout_p=0.0, causal=True)
   
   # Eager Attention (fallback)
   attention = torch.nn.functional.scaled_dot_product_attention(q, k, v)
   # ‚Üë –î–∞–µ—Ç –†–ê–ó–ù–´–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã!
   ```

3. **–ü—Ä–æ–±–ª–µ–º—ã —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é**:
   - Flash Attention –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
   - Eager implementation –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å —á–∏—Å–ª–µ–Ω–Ω—É—é –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å

## üîß –†–ï–®–ï–ù–ò–Ø –ü–†–û–ë–õ–ï–ú–´

### –í–∞—Ä–∏–∞–Ω—Ç 1: –û–∂–∏–¥–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Flash Attention (–î–û–õ–ì–û–°–†–û–ß–ù–û)

```bash
# –ö–æ–≥–¥–∞ flash-attn –¥–æ–±–∞–≤–∏—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É sm_120:
pip install flash-attn>=2.9.0  # –ë—É–¥—É—â–∞—è –≤–µ—Ä—Å–∏—è —Å Blackwell
```

**–°—Ç–∞—Ç—É—Å**: Flash Attention —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞–¥ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Blackwell, –Ω–æ ETA –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞.

### –í–∞—Ä–∏–∞–Ω—Ç 2: –ö–∞—Å—Ç–æ–º–Ω–∞—è —Å–±–æ—Ä–∫–∞ Flash Attention (–≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–õ–¨–ù–û)

```bash
# –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–±—Ä–∞—Ç—å flash-attn —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π sm_120
export FLASH_ATTN_CUDA_ARCHS="120"
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
python setup.py install
```

**–†–∏—Å–∫–∏**: –ú–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å, —Ç–∞–∫ –∫–∞–∫ —Ç—Ä–µ–±—É—é—Ç—Å—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ CUDA kernels.

### –í–∞—Ä–∏–∞–Ω—Ç 3: –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è dots.ocr –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ Flash Attention (–°–õ–û–ñ–ù–û)

–ü–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è:
1. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å eager attention
2. –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –≤–µ—Å–æ–≤ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ OCR

### –í–∞—Ä–∏–∞–Ω—Ç 4: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ vLLM —Å Blackwell –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)

```bash
# vLLM 0.11.0+ –∏–º–µ–µ—Ç –ª—É—á—à—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É Blackwell
pip install vllm>=0.11.0

# –ó–∞–ø—É—Å–∫ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º eager attention
vllm serve rednote-hilab/dots.ocr \
    --trust-remote-code \
    --enforce-eager \
    --gpu-memory-utilization 0.9
```

## üìä –°–†–ê–í–ù–ï–ù–ò–ï –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í

### –ú–æ–¥–µ–ª–∏ –ë–ï–ó –∂–µ—Å—Ç–∫–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç Flash Attention:

| –ú–æ–¥–µ–ª—å | Flash Attention | –ö–∞—á–µ—Å—Ç–≤–æ OCR | –°–∫–æ—Ä–æ—Å—Ç—å | Blackwell |
|--------|----------------|--------------|----------|-----------|
| **qwen_vl_2b** | –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 3.91s | ‚úÖ |
| **qwen3_vl_2b** | –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 23.12s | ‚úÖ |
| **got_ocr_hf** | –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ | ‚≠ê‚≠ê‚≠ê‚≠ê | 84.14s | ‚úÖ |
| **dots.ocr** | **–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ??? | ‚ùå |

## üéØ –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò

### –î–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ qwen_vl_2b** –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é OCR:
   ```python
   # –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ + —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Blackwell
   model = "Qwen/Qwen2-VL-2B-Instruct"
   attn_implementation = "eager"  # –†–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ
   ```

2. **–†–µ–∞–ª–∏–∑—É–π—Ç–µ fallback —Å–∏—Å—Ç–µ–º—É**:
   ```python
   def smart_ocr(image, prompt):
       # –ü–æ–ø—ã—Ç–∫–∞ 1: dots.ocr (–µ—Å–ª–∏ flash-attn –¥–æ—Å—Ç—É–ø–µ–Ω)
       if flash_attn_available():
           try:
               return dots_ocr.process(image, prompt)
           except:
               pass
       
       # Fallback: qwen_vl_2b (–≤—Å–µ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç)
       return qwen_vl_2b.process(image, prompt)
   ```

### –î–ª—è –±—É–¥—É—â–µ–≥–æ:

1. **–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Flash Attention**:
   - –°–ª–µ–¥–∏—Ç–µ –∑–∞ —Ä–µ–ª–∏–∑–∞–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π sm_120
   - –¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏

2. **–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã**:
   - **DeepSeek-OCR** - –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ –∂–µ—Å—Ç–∫–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
   - **GOT-OCR 2.0** - –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é

## üí° –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

**–í–∞—à–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∞–±—Å–æ–ª—é—Ç–Ω–æ –≤–µ—Ä–Ω–∞!** dots.ocr –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∂–µ—Å—Ç–∫–æ –∑–∞–≤—è–∑–∞–Ω–∞ –Ω–∞ Flash Attention 2, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π —Å RTX 5070 Ti Blackwell –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç.

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: 
- **–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ qwen_vl_2b (–æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ + –ø–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
- **–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ**: –û–∂–∏–¥–∞–π—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è flash-attn —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π sm_120
- **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ**: –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ vLLM —Å–µ—Ä–≤–µ—Ä —Å enforce-eager —Ä–µ–∂–∏–º–æ–º

**–í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ —Å 85.7% —É—Å–ø–µ—à–Ω–æ—Å—Ç—å—é –±–µ–∑ dots.ocr!**

---
*–ê–Ω–∞–ª–∏–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω: 24 —è–Ω–≤–∞—Ä—è 2026*
*–°—Ç–∞—Ç—É—Å: Flash Attention 2 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–∞ –¥–ª—è dots.ocr*