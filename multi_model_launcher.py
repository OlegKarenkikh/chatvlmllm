#!/usr/bin/env python3
"""
–ú–Ω–æ–≥–æ–º–æ–¥–µ–ª—å–Ω—ã–π –ª–∞—É–Ω—á–µ—Ä –¥–ª—è vLLM —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ä–µ—Å—É—Ä—Å–∞–º–∏
"""

import json
import subprocess
import time
import requests
import os
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Any

class MultiModelLauncher:
    def __init__(self, config_file: str = "vllm_models_config.json"):
        self.config_file = config_file
        self.configs = self.load_configs()
        self.cache_path = str(os.path.expanduser("~/.cache/huggingface/hub")).replace('\\', '/')
        self.running_containers = {}
        
    def load_configs(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {self.config_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return {}
        except json.JSONDecodeError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ JSON —Ñ–∞–π–ª–µ: {e}")
            return {}
    
    def run_command(self, command: str, capture_output: bool = True) -> tuple[bool, str]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
        try:
            result = subprocess.run(
                command, 
                shell=True, 
                check=True, 
                capture_output=capture_output, 
                text=True
            )
            return True, result.stdout.strip() if result.stdout else ""
        except subprocess.CalledProcessError as e:
            error_msg = e.stderr.strip() if e.stderr else str(e)
            return False, error_msg
    
    def check_gpu_memory(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU"""
        success, output = self.run_command("nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv,noheader,nounits")
        
        if success:
            lines = output.strip().split('\n')
            gpu_info = []
            
            for i, line in enumerate(lines):
                parts = line.split(', ')
                if len(parts) == 3:
                    total, used, free = map(int, parts)
                    gpu_info.append({
                        'gpu_id': i,
                        'total_mb': total,
                        'used_mb': used,
                        'free_mb': free,
                        'usage_percent': round((used / total) * 100, 1)
                    })
            
            return {'success': True, 'gpus': gpu_info}
        else:
            return {'success': False, 'error': output}
    
    def get_running_containers(self) -> List[Dict[str, str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ vLLM"""
        success, output = self.run_command("docker ps --filter ancestor=vllm/vllm-openai:latest --format json")
        
        containers = []
        if success and output:
            for line in output.strip().split('\n'):
                try:
                    container_info = json.loads(line)
                    containers.append({
                        'id': container_info['ID'],
                        'name': container_info['Names'],
                        'ports': container_info['Ports'],
                        'status': container_info['Status']
                    })
                except json.JSONDecodeError:
                    continue
        
        return containers
    
    def stop_container(self, container_name: str) -> bool:
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞"""
        print(f"üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ {container_name}...")
        
        success, _ = self.run_command(f"docker stop {container_name}")
        if success:
            self.run_command(f"docker rm {container_name}")
            return True
        return False
    
    def wait_for_model(self, port: int, model_name: str, timeout: int = 300) -> bool:
        """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ {model_name} –Ω–∞ –ø–æ—Ä—Ç—É {port}...")
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=5)
                if response.status_code == 200:
                    print(f"‚úÖ {model_name} –≥–æ—Ç–æ–≤–∞!")
                    return True
            except requests.exceptions.ConnectionError:
                pass
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {model_name}: {e}")
            
            time.sleep(10)
        
        print(f"‚ùå {model_name} –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª–∞—Å—å –∑–∞ {timeout} —Å–µ–∫—É–Ω–¥")
        return False
    
    def launch_model(self, model_name: str, wait: bool = True) -> bool:
        """–ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if model_name not in self.configs:
            print(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return False
        
        config = self.configs[model_name]
        container_name = config['container_name']
        port = config['port']
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        self.stop_container(container_name)
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã Docker
        vllm_params = config['vllm_params']
        
        docker_command = f"""
        docker run -d \
            --gpus all \
            --name {container_name} \
            --restart unless-stopped \
            -p {port}:{port} \
            -v {self.cache_path}:/root/.cache/huggingface/hub:ro \
            --shm-size=8g \
            vllm/vllm-openai:latest \
            --model {model_name} \
            --trust-remote-code \
            --max-model-len {vllm_params['max_model_len']} \
            --gpu-memory-utilization {vllm_params['gpu_memory_utilization']} \
            --host 0.0.0.0 \
            --port {port} \
            --disable-log-requests
        """.strip().replace('\n', ' ').replace('\\', '')
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if vllm_params.get('enforce_eager'):
            docker_command += " --enforce-eager"
        
        print(f"üöÄ –ó–∞–ø—É—Å–∫ {model_name} –Ω–∞ –ø–æ—Ä—Ç—É {port}...")
        print(f"üì¶ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä: {container_name}")
        print(f"üíæ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {config['size_gb']} –ì–ë")
        
        success, output = self.run_command(docker_command)
        
        if success:
            print(f"‚úÖ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä {container_name} –∑–∞–ø—É—â–µ–Ω")
            
            if wait:
                if self.wait_for_model(port, model_name):
                    self.running_containers[model_name] = {
                        'container_name': container_name,
                        'port': port,
                        'status': 'running'
                    }
                    return True
                else:
                    print(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –≥–æ—Ç–æ–≤–∞")
                    return False
            else:
                self.running_containers[model_name] = {
                    'container_name': container_name,
                    'port': port,
                    'status': 'starting'
                }
                return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ {model_name}: {output}")
            return False
    
    def launch_multiple_models(self, model_names: List[str], sequential: bool = True) -> Dict[str, bool]:
        """–ó–∞–ø—É—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        results = {}
        
        if sequential:
            print("üîÑ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π...")
            for model_name in model_names:
                results[model_name] = self.launch_model(model_name, wait=True)
                if not results[model_name]:
                    print(f"‚ö†Ô∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ —Å {model_name}")
                    break
        else:
            print("üîÑ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π...")
            # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è
            for model_name in model_names:
                results[model_name] = self.launch_model(model_name, wait=False)
            
            # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö
            for model_name in model_names:
                if results[model_name]:
                    config = self.configs[model_name]
                    port = config['port']
                    if self.wait_for_model(port, model_name):
                        self.running_containers[model_name]['status'] = 'running'
                    else:
                        results[model_name] = False
        
        return results
    
    def stop_all_models(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π...")
        
        containers = self.get_running_containers()
        for container in containers:
            self.stop_container(container['name'])
        
        self.running_containers.clear()
        print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    
    def show_status(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üìä –°–¢–ê–¢–£–° –ú–û–î–ï–õ–ï–ô")
        print("=" * 40)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
        gpu_info = self.check_gpu_memory()
        if gpu_info['success']:
            for gpu in gpu_info['gpus']:
                print(f"üéÆ GPU {gpu['gpu_id']}: {gpu['used_mb']}/{gpu['total_mb']} –ú–ë ({gpu['usage_percent']}%)")
        
        print()
        
        # –ó–∞–ø—É—â–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
        containers = self.get_running_containers()
        if containers:
            print("üü¢ –ó–ê–ü–£–©–ï–ù–ù–´–ï –ú–û–î–ï–õ–ò:")
            for container in containers:
                print(f"   ‚Ä¢ {container['name']} - {container['status']}")
                print(f"     –ü–æ—Ä—Ç—ã: {container['ports']}")
        else:
            print("üî¥ –ù–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        
        print()
        
        # –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        print("üìã –î–û–°–¢–£–ü–ù–´–ï –ú–û–î–ï–õ–ò:")
        sorted_models = sorted(self.configs.items(), key=lambda x: x[1]['priority'])
        
        for model_name, config in sorted_models:
            status = "üü¢" if any(c['name'] == config['container_name'] for c in containers) else "üî¥"
            print(f"   {status} {model_name}")
            print(f"      –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {config['category']}")
            print(f"      –†–∞–∑–º–µ—Ä: {config['size_gb']} –ì–ë")
            print(f"      –ü–æ—Ä—Ç: {config['port']}")
            if config['issues']:
                print(f"      –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(config['issues'])}")
    
    def create_unified_client(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        client_code = '''#!/usr/bin/env python3
"""
–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π vLLM
"""

import requests
import base64
import json
from pathlib import Path
from typing import Dict, Any, Optional

class UnifiedVLLMClient:
    def __init__(self):
        self.models = {}
        self.load_model_configs()
    
    def load_model_configs(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π"""
        try:
            with open('vllm_models_config.json', 'r', encoding='utf-8') as f:
                configs = json.load(f)
            
            for model_name, config in configs.items():
                self.models[model_name] = {
                    'url': f"http://localhost:{config['port']}",
                    'category': config['category'],
                    'port': config['port']
                }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {e}")
    
    def check_model_health(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            return False
        
        try:
            url = self.models[model_name]['url']
            response = requests.get(f"{url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self) -> Dict[str, Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        available = {}
        for model_name, config in self.models.items():
            if self.check_model_health(model_name):
                available[model_name] = config
        return available
    
    def process_image(self, model_name: str, image_path: str, 
                     prompt: str = "Extract all text from this image") -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not self.check_model_health(model_name):
            return {"success": False, "error": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"}
        
        try:
            with open(image_path, "rb") as f:
                image_data = f.read()
            
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            ext = Path(image_path).suffix.lower()
            mime_types = {
                '.png': 'image/png',
                '.jpg': 'image/jpeg', 
                '.jpeg': 'image/jpeg'
            }
            mime_type = mime_types.get(ext, 'image/jpeg')
            
            payload = {
                "model": model_name,
                "messages": [{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{image_base64}"}}
                    ]
                }],
                "max_tokens": 1000,
                "temperature": 0.1
            }
            
            url = self.models[model_name]['url']
            response = requests.post(f"{url}/v1/chat/completions", json=payload, timeout=120)
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "text": result["choices"][0]["message"]["content"],
                    "model": model_name,
                    "usage": result.get("usage", {})
                }
            else:
                return {"success": False, "error": f"HTTP {response.status_code}"}
                
        except Exception as e:
            return {"success": False, "error": str(e)}

def main():
    client = UnifiedVLLMClient()
    
    print("üöÄ –£–ù–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–´–ô –ö–õ–ò–ï–ù–¢ VLLM")
    print("=" * 35)
    
    available = client.get_available_models()
    if available:
        print("‚úÖ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
        for model_name, config in available.items():
            print(f"   ‚Ä¢ {model_name} (–ø–æ—Ä—Ç {config['port']}, {config['category']})")
    else:
        print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")

if __name__ == "__main__":
    main()
'''
        
        with open('unified_vllm_client.py', 'w', encoding='utf-8') as f:
            f.write(client_code)
        
        print("üíæ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: unified_vllm_client.py")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    parser = argparse.ArgumentParser(description="–ú–Ω–æ–≥–æ–º–æ–¥–µ–ª—å–Ω—ã–π –ª–∞—É–Ω—á–µ—Ä vLLM")
    parser.add_argument("--launch", nargs="+", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª–∏")
    parser.add_argument("--launch-all", action="store_true", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏")
    parser.add_argument("--launch-ocr", action="store_true", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ OCR –º–æ–¥–µ–ª–∏")
    parser.add_argument("--launch-vlm", action="store_true", help="–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ VLM –º–æ–¥–µ–ª–∏")
    parser.add_argument("--stop-all", action="store_true", help="–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏")
    parser.add_argument("--status", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å")
    parser.add_argument("--sequential", action="store_true", help="–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫")
    parser.add_argument("--create-client", action="store_true", help="–°–æ–∑–¥–∞—Ç—å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç")
    
    args = parser.parse_args()
    
    launcher = MultiModelLauncher()
    
    if not launcher.configs:
        print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π")
        return
    
    if args.status:
        launcher.show_status()
        return
    
    if args.stop_all:
        launcher.stop_all_models()
        return
    
    if args.create_client:
        launcher.create_unified_client()
        return
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–ø—É—Å–∫–∞
    models_to_launch = []
    
    if args.launch:
        models_to_launch = args.launch
    elif args.launch_all:
        models_to_launch = list(launcher.configs.keys())
    elif args.launch_ocr:
        models_to_launch = [name for name, config in launcher.configs.items() 
                           if config['category'] == 'ocr']
    elif args.launch_vlm:
        models_to_launch = [name for name, config in launcher.configs.items() 
                           if config['category'] == 'vlm']
    
    if models_to_launch:
        print(f"üöÄ –ó–ê–ü–£–°–ö –ú–û–î–ï–õ–ï–ô: {len(models_to_launch)}")
        print("=" * 40)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –ø–∞–º—è—Ç–∏
        gpu_info = launcher.check_gpu_memory()
        if gpu_info['success']:
            total_memory = sum(gpu['free_mb'] for gpu in gpu_info['gpus'])
            print(f"üíæ –î–æ—Å—Ç—É–ø–Ω–∞—è GPU –ø–∞–º—è—Ç—å: {total_memory} –ú–ë")
        
        # –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–µ–π
        results = launcher.launch_multiple_models(models_to_launch, args.sequential)
        
        # –û—Ç—á–µ—Ç –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ó–ê–ü–£–°–ö–ê:")
        print("=" * 25)
        
        successful = [name for name, success in results.items() if success]
        failed = [name for name, success in results.items() if not success]
        
        if successful:
            print("‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω—ã:")
            for model_name in successful:
                config = launcher.configs[model_name]
                print(f"   ‚Ä¢ {model_name} (–ø–æ—Ä—Ç {config['port']})")
        
        if failed:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å:")
            for model_name in failed:
                print(f"   ‚Ä¢ {model_name}")
        
        print(f"\nüéØ –ò—Ç–æ–≥–æ: {len(successful)}/{len(models_to_launch)} –º–æ–¥–µ–ª–µ–π –∑–∞–ø—É—â–µ–Ω–æ")
        
        if successful:
            print("\nüí° –°–æ–∑–¥–∞–π—Ç–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç:")
            print("   python multi_model_launcher.py --create-client")
    else:
        launcher.show_status()
        print("\nüí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")
        print("   python multi_model_launcher.py --launch-ocr")
        print("   python multi_model_launcher.py --launch rednote-hilab/dots.ocr")
        print("   python multi_model_launcher.py --launch-all --sequential")
        print("   python multi_model_launcher.py --status")
        print("   python multi_model_launcher.py --stop-all")

if __name__ == "__main__":
    main()