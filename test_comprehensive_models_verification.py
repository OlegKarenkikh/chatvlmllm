#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

import time
import torch
from PIL import Image, ImageDraw, ImageFont
import sys
import os
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def create_test_document():
    """–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è OCR"""
    img = Image.new('RGB', (600, 400), color='white')
    draw = ImageDraw.Draw(img)
    
    try:
        font = ImageFont.truetype("arial.ttf", 20)
        small_font = ImageFont.truetype("arial.ttf", 14)
    except:
        font = ImageFont.load_default()
        small_font = ImageFont.load_default()
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    draw.text((50, 30), "–¢–ï–°–¢–û–í–´–ô –î–û–ö–£–ú–ï–ù–¢", fill='black', font=font)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
    draw.text((50, 80), "–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞: 123456789", fill='black', font=small_font)
    draw.text((50, 110), "–î–∞—Ç–∞ –≤—ã–¥–∞—á–∏: 19.01.2026", fill='black', font=small_font)
    draw.text((50, 140), "–°—Ç–∞—Ç—É—Å: –ê–ö–¢–ò–í–ï–ù", fill='black', font=small_font)
    
    # –¢–∞–±–ª–∏—Ü–∞
    draw.text((50, 180), "–¢–∞–±–ª–∏—Ü–∞ –¥–∞–Ω–Ω—ã—Ö:", fill='black', font=small_font)
    draw.rectangle([50, 200, 550, 280], outline='black', width=2)
    draw.line([50, 230, 550, 230], fill='black', width=1)
    draw.line([200, 200, 200, 280], fill='black', width=1)
    draw.line([350, 200, 350, 280], fill='black', width=1)
    
    draw.text((60, 210), "–ü–∞—Ä–∞–º–µ—Ç—Ä", fill='black', font=small_font)
    draw.text((210, 210), "–ó–Ω–∞—á–µ–Ω–∏–µ", fill='black', font=small_font)
    draw.text((360, 210), "–ï–¥–∏–Ω–∏—Ü–∞", fill='black', font=small_font)
    
    draw.text((60, 240), "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", fill='black', font=small_font)
    draw.text((210, 240), "25.5", fill='black', font=small_font)
    draw.text((360, 240), "¬∞C", fill='black', font=small_font)
    
    draw.text((60, 260), "–í–ª–∞–∂–Ω–æ—Å—Ç—å", fill='black', font=small_font)
    draw.text((210, 260), "65", fill='black', font=small_font)
    draw.text((360, 260), "%", fill='black', font=small_font)
    
    # –§–æ—Ä–º—É–ª–∞
    draw.text((50, 310), "–§–æ—Ä–º—É–ª–∞: E = mc¬≤", fill='black', font=small_font)
    
    return img

def test_model_performance(model_name, expected_keywords=None):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏"""
    print(f"\nüöÄ –¢–ï–°–¢ –ú–û–î–ï–õ–ò: {model_name}")
    print("=" * 50)
    
    if expected_keywords is None:
        expected_keywords = ["–¢–ï–°–¢–û–í–´–ô", "–î–û–ö–£–ú–ï–ù–¢", "123456789", "19.01.2026", "–ê–ö–¢–ò–í–ï–ù", "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", "25.5"]
    
    try:
        from models.model_loader import ModelLoader
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
        if torch.cuda.is_available():
            print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
            vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"‚úÖ VRAM: {vram_gb:.2f}GB")
        else:
            print("‚ùå GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return {"status": "error", "error": "No GPU"}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
        start_load = time.time()
        
        model = ModelLoader.load_model(model_name)
        load_time = time.time() - start_load
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        print("üñºÔ∏è –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç...")
        image = create_test_document()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        print("üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
        start_process = time.time()
        
        if hasattr(model, 'process_image'):
            result = model.process_image(image)
        elif hasattr(model, 'chat'):
            result = model.chat(image, "–ò–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        else:
            result = "–ú–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω"
        
        process_time = time.time() - start_process
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {process_time:.3f}s")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç ({len(result)} —Å–∏–º–≤–æ–ª–æ–≤):")
        print(f"   {result[:200]}{'...' if len(result) > 200 else ''}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ OCR
        found_keywords = 0
        for keyword in expected_keywords:
            if keyword.upper() in result.upper():
                found_keywords += 1
        
        quality_score = (found_keywords / len(expected_keywords)) * 100
        print(f"üéØ –ö–∞—á–µ—Å—Ç–≤–æ OCR: {found_keywords}/{len(expected_keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ ({quality_score:.1f}%)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º—É—Å–æ—Ä–Ω—ã–π –≤—ã–≤–æ–¥
        is_garbage = False
        garbage_indicators = ["Champion", "kaps", "ADDR", "ƒ†ƒ†ƒ†", "ƒäƒäƒä"]
        for indicator in garbage_indicators:
            if indicator in result:
                is_garbage = True
                break
        
        if is_garbage:
            print("‚ö†Ô∏è –û–ë–ù–ê–†–£–ñ–ï–ù –ú–£–°–û–†–ù–´–ô –í–´–í–û–î!")
        
        # –í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print("üîÑ –í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
        model.unload()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        status = "excellent" if quality_score >= 80 and not is_garbage else \
                "good" if quality_score >= 60 and not is_garbage else \
                "poor" if not is_garbage else "garbage"
        
        result_data = {
            "status": status,
            "load_time": load_time,
            "process_time": process_time,
            "output_length": len(result),
            "quality_score": quality_score,
            "found_keywords": found_keywords,
            "total_keywords": len(expected_keywords),
            "is_garbage": is_garbage,
            "output_sample": result[:500]
        }
        
        print(f"üèÜ –°–¢–ê–¢–£–°: {status.upper()}")
        return result_data
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "error": str(e)}

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üî¨ –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –í–°–ï–• –ú–û–î–ï–õ–ï–ô")
    print("=" * 60)
    
    # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    models_to_test = [
        "qwen_vl_2b",      # –≠—Ç–∞–ª–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        "got_ocr_hf",      # –ü—Ä–æ–±–ª–µ–º–Ω–∞—è –º–æ–¥–µ–ª—å
        "qwen3_vl_2b",     # –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å
        "dots_ocr",        # –î–æ–∫—É–º–µ–Ω—Ç-–ø–∞—Ä—Å–µ—Ä
        "phi3_vision",     # Microsoft –º–æ–¥–µ–ª—å
        "deepseek_ocr",    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è
        "got_ocr_ucas"     # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è GOT-OCR
    ]
    
    results = {}
    
    for model_name in models_to_test:
        try:
            result = test_model_performance(model_name)
            results[model_name] = result
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏
            time.sleep(2)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ {model_name}: {e}")
            results[model_name] = {"status": "critical_error", "error": str(e)}
    
    # –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 60)
    print("üìä –°–í–û–î–ù–´–ô –û–¢–ß–ï–¢")
    print("=" * 60)
    
    working_models = []
    problematic_models = []
    failed_models = []
    
    for model_name, result in results.items():
        status = result.get("status", "unknown")
        
        if status in ["excellent", "good"]:
            working_models.append(model_name)
            load_time = result.get("load_time", 0)
            process_time = result.get("process_time", 0)
            quality = result.get("quality_score", 0)
            print(f"‚úÖ {model_name:15} | {load_time:6.2f}s –∑–∞–≥—Ä—É–∑–∫–∞ | {process_time:6.3f}s –æ–±—Ä–∞–±–æ—Ç–∫–∞ | {quality:5.1f}% –∫–∞—á–µ—Å—Ç–≤–æ")
        elif status == "poor":
            problematic_models.append(model_name)
            print(f"‚ö†Ô∏è {model_name:15} | –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ OCR")
        elif status == "garbage":
            problematic_models.append(model_name)
            print(f"üóëÔ∏è {model_name:15} | –ú—É—Å–æ—Ä–Ω—ã–π –≤—ã–≤–æ–¥")
        else:
            failed_models.append(model_name)
            error = result.get("error", "Unknown error")
            print(f"‚ùå {model_name:15} | –û—à–∏–±–∫–∞: {error}")
    
    print(f"\nüìà –ò–¢–û–ì–ò:")
    print(f"‚úÖ –†–∞–±–æ—á–∏–µ –º–æ–¥–µ–ª–∏: {len(working_models)}/{len(models_to_test)} ({len(working_models)/len(models_to_test)*100:.1f}%)")
    print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–æ–¥–µ–ª–∏: {len(problematic_models)}")
    print(f"‚ùå –ù–µ—Ä–∞–±–æ—á–∏–µ –º–æ–¥–µ–ª–∏: {len(failed_models)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("comprehensive_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ comprehensive_test_results.json")
    
    return results

if __name__ == "__main__":
    results = main()