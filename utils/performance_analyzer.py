#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""

import json
import os
import glob
from datetime import datetime
from typing import Dict, List, Optional, Any
import pandas as pd

class PerformanceAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, results_dir: str = "."):
        self.results_dir = results_dir
        self.historical_data = {}
        self.load_historical_results()
    
    def load_historical_results(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        result_patterns = [
            "benchmark_results_*.json",
            "*_test_results*.json", 
            "final_working_models.json",
            "working_models_config.json",
            "dots_ocr_*_results.json",
            "official_prompts_*_results.json",
            "vllm_*_test_*.json"
        ]
        
        for pattern in result_patterns:
            files = glob.glob(os.path.join(self.results_dir, pattern))
            for file_path in files:
                try:
                    self._load_result_file(file_path)
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
    
    def _load_result_file(self, file_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        file_name = os.path.basename(file_path)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if "benchmark_results" in file_name:
            self._process_benchmark_results(data, file_name)
        elif "working_models" in file_name:
            self._process_working_models(data, file_name)
        elif "test_results" in file_name:
            self._process_test_results(data, file_name)
        else:
            # –û–±—â–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            self.historical_data[file_name] = data
    
    def _process_benchmark_results(self, data: Dict, source: str) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"""
        
        model_name = data.get("model", "unknown")
        timestamp = data.get("timestamp", "")
        
        if model_name not in self.historical_data:
            self.historical_data[model_name] = {
                "benchmarks": [],
                "performance_metrics": {},
                "last_updated": timestamp
            }
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        total_tests = 0
        successful_tests = 0
        total_time = 0
        avg_processing_time = 0
        
        for result in data.get("results", []):
            for test in result.get("tests", []):
                total_tests += 1
                if test.get("success", False):
                    successful_tests += 1
                    total_time += test.get("processing_time", 0)
        
        if successful_tests > 0:
            avg_processing_time = total_time / successful_tests
        
        benchmark_summary = {
            "timestamp": timestamp,
            "source": source,
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": (successful_tests / total_tests * 100) if total_tests > 0 else 0,
            "avg_processing_time": avg_processing_time,
            "total_documents": data.get("total_documents", 0)
        }
        
        self.historical_data[model_name]["benchmarks"].append(benchmark_summary)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.historical_data[model_name]["performance_metrics"].update({
            "latest_success_rate": benchmark_summary["success_rate"],
            "latest_avg_time": avg_processing_time,
            "total_tests_run": total_tests
        })
    
    def _process_working_models(self, data: Dict, source: str) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–∞–±–æ—á–∏—Ö –º–æ–¥–µ–ª—è—Ö"""
        
        for model_name, model_info in data.items():
            if isinstance(model_info, dict):
                if model_name not in self.historical_data:
                    self.historical_data[model_name] = {
                        "benchmarks": [],
                        "performance_metrics": {},
                        "status_info": {}
                    }
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç–∞—Ç—É—Å–µ
                status_info = {
                    "status": model_info.get("status", "unknown"),
                    "last_tested": model_info.get("last_tested", ""),
                    "source": source
                }
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤
                test_results = model_info.get("test_results", {})
                if test_results:
                    status_info.update({
                        "text_test_success": test_results.get("text_test_success", False),
                        "vision_test_success": test_results.get("vision_test_success", False),
                        "inference_time": test_results.get("inference_time", 0),
                        "load_time": test_results.get("load_time", 0)
                    })
                
                self.historical_data[model_name]["status_info"] = status_info
    
    def _process_test_results(self, data: Dict, source: str) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        timestamp = data.get("timestamp", "")
        test_results = data.get("test_results", [])
        
        if isinstance(test_results, list):
            for result in test_results:
                model_name = result.get("model", "unknown")
                
                if model_name not in self.historical_data:
                    self.historical_data[model_name] = {
                        "benchmarks": [],
                        "performance_metrics": {},
                        "test_history": []
                    }
                
                test_summary = {
                    "timestamp": timestamp,
                    "source": source,
                    "success": result.get("success", False),
                    "processing_time": result.get("processing_time", 0),
                    "tokens_used": result.get("tokens_used", 0)
                }
                
                self.historical_data[model_name]["test_history"].append(test_summary)
    
    def get_model_comparison_data(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        
        comparison_data = []
        
        for model_name, model_data in self.historical_data.items():
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –º–æ–¥–µ–ª—è–º–∏
            if not isinstance(model_data, dict) or "benchmarks" not in model_data:
                continue
            
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
            row = {
                "–ú–æ–¥–µ–ª—å": model_name,
                "–°—Ç–∞—Ç—É—Å": "‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å (%)": 0,
                "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (—Å)": 0,
                "–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤": 0,
                "–ü–æ—Å–ª–µ–¥–Ω–µ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
            }
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
            benchmarks = model_data.get("benchmarks", [])
            if benchmarks:
                latest_benchmark = benchmarks[-1]  # –ü–æ—Å–ª–µ–¥–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫
                row.update({
                    "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å (%)": round(latest_benchmark.get("success_rate", 0), 1),
                    "–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (—Å)": round(latest_benchmark.get("avg_processing_time", 0), 3),
                    "–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤": latest_benchmark.get("total_tests", 0),
                    "–ü–æ—Å–ª–µ–¥–Ω–µ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ": latest_benchmark.get("timestamp", "").split("T")[0]
                })
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–∞—Ç—É—Å–µ
            status_info = model_data.get("status_info", {})
            if status_info:
                status = status_info.get("status", "unknown")
                if status == "tested_working":
                    row["–°—Ç–∞—Ç—É—Å"] = "‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç"
                elif status == "partially_working":
                    row["–°—Ç–∞—Ç—É—Å"] = "‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ"
                elif status == "not_working":
                    row["–°—Ç–∞—Ç—É—Å"] = "‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
                
                if status_info.get("last_tested"):
                    row["–ü–æ—Å–ª–µ–¥–Ω–µ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"] = status_info["last_tested"].split(" ")[0]
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
            if row["–£—Å–ø–µ—à–Ω–æ—Å—Ç—å (%)"] >= 90:
                row["–°—Ç–∞—Ç—É—Å"] = "‚úÖ –û—Ç–ª–∏—á–Ω–æ"
            elif row["–£—Å–ø–µ—à–Ω–æ—Å—Ç—å (%)"] >= 70:
                row["–°—Ç–∞—Ç—É—Å"] = "‚ö†Ô∏è –•–æ—Ä–æ—à–æ"
            elif row["–£—Å–ø–µ—à–Ω–æ—Å—Ç—å (%)"] > 0:
                row["–°—Ç–∞—Ç—É—Å"] = "‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ"
            
            comparison_data.append(row)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
        df = pd.DataFrame(comparison_data)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        if not df.empty:
            df = df.sort_values("–£—Å–ø–µ—à–Ω–æ—Å—Ç—å (%)", ascending=False)
        
        return df
    
    def get_model_details(self, model_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        
        if model_name not in self.historical_data:
            return {"error": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"}
        
        model_data = self.historical_data[model_name]
        
        details = {
            "model_name": model_name,
            "benchmarks_count": len(model_data.get("benchmarks", [])),
            "test_history_count": len(model_data.get("test_history", [])),
            "performance_metrics": model_data.get("performance_metrics", {}),
            "status_info": model_data.get("status_info", {}),
            "recent_benchmarks": model_data.get("benchmarks", [])[-3:],  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3
            "recent_tests": model_data.get("test_history", [])[-5:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5
        }
        
        return details
    
    def get_performance_trends(self, model_name: str) -> Dict[str, List]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        
        if model_name not in self.historical_data:
            return {"error": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"}
        
        model_data = self.historical_data[model_name]
        benchmarks = model_data.get("benchmarks", [])
        
        trends = {
            "timestamps": [],
            "success_rates": [],
            "processing_times": [],
            "test_counts": []
        }
        
        for benchmark in benchmarks:
            trends["timestamps"].append(benchmark.get("timestamp", ""))
            trends["success_rates"].append(benchmark.get("success_rate", 0))
            trends["processing_times"].append(benchmark.get("avg_processing_time", 0))
            trends["test_counts"].append(benchmark.get("total_tests", 0))
        
        return trends
    
    def get_summary_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º"""
        
        total_models = len([k for k, v in self.historical_data.items() 
                           if isinstance(v, dict) and "benchmarks" in v])
        
        working_models = 0
        total_tests = 0
        avg_success_rate = 0
        
        success_rates = []
        
        for model_name, model_data in self.historical_data.items():
            if not isinstance(model_data, dict) or "benchmarks" not in model_data:
                continue
            
            benchmarks = model_data.get("benchmarks", [])
            if benchmarks:
                latest = benchmarks[-1]
                success_rate = latest.get("success_rate", 0)
                success_rates.append(success_rate)
                total_tests += latest.get("total_tests", 0)
                
                if success_rate > 50:  # –°—á–∏—Ç–∞–µ–º —Ä–∞–±–æ—á–µ–π –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç—å > 50%
                    working_models += 1
        
        if success_rates:
            avg_success_rate = sum(success_rates) / len(success_rates)
        
        return {
            "total_models": total_models,
            "working_models": working_models,
            "total_tests_run": total_tests,
            "average_success_rate": round(avg_success_rate, 1),
            "models_with_data": len(success_rates)
        }

def test_performance_analyzer():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
    print("=" * 50)
    
    analyzer = PerformanceAnalyzer()
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = analyzer.get_summary_statistics()
    print(f"\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏: {stats['total_models']}")
    print(f"   –†–∞–±–æ—á–∏—Ö –º–æ–¥–µ–ª–µ–π: {stats['working_models']}")
    print(f"   –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ: {stats['total_tests_run']}")
    print(f"   –°—Ä–µ–¥–Ω—è—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {stats['average_success_rate']}%")
    
    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    comparison_df = analyzer.get_model_comparison_data()
    print(f"\nüìã –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ({len(comparison_df)} –º–æ–¥–µ–ª–µ–π):")
    if not comparison_df.empty:
        print(comparison_df.to_string(index=False))
    else:
        print("   –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    
    # –î–µ—Ç–∞–ª–∏ –ø–æ –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
    if not comparison_df.empty:
        first_model = comparison_df.iloc[0]["–ú–æ–¥–µ–ª—å"]
        details = analyzer.get_model_details(first_model)
        print(f"\nüîç –î–µ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–∏ '{first_model}':")
        print(f"   –ë–µ–Ω—á–º–∞—Ä–∫–æ–≤: {details['benchmarks_count']}")
        print(f"   –¢–µ—Å—Ç–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏–∏: {details['test_history_count']}")
    
    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

if __name__ == "__main__":
    test_performance_analyzer()