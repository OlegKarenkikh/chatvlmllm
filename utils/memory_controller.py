#!/usr/bin/env python3
"""
–ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –ø–∞–º—è—Ç–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –º–µ–∂–¥—É vLLM –∏ Transformers
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å –≤—ã–≥—Ä—É–∑–∫–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π —Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é
"""

import gc
import time
import psutil
import subprocess
import json
import os
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import threading
import logging

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ExecutionMode(Enum):
    """–†–µ–∂–∏–º—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
    VLLM = "vllm"
    TRANSFORMERS = "transformers"

@dataclass
class MemoryInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏"""
    total_gb: float
    used_gb: float
    free_gb: float
    utilization_percent: float

@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    name: str
    mode: ExecutionMode
    memory_usage_gb: float
    is_loaded: bool
    container_id: Optional[str] = None
    process_id: Optional[int] = None

class MemoryController:
    """–ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –ø–∞–º—è—Ç–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self):
        self.current_mode = None
        self.loaded_models: Dict[str, ModelInfo] = {}
        self.vllm_containers: Dict[str, str] = {}  # model_name -> container_id
        self.transformers_models: Dict[str, Any] = {}  # model_name -> model_instance
        self.memory_threshold_gb = 2.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∑–µ—Ä–≤ –ø–∞–º—è—Ç–∏
        self.cleanup_lock = threading.Lock()
        
    def get_gpu_memory_info(self) -> MemoryInfo:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU –ø–∞–º—è—Ç–∏"""
        if not TORCH_AVAILABLE or not torch.cuda.is_available():
            return MemoryInfo(0, 0, 0, 0)
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º nvidia-smi –¥–ª—è —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            result = subprocess.run([
                "nvidia-smi", 
                "--query-gpu=memory.total,memory.used,memory.free",
                "--format=csv,noheader,nounits"
            ], capture_output=True, text=True, check=True)
            
            line = result.stdout.strip().split('\n')[0]
            total_mb, used_mb, free_mb = map(int, line.split(', '))
            
            total_gb = total_mb / 1024
            used_gb = used_mb / 1024
            free_gb = free_mb / 1024
            utilization = (used_gb / total_gb) * 100
            
            return MemoryInfo(total_gb, used_gb, free_gb, utilization)
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU –ø–∞–º—è—Ç–∏: {e}")
            # Fallback –Ω–∞ PyTorch
            try:
                props = torch.cuda.get_device_properties(0)
                total_gb = props.total_memory / (1024 ** 3)
                allocated_gb = torch.cuda.memory_allocated(0) / (1024 ** 3)
                reserved_gb = torch.cuda.memory_reserved(0) / (1024 ** 3)
                free_gb = total_gb - reserved_gb
                utilization = (reserved_gb / total_gb) * 100
                
                return MemoryInfo(total_gb, reserved_gb, free_gb, utilization)
            except:
                return MemoryInfo(0, 0, 0, 0)
    
    def get_system_memory_info(self) -> MemoryInfo:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        memory = psutil.virtual_memory()
        total_gb = memory.total / (1024 ** 3)
        used_gb = memory.used / (1024 ** 3)
        free_gb = memory.available / (1024 ** 3)
        utilization = memory.percent
        
        return MemoryInfo(total_gb, used_gb, free_gb, utilization)
    
    def cleanup_gpu_memory(self, force: bool = False) -> bool:
        """–û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏"""
        with self.cleanup_lock:
            try:
                logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏...")
                
                if TORCH_AVAILABLE and torch.cuda.is_available():
                    # –û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞ –≤—Å–µ—Ö GPU
                    for i in range(torch.cuda.device_count()):
                        with torch.cuda.device(i):
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                    
                    # IPC –æ—á–∏—Å—Ç–∫–∞
                    try:
                        torch.cuda.ipc_collect()
                    except:
                        pass
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
                for _ in range(3):
                    gc.collect()
                    time.sleep(0.1)
                
                logger.info("‚úÖ GPU –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")
                return True
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ GPU –ø–∞–º—è—Ç–∏: {e}")
                return False
    
    def kill_gpu_processes(self, exclude_pids: List[int] = None) -> List[Dict]:
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö GPU"""
        if exclude_pids is None:
            exclude_pids = []
        
        killed_processes = []
        
        try:
            result = subprocess.run([
                "nvidia-smi", 
                "--query-compute-apps=pid,process_name,used_memory",
                "--format=csv,noheader,nounits"
            ], capture_output=True, text=True, check=True)
            
            for line in result.stdout.strip().split('\n'):
                if not line.strip():
                    continue
                    
                parts = [p.strip() for p in line.split(',')]
                if len(parts) >= 3:
                    pid = int(parts[0])
                    name = parts[1]
                    memory_mb = int(parts[2])
                    
                    if pid in exclude_pids:
                        continue
                    
                    try:
                        if psutil.pid_exists(pid):
                            process = psutil.Process(pid)
                            logger.info(f"üî™ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞: {name} (PID: {pid}, {memory_mb} MB)")
                            
                            process.terminate()
                            time.sleep(1)
                            
                            if process.is_running():
                                process.kill()
                                time.sleep(0.5)
                            
                            killed_processes.append({
                                'pid': pid,
                                'name': name,
                                'memory_mb': memory_mb
                            })
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≤–µ—Ä—à–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å {pid}: {e}")
        
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {e}")
        
        return killed_processes
    
    def check_memory_availability(self, required_gb: float) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏"""
        gpu_info = self.get_gpu_memory_info()
        
        if gpu_info.free_gb < required_gb:
            return False, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏: —Ç—Ä–µ–±—É–µ—Ç—Å—è {required_gb:.1f}GB, –¥–æ—Å—Ç—É–ø–Ω–æ {gpu_info.free_gb:.1f}GB"
        
        if gpu_info.free_gb < (required_gb + self.memory_threshold_gb):
            return False, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑–µ—Ä–≤–∞ –ø–∞–º—è—Ç–∏: —Ç—Ä–µ–±—É–µ—Ç—Å—è {required_gb + self.memory_threshold_gb:.1f}GB, –¥–æ—Å—Ç—É–ø–Ω–æ {gpu_info.free_gb:.1f}GB"
        
        return True, f"–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏: {gpu_info.free_gb:.1f}GB –¥–æ—Å—Ç—É–ø–Ω–æ"
    
    def get_vllm_containers(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
        containers = {}
        
        try:
            result = subprocess.run([
                "docker", "ps", "--format", "{{.ID}}\t{{.Names}}\t{{.Image}}"
            ], capture_output=True, text=True, check=True)
            
            for line in result.stdout.strip().split('\n'):
                if not line.strip():
                    continue
                
                parts = line.split('\t')
                if len(parts) >= 3:
                    container_id, name, image = parts
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –ø–æ –æ–±—Ä–∞–∑—É –∏–ª–∏ –∏–º–µ–Ω–∏
                    if 'vllm' in image.lower() or 'vllm' in name.lower():
                        containers[name] = container_id
        
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤: {e}")
        
        return containers
    
    def stop_vllm_containers(self, model_names: List[str] = None) -> List[str]:
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
        stopped_containers = []
        containers = self.get_vllm_containers()
        
        for name, container_id in containers.items():
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
            if model_names and not any(model in name for model in model_names):
                continue
            
            try:
                logger.info(f"üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: {name}")
                subprocess.run(["docker", "stop", container_id], check=True, capture_output=True)
                stopped_containers.append(name)
                
                # –£–¥–∞–ª—è–µ–º –∏–∑ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
                if name in self.vllm_containers:
                    del self.vllm_containers[name]
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ {name}: {e}")
        
        return stopped_containers
    
    def unload_transformers_models(self, model_names: List[str] = None) -> List[str]:
        """–í—ã–≥—Ä—É–∑–∫–∞ Transformers –º–æ–¥–µ–ª–µ–π"""
        unloaded_models = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ModelLoader –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏
        try:
            from models.model_loader import ModelLoader
            
            loaded_models = ModelLoader.get_loaded_models()
            
            for model_name in loaded_models:
                # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
                if model_names and model_name not in model_names:
                    continue
                
                logger.info(f"üì§ –í—ã–≥—Ä—É–∑–∫–∞ Transformers –º–æ–¥–µ–ª–∏: {model_name}")
                
                if ModelLoader.unload_model(model_name):
                    unloaded_models.append(model_name)
                    
                    # –£–¥–∞–ª—è–µ–º –∏–∑ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
                    if model_name in self.transformers_models:
                        del self.transformers_models[model_name]
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ Transformers –º–æ–¥–µ–ª–µ–π: {e}")
        
        return unloaded_models
    
    def switch_to_vllm_mode(self, target_model: str = None) -> Tuple[bool, str]:
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º vLLM"""
        logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º vLLM (–º–æ–¥–µ–ª—å: {target_model})")
        
        try:
            # 1. –í—ã–≥—Ä—É–∂–∞–µ–º –≤—Å–µ Transformers –º–æ–¥–µ–ª–∏
            unloaded_transformers = self.unload_transformers_models()
            if unloaded_transformers:
                logger.info(f"üì§ –í—ã–≥—Ä—É–∂–µ–Ω—ã Transformers –º–æ–¥–µ–ª–∏: {unloaded_transformers}")
            
            # 2. –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å
            self.cleanup_gpu_memory()
            
            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ –¥–ª—è vLLM
            required_memory = 8.0  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å vLLM
            can_load, message = self.check_memory_availability(required_memory)
            
            if not can_load:
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞...")
                self.kill_gpu_processes()
                self.cleanup_gpu_memory()
                
                # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                can_load, message = self.check_memory_availability(required_memory)
                if not can_load:
                    return False, f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –ø–∞–º—è—Ç—å –¥–ª—è vLLM: {message}"
            
            # 4. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥—Ä—É–≥–∏–µ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å)
            if target_model:
                current_containers = self.get_vllm_containers()
                other_models = [name for name in current_containers.keys() 
                              if target_model not in name]
                
                if other_models:
                    stopped = self.stop_vllm_containers(other_models)
                    logger.info(f"üõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π: {stopped}")
            
            self.current_mode = ExecutionMode.VLLM
            
            gpu_info = self.get_gpu_memory_info()
            return True, f"–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ vLLM —Ä–µ–∂–∏–º —É—Å–ø–µ—à–Ω–æ. –î–æ—Å—Ç—É–ø–Ω–æ {gpu_info.free_gb:.1f}GB GPU –ø–∞–º—è—Ç–∏"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –≤ vLLM —Ä–µ–∂–∏–º: {e}")
            return False, str(e)
    
    def switch_to_transformers_mode(self, target_model: str = None) -> Tuple[bool, str]:
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º Transformers"""
        logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º Transformers (–º–æ–¥–µ–ª—å: {target_model})")
        
        try:
            # 1. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
            stopped_containers = self.stop_vllm_containers()
            if stopped_containers:
                logger.info(f"üõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã: {stopped_containers}")
            
            # 2. –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å
            self.cleanup_gpu_memory()
            
            # 3. –í—ã–≥—Ä—É–∂–∞–µ–º –¥—Ä—É–≥–∏–µ Transformers –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å)
            if target_model:
                try:
                    from models.model_loader import ModelLoader
                    loaded_models = ModelLoader.get_loaded_models()
                    other_models = [name for name in loaded_models if name != target_model]
                    
                    if other_models:
                        unloaded = self.unload_transformers_models(other_models)
                        logger.info(f"üì§ –í—ã–≥—Ä—É–∂–µ–Ω—ã –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏: {unloaded}")
                except:
                    pass
            
            # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ –¥–ª—è Transformers
            required_memory = 4.0  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å Transformers
            can_load, message = self.check_memory_availability(required_memory)
            
            if not can_load:
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞...")
                self.kill_gpu_processes()
                self.cleanup_gpu_memory()
                
                # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                can_load, message = self.check_memory_availability(required_memory)
                if not can_load:
                    return False, f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –ø–∞–º—è—Ç—å –¥–ª—è Transformers: {message}"
            
            self.current_mode = ExecutionMode.TRANSFORMERS
            
            gpu_info = self.get_gpu_memory_info()
            return True, f"–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤ Transformers —Ä–µ–∂–∏–º —É—Å–ø–µ—à–Ω–æ. –î–æ—Å—Ç—É–ø–Ω–æ {gpu_info.free_gb:.1f}GB GPU –ø–∞–º—è—Ç–∏"
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –≤ Transformers —Ä–µ–∂–∏–º: {e}")
            return False, str(e)
    
    def change_model_in_container(self, new_model: str, container_type: str = "vllm") -> Tuple[bool, str]:
        """–°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏"""
        logger.info(f"üîÑ –°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ {new_model} –≤ {container_type}")
        
        try:
            if container_type == "vllm":
                # –î–ª—è vLLM –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                
                # 1. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
                stopped = self.stop_vllm_containers()
                logger.info(f"üõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã: {stopped}")
                
                # 2. –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
                self.cleanup_gpu_memory()
                
                # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à –º–æ–¥–µ–ª–∏
                try:
                    from models.model_loader import ModelLoader
                    is_cached, cache_msg = ModelLoader.check_model_cache(new_model)
                    
                    if not is_cached:
                        return False, f"–ú–æ–¥–µ–ª—å {new_model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ: {cache_msg}"
                    
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {new_model} –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–µ—à –º–æ–¥–µ–ª–∏: {e}")
                
                # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
                required_memory = 8.0
                can_load, message = self.check_memory_availability(required_memory)
                
                if not can_load:
                    return False, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ {new_model}: {message}"
                
                # 5. –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (—ç—Ç–æ –¥–æ–ª–∂–Ω–æ –¥–µ–ª–∞—Ç—å—Å—è –≤–Ω–µ—à–Ω–∏–º –∫–æ–¥–æ–º)
                return True, f"–ì–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É {new_model} –≤ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ"
                
            else:  # transformers
                # –î–ª—è Transformers –≤—ã–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â—É—é –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—É—é
                
                # 1. –í—ã–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
                unloaded = self.unload_transformers_models()
                logger.info(f"üì§ –í—ã–≥—Ä—É–∂–µ–Ω—ã –º–æ–¥–µ–ª–∏: {unloaded}")
                
                # 2. –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
                self.cleanup_gpu_memory()
                
                # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                try:
                    from models.model_loader import ModelLoader
                    is_cached, cache_msg = ModelLoader.check_model_cache(new_model)
                    
                    if not is_cached:
                        return False, f"–ú–æ–¥–µ–ª—å {new_model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ: {cache_msg}"
                    
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {new_model} –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ")
                except Exception as e:
                    return False, f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–µ—à–∞ –º–æ–¥–µ–ª–∏: {e}"
                
                # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
                required_memory = 4.0
                can_load, message = self.check_memory_availability(required_memory)
                
                if not can_load:
                    return False, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ {new_model}: {message}"
                
                # 5. –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
                try:
                    from models.model_loader import ModelLoader
                    model = ModelLoader.load_model(new_model)
                    self.transformers_models[new_model] = model
                    
                    gpu_info = self.get_gpu_memory_info()
                    return True, f"–ú–æ–¥–µ–ª—å {new_model} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {gpu_info.used_gb:.1f}GB GPU –ø–∞–º—è—Ç–∏"
                    
                except Exception as e:
                    return False, f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {new_model}: {e}"
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏: {e}")
            return False, str(e)
    
    def get_cached_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        cached_models = []
        
        try:
            from models.model_loader import ModelLoader
            config = ModelLoader.load_config()
            
            for model_key in config.get("models", {}).keys():
                is_cached, _ = ModelLoader.check_model_cache(model_key)
                if is_cached:
                    cached_models.append(model_key)
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {e}")
        
        return cached_models
    
    def get_memory_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞ –ø–∞–º—è—Ç–∏"""
        gpu_info = self.get_gpu_memory_info()
        system_info = self.get_system_memory_info()
        
        return {
            "current_mode": self.current_mode.value if self.current_mode else None,
            "gpu_memory": {
                "total_gb": gpu_info.total_gb,
                "used_gb": gpu_info.used_gb,
                "free_gb": gpu_info.free_gb,
                "utilization_percent": gpu_info.utilization_percent
            },
            "system_memory": {
                "total_gb": system_info.total_gb,
                "used_gb": system_info.used_gb,
                "free_gb": system_info.free_gb,
                "utilization_percent": system_info.utilization_percent
            },
            "loaded_models": {
                "transformers": list(self.transformers_models.keys()),
                "vllm_containers": list(self.vllm_containers.keys())
            },
            "cached_models": self.get_cached_models(),
            "memory_threshold_gb": self.memory_threshold_gb
        }
    
    def emergency_cleanup(self) -> Tuple[bool, str]:
        """–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"""
        logger.warning("üö® –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏...")
        
        try:
            # 1. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
            stopped_containers = self.stop_vllm_containers()
            
            # 2. –í—ã–≥—Ä—É–∂–∞–µ–º –≤—Å–µ Transformers –º–æ–¥–µ–ª–∏
            unloaded_models = self.unload_transformers_models()
            
            # 3. –ó–∞–≤–µ—Ä—à–∞–µ–º GPU –ø—Ä–æ—Ü–µ—Å—Å—ã
            killed_processes = self.kill_gpu_processes()
            
            # 4. –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å
            self.cleanup_gpu_memory()
            
            # 5. –û—á–∏—â–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ
            self.vllm_containers.clear()
            self.transformers_models.clear()
            self.current_mode = None
            
            gpu_info = self.get_gpu_memory_info()
            
            summary = f"""–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:
- –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤: {len(stopped_containers)}
- –í—ã–≥—Ä—É–∂–µ–Ω–æ Transformers –º–æ–¥–µ–ª–µ–π: {len(unloaded_models)}
- –ó–∞–≤–µ—Ä—à–µ–Ω–æ GPU –ø—Ä–æ—Ü–µ—Å—Å–æ–≤: {len(killed_processes)}
- –î–æ—Å—Ç—É–ø–Ω–æ GPU –ø–∞–º—è—Ç–∏: {gpu_info.free_gb:.1f}GB"""
            
            logger.info("‚úÖ " + summary.replace('\n', ' '))
            return True, summary
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏: {e}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
memory_controller = MemoryController()