#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è dots.ocr —á–µ—Ä–µ–∑ vLLM Docker –≤ –ø—Ä–æ–µ–∫—Ç chatvlmllm
–†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã Flash Attention –Ω–∞ RTX 5070 Ti Blackwell
"""

import sys
import os
import logging
from typing import Dict, List, Any, Optional
from PIL import Image

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from vllm_dots_ocr_client import VLLMDotsOCRClient
except ImportError:
    VLLMDotsOCRClient = None

logger = logging.getLogger(__name__)

class DotsOCRVLLMIntegration:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è dots.ocr —á–µ—Ä–µ–∑ vLLM Docker –¥–ª—è chatvlmllm –ø—Ä–æ–µ–∫—Ç–∞
    –û–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã Flash Attention –Ω–∞ RTX 5070 Ti Blackwell
    """
    
    def __init__(self, vllm_url: str = "http://localhost:8000"):
        self.vllm_url = vllm_url
        self.vllm_client = None
        self.is_available = False
        self.fallback_model = None
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ vLLM –∫–ª–∏–µ–Ω—Ç–∞
        self._initialize_vllm_client()
    
    def _initialize_vllm_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è vLLM –∫–ª–∏–µ–Ω—Ç–∞"""
        if VLLMDotsOCRClient is None:
            logger.warning("vLLM –∫–ª–∏–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –º–æ–¥—É–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        try:
            self.vllm_client = VLLMDotsOCRClient(self.vllm_url)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞
            if self.vllm_client.health_check():
                self.is_available = True
                logger.info(f"‚úÖ vLLM dots.ocr —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω: {self.vllm_url}")
            else:
                logger.warning(f"‚ö†Ô∏è vLLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {self.vllm_url}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ vLLM –∫–ª–∏–µ–Ω—Ç–∞: {e}")
    
    def set_fallback_model(self, fallback_model):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ fallback –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, qwen_vl_2b)"""
        self.fallback_model = fallback_model
        logger.info("‚úÖ Fallback –º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
    
    def is_vllm_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ vLLM —Å–µ—Ä–≤–µ—Ä–∞"""
        if not self.vllm_client:
            return False
        
        try:
            return self.vllm_client.health_check()
        except:
            return False
    
    def process_image_file(self, image_path: str, prompt: str = "Extract all text from this image") -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è OCR
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        # –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ vLLM
        if self.is_vllm_available():
            try:
                result = self.vllm_client.process_image(image_path, prompt)
                
                if result["success"]:
                    return {
                        "success": True,
                        "content": result["content"],
                        "model": "dots.ocr-vllm",
                        "processing_time": result["processing_time"],
                        "method": "vllm"
                    }
                else:
                    logger.warning(f"vLLM –æ—à–∏–±–∫–∞: {result['error']}")
                    
            except Exception as e:
                logger.error(f"vLLM –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
        
        # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        return self._fallback_process(image_path, prompt)
    
    def process_pil_image(self, image: Image.Image, prompt: str = "Extract all text from this image") -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Args:
            image: PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è OCR
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        # –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ vLLM
        if self.is_vllm_available():
            try:
                result = self.vllm_client.process_pil_image(image, prompt)
                
                if result["success"]:
                    return {
                        "success": True,
                        "content": result["content"],
                        "model": "dots.ocr-vllm",
                        "processing_time": result["processing_time"],
                        "method": "vllm"
                    }
                else:
                    logger.warning(f"vLLM –æ—à–∏–±–∫–∞: {result['error']}")
                    
            except Exception as e:
                logger.error(f"vLLM –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
        
        # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        return self._fallback_process_pil(image, prompt)
    
    def chat_completion(self, messages: List[Dict], max_tokens: int = 2048) -> Dict[str, Any]:
        """
        OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API –¥–ª—è chatvlmllm –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        
        Args:
            messages: –°–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –û—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI API
        """
        # –ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ vLLM
        if self.is_vllm_available():
            try:
                result = self.vllm_client.chat_completion(messages, max_tokens)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏
                if "error" not in result:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–æ–¥–µ
                    if "usage" not in result:
                        result["usage"] = {}
                    result["usage"]["method"] = "vllm"
                    result["usage"]["server"] = self.vllm_url
                    
                    return result
                else:
                    logger.warning(f"vLLM chat_completion –æ—à–∏–±–∫–∞: {result['error']}")
                    
            except Exception as e:
                logger.error(f"vLLM chat_completion –∏—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
        
        # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        return self._fallback_chat_completion(messages, max_tokens)
    
    def _fallback_process(self, image_path: str, prompt: str) -> Dict[str, Any]:
        """Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å"""
        if self.fallback_model:
            try:
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ fallback –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –º–µ—Ç–æ–¥ process_image
                result = self.fallback_model.process_image(image_path, prompt)
                
                if result:
                    return {
                        "success": True,
                        "content": result,
                        "model": "fallback",
                        "processing_time": "N/A",
                        "method": "fallback"
                    }
            except Exception as e:
                logger.error(f"Fallback –æ—à–∏–±–∫–∞: {e}")
        
        return {
            "success": False,
            "error": "vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏ fallback –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞",
            "model": "none",
            "method": "none"
        }
    
    def _fallback_process_pil(self, image: Image.Image, prompt: str) -> Dict[str, Any]:
        """Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if self.fallback_model:
            try:
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ fallback –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                result = self.fallback_model.process_image(image, prompt)
                
                if result:
                    return {
                        "success": True,
                        "content": result,
                        "model": "fallback",
                        "processing_time": "N/A",
                        "method": "fallback"
                    }
            except Exception as e:
                logger.error(f"Fallback PIL –æ—à–∏–±–∫–∞: {e}")
        
        return {
            "success": False,
            "error": "vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏ fallback –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞",
            "model": "none",
            "method": "none"
        }
    
    def _fallback_chat_completion(self, messages: List[Dict], max_tokens: int) -> Dict[str, Any]:
        """Fallback chat completion"""
        if self.fallback_model and hasattr(self.fallback_model, 'chat_completion'):
            try:
                result = self.fallback_model.chat_completion(messages, max_tokens)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–æ–¥–µ
                if "usage" not in result:
                    result["usage"] = {}
                result["usage"]["method"] = "fallback"
                
                return result
                
            except Exception as e:
                logger.error(f"Fallback chat_completion –æ—à–∏–±–∫–∞: {e}")
        
        return {
            "error": "vLLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏ fallback –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç chat_completion",
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": "OCR —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
                },
                "finish_reason": "error"
            }],
            "usage": {
                "method": "none",
                "error": True
            }
        }
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        vllm_available = self.is_vllm_available()
        
        status = {
            "vllm_url": self.vllm_url,
            "vllm_available": vllm_available,
            "fallback_available": self.fallback_model is not None,
            "recommended_method": "vllm" if vllm_available else "fallback"
        }
        
        if vllm_available and self.vllm_client:
            try:
                server_info = self.vllm_client.get_server_info()
                status["vllm_info"] = server_info
            except:
                pass
        
        return status

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ chatvlmllm
_dots_ocr_vllm_instance = None

def get_dots_ocr_vllm_integration(vllm_url: str = "http://localhost:8000") -> DotsOCRVLLMIntegration:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    global _dots_ocr_vllm_instance
    
    if _dots_ocr_vllm_instance is None:
        _dots_ocr_vllm_instance = DotsOCRVLLMIntegration(vllm_url)
    
    return _dots_ocr_vllm_instance

def initialize_dots_ocr_vllm(vllm_url: str = "http://localhost:8000", fallback_model=None) -> bool:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è dots.ocr vLLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    
    Args:
        vllm_url: URL vLLM —Å–µ—Ä–≤–µ—Ä–∞
        fallback_model: Fallback –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, qwen_vl_2b)
        
    Returns:
        True –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –¥–æ—Å—Ç—É–ø–µ–Ω
    """
    integration = get_dots_ocr_vllm_integration(vllm_url)
    
    if fallback_model:
        integration.set_fallback_model(fallback_model)
    
    status = integration.get_status()
    
    logger.info(f"üîß dots.ocr vLLM –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:")
    logger.info(f"   vLLM: {'‚úÖ' if status['vllm_available'] else '‚ùå'}")
    logger.info(f"   Fallback: {'‚úÖ' if status['fallback_available'] else '‚ùå'}")
    logger.info(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –º–µ—Ç–æ–¥: {status['recommended_method']}")
    
    return status['vllm_available'] or status['fallback_available']

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
def test_dots_ocr_vllm_integration():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ dots.ocr vLLM"""
    print("üß™ –¢–ï–°–¢ –ò–ù–¢–ï–ì–†–ê–¶–ò–ò DOTS.OCR VLLM")
    print("=" * 50)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    integration = get_dots_ocr_vllm_integration()
    
    # –°—Ç–∞—Ç—É—Å
    status = integration.get_status()
    print("üìã –°—Ç–∞—Ç—É—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:")
    print(f"   vLLM –¥–æ—Å—Ç—É–ø–µ–Ω: {status['vllm_available']}")
    print(f"   Fallback –¥–æ—Å—Ç—É–ø–µ–Ω: {status['fallback_available']}")
    print(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –º–µ—Ç–æ–¥: {status['recommended_method']}")
    
    if not status['vllm_available'] and not status['fallback_available']:
        print("‚ùå –ù–∏ –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return False
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    print("\nüñºÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    try:
        from PIL import Image, ImageDraw, ImageFont
        
        img = Image.new('RGB', (500, 150), color='white')
        draw = ImageDraw.Draw(img)
        
        try:
            font = ImageFont.truetype("arial.ttf", 28)
        except:
            font = ImageFont.load_default()
        
        draw.text((50, 60), "INTEGRATION TEST", fill='black', font=font)
        img.save('integration_test.png')
        
        print("‚úÖ –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return False
    
    # –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
    print("\nüîç –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞...")
    result = integration.process_image_file('integration_test.png', "Extract all text")
    
    if result["success"]:
        print(f"‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω —á–µ—Ä–µ–∑ {result['method']}")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['content']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['processing_time']}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {result.get('error', 'Unknown')}")
    
    # –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    print("\nüîç –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    pil_result = integration.process_pil_image(img, "Extract all text")
    
    if pil_result["success"]:
        print(f"‚úÖ PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–µ—Ä–µ–∑ {pil_result['method']}")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {pil_result['content']}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PIL: {pil_result.get('error', 'Unknown')}")
    
    # –¢–µ—Å—Ç OpenAI API
    print("\nüîç –¢–µ—Å—Ç OpenAI API...")
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": "integration_test.png"}
                },
                {
                    "type": "text",
                    "text": "Extract all text from this image"
                }
            ]
        }
    ]
    
    api_result = integration.chat_completion(messages)
    
    if "error" not in api_result:
        content = api_result["choices"][0]["message"]["content"]
        method = api_result.get("usage", {}).get("method", "unknown")
        print(f"‚úÖ OpenAI API —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ {method}")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {content}")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI API: {api_result['error']}")
    
    print("\nüéâ –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    return True

if __name__ == "__main__":
    success = test_dots_ocr_vllm_integration()
    
    if success:
        print("\n‚úÖ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ –í CHATVLMLLM!")
    else:
        print("\n‚ùå –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê")