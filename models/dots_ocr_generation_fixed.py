"""
–§–ò–ù–ê–õ–¨–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï DOTS.OCR –° –£–õ–£–ß–®–ï–ù–ù–û–ô –ì–ï–ù–ï–†–ê–¶–ò–ï–ô

–ü—Ä–æ–±–ª–µ–º—ã –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã:
1. Dtype mismatch (BFloat16/Half)
2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤
3. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
"""

import os
import json
import torch
from typing import Any, Dict, List, Optional, Union
from PIL import Image
import traceback

from models.base_model import BaseModel
from utils.logger import logger


class DotsOCRGenerationFixedModel(BaseModel):
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è dots.ocr —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = None
        self.processor = None
        self.max_new_tokens = config.get('max_new_tokens', 512)
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è dots.ocr
        self.prompts = {
            "text_extraction": "Extract all text from this image. Provide only the text content without any formatting or repetition.",
            "minimal": "What text is in this image?",
            "simple": "Read the text.",
            "ocr": "Perform OCR on this image."
        }
    
    def _fix_model_dtypes(self, model):
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –º–æ–¥–µ–ª–∏."""
        try:
            logger.info("üîß –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è...")
            
            # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫ float16
            target_dtype = torch.float16
            converted_count = 0
            
            for name, param in model.named_parameters():
                if param.dtype != target_dtype:
                    logger.debug(f"Converting {name} from {param.dtype} to {target_dtype}")
                    param.data = param.data.to(target_dtype)
                    converted_count += 1
            
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –±—É—Ñ–µ—Ä—ã
            for name, buffer in model.named_buffers():
                if buffer.dtype not in [torch.int64, torch.int32, torch.bool] and buffer.dtype != target_dtype:
                    logger.debug(f"Converting buffer {name} from {buffer.dtype} to {target_dtype}")
                    buffer.data = buffer.data.to(target_dtype)
                    converted_count += 1
            
            logger.info(f"‚úÖ Dtype –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω—ã ({converted_count} –∫–æ–Ω–≤–µ—Ä—Å–∏–π)")
            return model
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å dtype: {e}")
            return model
    
    def load_model(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º."""
        try:
            logger.info(f"Loading dots.ocr with full fixes from {self.model_path}")
            
            from transformers import AutoModelForCausalLM, AutoProcessor
            
            device = self._get_device()
            logger.info(f"Using device: {device}")
            
            # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏
            load_kwargs = self._get_load_kwargs()
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è –≤—Å–µ–≥–æ
            load_kwargs.update({
                'torch_dtype': torch.float16,
                'trust_remote_code': True,
                'attn_implementation': "eager",
                'low_cpu_mem_usage': True,
                'use_safetensors': True
            })
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            logger.info("Loading model weights...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **load_kwargs
            )
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            self.model = self._fix_model_dtypes(self.model)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º video_processor –ø—Ä–æ–±–ª–µ–º—ã
            logger.info("Loading processor...")
            try:
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
                self.processor = AutoProcessor.from_pretrained(
                    self.model_path, 
                    trust_remote_code=True,
                    use_fast=False
                )
            except TypeError as e:
                if "video_processor" in str(e):
                    logger.warning("video_processor error detected, using manual processor loading...")
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –≤—Ä—É—á–Ω—É—é
                    from transformers import AutoImageProcessor, AutoTokenizer
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
                    image_processor = AutoImageProcessor.from_pretrained(
                        self.model_path, 
                        trust_remote_code=True
                    )
                    tokenizer = AutoTokenizer.from_pretrained(
                        self.model_path, 
                        trust_remote_code=True,
                        use_fast=False
                    )
                    
                    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≤—Ä—É—á–Ω—É—é —Å –ø—É—Å—Ç—ã–º video_processor
                    try:
                        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ dots.ocr
                        from transformers.models.auto.processing_auto import AutoProcessor
                        processor_class = AutoProcessor._get_processor_class_from_config(
                            self.model_path, trust_remote_code=True
                        )
                        
                        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å video_processor=None (–±—É–¥–µ—Ç –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ –ø—É—Å—Ç–æ–π)
                        from transformers.models.qwen2_5_vl.processing_qwen2_5_vl import Qwen25VLProcessor
                        
                        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π video_processor
                        class DummyVideoProcessor:
                            def __init__(self):
                                pass
                        
                        dummy_video_processor = DummyVideoProcessor()
                        
                        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                        self.processor = processor_class(
                            image_processor=image_processor,
                            tokenizer=tokenizer,
                            video_processor=dummy_video_processor
                        )
                        
                        logger.info("‚úÖ Processor loaded with manual video_processor fix")
                        
                    except Exception as manual_error:
                        logger.error(f"Manual processor creation failed: {manual_error}")
                        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
                        class SimpleProcessor:
                            def __init__(self, image_processor, tokenizer):
                                self.image_processor = image_processor
                                self.tokenizer = tokenizer
                            
                            def apply_chat_template(self, messages, **kwargs):
                                return self.tokenizer.apply_chat_template(messages, **kwargs)
                            
                            def __call__(self, text=None, images=None, videos=None, **kwargs):
                                # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –≤–∏–¥–µ–æ
                                if text and images:
                                    image_inputs = self.image_processor(images, **kwargs)
                                    text_inputs = self.tokenizer(text, **kwargs)
                                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Ö–æ–¥—ã
                                    return {**image_inputs, **text_inputs}
                                elif text:
                                    return self.tokenizer(text, **kwargs)
                                elif images:
                                    return self.image_processor(images, **kwargs)
                                return {}
                        
                        self.processor = SimpleProcessor(image_processor, tokenizer)
                        logger.info("‚úÖ Using simple processor fallback")
                else:
                    raise e
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º eval
            self.model.eval()
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if hasattr(self.processor, 'tokenizer'):
                tokenizer = self.processor.tokenizer
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    logger.info("Set pad_token to eos_token")
                
                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                if hasattr(tokenizer, 'repetition_penalty'):
                    tokenizer.repetition_penalty = 1.2
                
                logger.info(f"Tokenizer vocab size: {len(tokenizer)}")
                logger.info(f"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
                logger.info(f"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
            
            logger.info("dots.ocr loaded successfully with full fixes")
            
        except Exception as e:
            logger.error(f"Failed to load dots.ocr: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise
    
    def _preprocess_image(self, image: Image.Image) -> Image.Image:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è dots.ocr."""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è dots.ocr (–∫—Ä–∞—Ç–Ω—ã–π 14)
            max_size = 1120  # 80 * 14 - —É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = tuple(int(dim * ratio) for dim in image.size)
                # –î–µ–ª–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∫—Ä–∞—Ç–Ω—ã–º–∏ 14
                new_size = (
                    ((new_size[0] + 13) // 14) * 14,
                    ((new_size[1] + 13) // 14) * 14
                )
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                logger.info(f"Resized image to {new_size}")
            
            return image
            
        except Exception as e:
            logger.warning(f"Image preprocessing failed: {e}")
            return image
    
    def _safe_generate_improved(self, inputs: Dict, prompt_type: str = "simple") -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π."""
        try:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ float16
            device = next(self.model.parameters()).device
            target_dtype = torch.float16
            
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            for key, value in inputs.items():
                if isinstance(value, torch.Tensor) and value.dtype not in [torch.int64, torch.int32, torch.bool]:
                    if value.dtype != target_dtype:
                        logger.debug(f"Converting input {key} from {value.dtype} to {target_dtype}")
                        inputs[key] = value.to(target_dtype)
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                if isinstance(value, torch.Tensor):
                    inputs[key] = value.to(device)
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            generation_kwargs = {
                'max_new_tokens': 256,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                'min_new_tokens': 1,    # –ú–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω
                'do_sample': True,      # –í–∫–ª—é—á–∞–µ–º sampling –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                'temperature': 0.7,     # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
                'top_p': 0.9,          # Nucleus sampling
                'top_k': 50,           # Top-k sampling
                'repetition_penalty': 1.3,  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                'no_repeat_ngram_size': 3,   # –ó–∞–ø—Ä–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è 3-–≥—Ä–∞–º–º
                'pad_token_id': self.processor.tokenizer.pad_token_id,
                'eos_token_id': self.processor.tokenizer.eos_token_id,
                'use_cache': True,
                'output_attentions': False,
                'output_hidden_states': False,
                'early_stopping': True  # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            }
            
            # –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if prompt_type in ["minimal", "simple"]:
                generation_kwargs.update({
                    'max_new_tokens': 128,
                    'temperature': 0.3,
                    'repetition_penalty': 1.5
                })
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º autocast –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                with torch.autocast(device_type='cuda', dtype=target_dtype, enabled=True):
                    generated_ids = self.model.generate(
                        **inputs,
                        **generation_kwargs
                    )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=True
            )[0]
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            output_text = output_text.strip()
            
            # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, !!!!!!)
            import re
            output_text = re.sub(r'(.)\1{5,}', r'\1', output_text)  # –£–¥–∞–ª—è–µ–º 6+ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            output_text = re.sub(r'\s+', ' ', output_text)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
            
            return output_text
            
        except Exception as e:
            logger.error(f"Generation failed: {e}")
            raise
    
    def _create_messages(self, image: Image.Image, prompt: str) -> List[Dict]:
        """–°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è dots.ocr."""
        return [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
    
    def _process_with_prompt(self, image: Image.Image, prompt: str, prompt_type: str = "simple") -> str:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º."""
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            processed_image = self._preprocess_image(image)
            
            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = self._create_messages(processed_image, prompt)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞
            try:
                text = self.processor.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
            except Exception as e:
                logger.warning(f"Chat template failed: {e}, using simple text")
                text = prompt
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            try:
                from qwen_vl_utils import process_vision_info
                image_inputs, video_inputs = process_vision_info(messages)
            except ImportError:
                logger.warning("qwen_vl_utils –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É")
                image_inputs = [processed_image]
                video_inputs = None
            except Exception as e:
                logger.warning(f"process_vision_info failed: {e}, using direct processing")
                image_inputs = [processed_image]
                video_inputs = None
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            try:
                inputs = self.processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt"
                )
            except Exception as e:
                logger.warning(f"Processor call failed: {e}, trying simplified approach")
                # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
                try:
                    inputs = self.processor(
                        text=[text],
                        images=image_inputs,
                        padding=True,
                        return_tensors="pt"
                    )
                except Exception as e2:
                    logger.error(f"Simplified processor call also failed: {e2}")
                    # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - —Ä—É—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    if hasattr(self.processor, 'tokenizer') and hasattr(self.processor, 'image_processor'):
                        text_inputs = self.processor.tokenizer(
                            [text], 
                            padding=True, 
                            return_tensors="pt"
                        )
                        image_inputs_processed = self.processor.image_processor(
                            image_inputs, 
                            return_tensors="pt"
                        )
                        inputs = {**text_inputs, **image_inputs_processed}
                    else:
                        raise e2
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            output_text = self._safe_generate_improved(inputs, prompt_type)
            
            return output_text
            
        except Exception as e:
            logger.error(f"Processing failed: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return f"[Processing error: {e}]"
    
    def process_image(self, image: Image.Image, prompt: Optional[str] = None, 
                     mode: str = "text_extraction") -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        if self.model is None:
            raise RuntimeError("Model not loaded")
        
        try:
            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç
            if prompt is None:
                prompt = self.prompts.get(mode, self.prompts["text_extraction"])
            
            logger.info(f"Processing with mode: {mode}")
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if image is None:
                raise ValueError("Image is None")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            result = self._process_with_prompt(image, prompt, mode)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if not result or result.strip() == "" or len(result.strip()) < 3:
                logger.warning("Poor result, trying alternative prompt")
                result = self._process_with_prompt(image, self.prompts["minimal"], "minimal")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
            if result and len(set(result.replace(' ', ''))) < 3:  # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
                logger.warning("Detected repetitive output, trying OCR prompt")
                result = self._process_with_prompt(image, self.prompts["ocr"], "simple")
            
            logger.info("Processing completed successfully")
            
            return result if result and len(result.strip()) > 2 else "[dots.ocr: No meaningful text detected]"
            
        except Exception as e:
            logger.error(f"Error processing image: {e}")
            return f"[dots.ocr error: {e}]"
    
    def extract_text(self, image: Image.Image) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç."""
        return self.process_image(image, mode="text_extraction")
    
    def chat(self, image: Image.Image, prompt: str, **kwargs) -> str:
        """–ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é."""
        return self.process_image(image, prompt=prompt, mode="custom")
    
    def extract_table(self, image: Image.Image) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü—ã."""
        return self.process_image(image, prompt="Extract table content from this image", mode="simple")
    
    def parse_document(self, image: Image.Image) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç."""
        try:
            result = self.process_image(image, mode="text_extraction")
            
            return {
                "success": True,
                "text": result,
                "method": "generation_fixed_parsing"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "text": f"Error: {e}"
            }
    
    def unload(self) -> None:
        """–í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å."""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–∞
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                except Exception as e:
                    logger.warning(f"Warning during CUDA cleanup: {e}")
            
            logger.info("dots.ocr unloaded successfully")
            
        except Exception as e:
            logger.warning(f"Warning during model unload: {e}")