#!/usr/bin/env python3
"""
dots.ocr –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ chatvlmllm
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è RTX 5070 Ti Blackwell –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
"""

import torch
import time
import logging
from transformers import AutoModelForCausalLM, AutoProcessor
from PIL import Image
import base64
import io
import requests
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class DotsOCRChatVLM:
    """dots.ocr –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è chatvlmllm –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.model_name = "rednote-hilab/dots.ocr"
        self.is_loaded = False
        
    def load_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
        try:
            start_time = time.time()
            
            logger.info("üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º dots.ocr –¥–ª—è chatvlmllm...")
            
            # Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 5070 Ti
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.enable_flash_sdp(True)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.bfloat16,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell
                attn_implementation="eager",  # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–ª—è RTX 5070 Ti
                device_map="auto",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            load_time = time.time() - start_time
            vram_used = torch.cuda.memory_allocated() / 1024**3
            
            logger.info(f"‚úÖ dots.ocr –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
            logger.info(f"‚úÖ VRAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {vram_used:.2f}GB")
            logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–∞ –¥–ª—è chatvlmllm –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
            
            self.is_loaded = True
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ dots.ocr: {e}")
            self.is_loaded = False
            return False
    
    def _process_image_from_url(self, image_url: str) -> Optional[Image.Image]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ URL –∏–ª–∏ base64"""
        try:
            if image_url.startswith('data:image'):
                # Base64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                header, data = image_url.split(',', 1)
                image_data = base64.b64decode(data)
                return Image.open(io.BytesIO(image_data)).convert('RGB')
            elif image_url.startswith('http'):
                # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                response = requests.get(image_url)
                return Image.open(io.BytesIO(response.content)).convert('RGB')
            else:
                # –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
                return Image.open(image_url).convert('RGB')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None
    
    def chat_completion(self, messages: List[Dict], max_tokens: int = 2048) -> Dict[str, Any]:
        """
        OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API –¥–ª—è chatvlmllm
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –û—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI API
        """
        if not self.is_loaded:
            return {
                "error": "dots.ocr model not loaded",
                "choices": [{
                    "message": {
                        "role": "assistant",
                        "content": "Model not available"
                    }
                }]
            }
        
        try:
            start_time = time.time()
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ messages
            image_content = None
            text_content = "Extract all text from this image"
            
            for message in messages:
                if message.get("role") == "user":
                    content = message.get("content", [])
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ content
                    if isinstance(content, str):
                        text_content = content
                    elif isinstance(content, list):
                        for item in content:
                            if isinstance(item, dict):
                                if item.get("type") == "image_url":
                                    image_url = item.get("image_url", {})
                                    if isinstance(image_url, dict):
                                        url = image_url.get("url")
                                    else:
                                        url = image_url
                                    
                                    if url:
                                        image_content = self._process_image_from_url(url)
                                        
                                elif item.get("type") == "text":
                                    text_content = item.get("text", text_content)
            
            if not image_content:
                return {
                    "error": "No image provided",
                    "choices": [{
                        "message": {
                            "role": "assistant", 
                            "content": "Please provide an image for OCR processing"
                        }
                    }]
                }
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ dots.ocr
            result = self.process_image(image_content, text_content, max_tokens)
            
            processing_time = time.time() - start_time
            
            return {
                "choices": [{
                    "message": {
                        "role": "assistant",
                        "content": result or "No text detected in the image"
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "processing_time": f"{processing_time:.3f}s",
                    "model": "dots.ocr"
                }
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ chat_completion: {e}")
            return {
                "error": str(e),
                "choices": [{
                    "message": {
                        "role": "assistant",
                        "content": f"Error processing request: {str(e)}"
                    }
                }]
            }
    
    def process_image(self, image: Image.Image, prompt: str, max_tokens: int = 2048) -> Optional[str]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ dots.ocr
        
        Args:
            image: PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            prompt: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not self.model or not self.processor:
            logger.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None
            
        try:
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è dots.ocr (–±–µ–∑ qwen_vl_utils)
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }]
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ chat template
            text = self.processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # –ü—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ qwen_vl_utils
            inputs = self.processor(
                text=[text],
                images=[image],
                padding=True,
                return_tensors="pt",
            ).to("cuda")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è Blackwell
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.1,
                    top_p=0.9,
                    use_cache=True,
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ—Ç–∏–≤ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            result = output_text.strip()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if not result or result.lower() in ['', 'none', 'no text']:
                logger.warning("dots.ocr –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                return None
                
            logger.info(f"‚úÖ dots.ocr –æ–±—Ä–∞–±–æ—Ç–∞–ª–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        if self.model:
            del self.model
            self.model = None
        if self.processor:
            del self.processor
            self.processor = None
        
        torch.cuda.empty_cache()
        self.is_loaded = False
        logger.info("üßπ dots.ocr –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ chatvlmllm
_dots_ocr_instance = None

def get_dots_ocr_instance() -> DotsOCRChatVLM:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ dots.ocr"""
    global _dots_ocr_instance
    
    if _dots_ocr_instance is None:
        _dots_ocr_instance = DotsOCRChatVLM()
        
    return _dots_ocr_instance

def initialize_dots_ocr() -> bool:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è dots.ocr –¥–ª—è chatvlmllm"""
    instance = get_dots_ocr_instance()
    return instance.load_model()

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
def test_chatvlm_integration():
    """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å chatvlmllm"""
    print("üß™ –¢–ï–°–¢ –ò–ù–¢–ï–ì–†–ê–¶–ò–ò DOTS.OCR –° CHATVLMLLM")
    print("=" * 60)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
    print(f"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}")
    print(f"üêç PyTorch: {torch.__version__}")
    print(f"‚ö° CUDA: {torch.version.cuda}")
    print()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    if not initialize_dots_ocr():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å dots.ocr")
        return False
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ chatvlmllm/OpenAI
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": "test_document.png"}
                },
                {
                    "type": "text", 
                    "text": "Extract all text from this document in Russian and English"
                }
            ]
        }
    ]
    
    # –¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
    instance = get_dots_ocr_instance()
    result = instance.chat_completion(messages, max_tokens=1024)
    
    print("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"‚úÖ –°—Ç–∞—Ç—É—Å: {'–£—Å–ø–µ—Ö' if 'error' not in result else '–û—à–∏–±–∫–∞'}")
    
    if 'error' not in result:
        content = result['choices'][0]['message']['content']
        print(f"üìù –¢–µ–∫—Å—Ç: {content[:200]}...")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result.get('usage', {}).get('processing_time', 'N/A')}")
        print("üéâ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –†–ê–ë–û–¢–ê–ï–¢!")
        return True
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
        return False

if __name__ == "__main__":
    success = test_chatvlm_integration()
    if success:
        print("\nüöÄ DOTS.OCR –ì–û–¢–û–í–ê –î–õ–Ø –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –í CHATVLMLLM!")
    else:
        print("\n‚ùå –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê")