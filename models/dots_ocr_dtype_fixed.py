"""
–§–ò–ù–ê–õ–¨–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï DTYPE –ü–†–û–ë–õ–ï–ú–´ –í DOTS.OCR

–ü—Ä–æ–±–ª–µ–º–∞: Input type (struct c10::BFloat16) and bias type (struct c10::Half) should be the same
–†–µ—à–µ–Ω–∏–µ: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–µ—Å–æ–≤ –∫ –æ–¥–Ω–æ–º—É —Ç–∏–ø—É –¥–∞–Ω–Ω—ã—Ö (float16)
"""

import os
import json
import torch
from typing import Any, Dict, List, Optional, Union
from PIL import Image
import traceback

from models.base_model import BaseModel
from utils.logger import logger


class DotsOCRDtypeFixedModel(BaseModel):
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è dots.ocr —Å —Ä–µ—à–µ–Ω–∏–µ–º dtype –ø—Ä–æ–±–ª–µ–º—ã."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = None
        self.processor = None
        self.max_new_tokens = config.get('max_new_tokens', 1024)
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        self.prompts = {
            "text_extraction": "Extract all text from this image.",
            "minimal": "What text do you see?",
            "simple": "Read the text in this image."
        }
    
    def _fix_model_dtypes(self, model):
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –º–æ–¥–µ–ª–∏."""
        try:
            logger.info("üîß –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è...")
            
            # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫ float16
            target_dtype = torch.float16
            
            for name, param in model.named_parameters():
                if param.dtype != target_dtype:
                    logger.debug(f"Converting {name} from {param.dtype} to {target_dtype}")
                    param.data = param.data.to(target_dtype)
            
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –±—É—Ñ–µ—Ä—ã
            for name, buffer in model.named_buffers():
                if buffer.dtype not in [torch.int64, torch.int32, torch.bool] and buffer.dtype != target_dtype:
                    logger.debug(f"Converting buffer {name} from {buffer.dtype} to {target_dtype}")
                    buffer.data = buffer.data.to(target_dtype)
            
            logger.info("‚úÖ Dtype –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
            return model
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å dtype: {e}")
            return model
    
    def load_model(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º dtype –ø—Ä–æ–±–ª–µ–º."""
        try:
            logger.info(f"Loading dots.ocr with dtype fixes from {self.model_path}")
            
            from transformers import AutoModelForCausalLM, AutoProcessor
            
            device = self._get_device()
            logger.info(f"Using device: {device}")
            
            # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏
            load_kwargs = self._get_load_kwargs()
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è –≤—Å–µ–≥–æ
            load_kwargs.update({
                'torch_dtype': torch.float16,  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ float16
                'trust_remote_code': True,
                'attn_implementation': "eager",
                'low_cpu_mem_usage': True,
                'use_safetensors': True
            })
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            logger.info("Loading model weights...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **load_kwargs
            )
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            self.model = self._fix_model_dtypes(self.model)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            logger.info("Loading processor...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_path, 
                trust_remote_code=True,
                use_fast=False
            )
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º eval
            self.model.eval()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            if hasattr(self.processor, 'tokenizer'):
                if self.processor.tokenizer.pad_token is None:
                    self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token
                    logger.info("Set pad_token to eos_token")
            
            logger.info("dots.ocr loaded successfully with dtype fixes")
            
        except Exception as e:
            logger.error(f"Failed to load dots.ocr: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise
    
    def _preprocess_image(self, image: Image.Image) -> Image.Image:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è dots.ocr."""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è dots.ocr (–∫—Ä–∞—Ç–Ω—ã–π 14)
            max_size = 1400  # 100 * 14
            
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = tuple(int(dim * ratio) for dim in image.size)
                # –î–µ–ª–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∫—Ä–∞—Ç–Ω—ã–º–∏ 14
                new_size = (
                    ((new_size[0] + 13) // 14) * 14,
                    ((new_size[1] + 13) // 14) * 14
                )
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                logger.info(f"Resized image to {new_size}")
            
            return image
            
        except Exception as e:
            logger.warning(f"Image preprocessing failed: {e}")
            return image
    
    def _safe_generate_with_dtype_fix(self, inputs: Dict, prompt_type: str = "simple") -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º dtype."""
        try:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ float16
            device = next(self.model.parameters()).device
            target_dtype = torch.float16
            
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            for key, value in inputs.items():
                if isinstance(value, torch.Tensor) and value.dtype not in [torch.int64, torch.int32, torch.bool]:
                    if value.dtype != target_dtype:
                        logger.debug(f"Converting input {key} from {value.dtype} to {target_dtype}")
                        inputs[key] = value.to(target_dtype)
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                if isinstance(value, torch.Tensor):
                    inputs[key] = value.to(device)
            
            # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            generation_kwargs = {
                'max_new_tokens': 512,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                'do_sample': False,
                'pad_token_id': self.processor.tokenizer.eos_token_id,
                'eos_token_id': self.processor.tokenizer.eos_token_id,
                'use_cache': True,
                'output_attentions': False,
                'output_hidden_states': False
            }
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º autocast –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                with torch.autocast(device_type='cuda', dtype=target_dtype, enabled=True):
                    generated_ids = self.model.generate(
                        **inputs,
                        **generation_kwargs
                    )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=True
            )[0]
            
            return output_text.strip()
            
        except Exception as e:
            logger.error(f"Generation failed: {e}")
            raise
    
    def _create_messages(self, image: Image.Image, prompt: str) -> List[Dict]:
        """–°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è dots.ocr."""
        return [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
    
    def _process_with_prompt(self, image: Image.Image, prompt: str, prompt_type: str = "simple") -> str:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º."""
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            processed_image = self._preprocess_image(image)
            
            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = self._create_messages(processed_image, prompt)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            try:
                from qwen_vl_utils import process_vision_info
                image_inputs, video_inputs = process_vision_info(messages)
            except ImportError:
                logger.warning("qwen_vl_utils –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É")
                image_inputs = [processed_image]
                video_inputs = None
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º dtype
            output_text = self._safe_generate_with_dtype_fix(inputs, prompt_type)
            
            return output_text
            
        except Exception as e:
            logger.error(f"Processing failed: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return f"[Processing error: {e}]"
    
    def process_image(self, image: Image.Image, prompt: Optional[str] = None, 
                     mode: str = "text_extraction") -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        if self.model is None:
            raise RuntimeError("Model not loaded")
        
        try:
            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç
            if prompt is None:
                prompt = self.prompts.get(mode, self.prompts["text_extraction"])
            
            logger.info(f"Processing with mode: {mode}")
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if image is None:
                raise ValueError("Image is None")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            result = self._process_with_prompt(image, prompt, mode)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if not result or result.strip() == "":
                logger.warning("Empty result, trying minimal prompt")
                result = self._process_with_prompt(image, self.prompts["minimal"], "minimal")
            
            logger.info("Processing completed successfully")
            
            return result if result else "[dots.ocr: No text detected]"
            
        except Exception as e:
            logger.error(f"Error processing image: {e}")
            return f"[dots.ocr error: {e}]"
    
    def extract_text(self, image: Image.Image) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç."""
        return self.process_image(image, mode="text_extraction")
    
    def chat(self, image: Image.Image, prompt: str, **kwargs) -> str:
        """–ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é."""
        return self.process_image(image, prompt=prompt, mode="custom")
    
    def extract_table(self, image: Image.Image) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü—ã."""
        return self.process_image(image, prompt="Extract table content from this image", mode="simple")
    
    def parse_document(self, image: Image.Image) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç."""
        try:
            result = self.process_image(image, mode="text_extraction")
            
            return {
                "success": True,
                "text": result,
                "method": "dtype_fixed_parsing"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "text": f"Error: {e}"
            }
    
    def unload(self) -> None:
        """–í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å."""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–∞
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                except Exception as e:
                    logger.warning(f"Warning during CUDA cleanup: {e}")
            
            logger.info("dots.ocr unloaded successfully")
            
        except Exception as e:
            logger.warning(f"Warning during model unload: {e}")