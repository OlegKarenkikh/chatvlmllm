"""
DOTS.OCR ULTIMATE FIX - –û–±—Ö–æ–¥ –ø—Ä–æ–±–ª–µ–º—ã img_mask

–≠—Ç–∞ –≤–µ—Ä—Å–∏—è –æ–±—Ö–æ–¥–∏—Ç –ø—Ä–æ–±–ª–µ–º—É —Å img_mask, –∫–æ—Ç–æ—Ä–∞—è –æ–±–Ω—É–ª—è–µ—Ç—Å—è –≤ forward pass –º–æ–¥–µ–ª–∏.
"""

import os
import json
import torch
from typing import Any, Dict, List, Optional, Union
from PIL import Image
import traceback

from models.base_model import BaseModel
from utils.logger import logger


class DotsOCRUltimateFixModel(BaseModel):
    """dots.ocr —Å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = None
        self.processor = None
        self.max_new_tokens = config.get('max_new_tokens', 512)
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è dots.ocr
        self.prompts = {
            "text_extraction": "Extract all text from this image.",
            "minimal": "What text is in this image?",
            "simple": "Read the text.",
            "ocr": "Perform OCR on this image."
        }
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å dots.ocr —Å –æ–±—Ö–æ–¥–æ–º –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º."""
        try:
            logger.info("Loading dots.ocr with ultimate fix from rednote-hilab/dots.ocr")
            
            from transformers import AutoModel, AutoImageProcessor, AutoTokenizer
            import torch
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if torch.cuda.is_available():
                device = "cuda"
                logger.info(f"GPU detected: {torch.cuda.get_device_name(0)} with {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB VRAM")
            else:
                device = "cpu"
                logger.info("Using CPU")
            
            logger.info(f"Using device: {device}")
            logger.info("FORCING GPU usage with device_map='auto'")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            logger.info("Loading model weights...")
            self.model = AutoModel.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True,
                attn_implementation="eager"  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º eager attention
            )
            
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            logger.info("üîß –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è...")
            dtype_fixes = 0
            for name, param in self.model.named_parameters():
                if param.dtype != torch.float16 and device == "cuda":
                    param.data = param.data.to(torch.float16)
                    dtype_fixes += 1
            logger.info(f"‚úÖ Dtype –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω—ã ({dtype_fixes} –∫–æ–Ω–≤–µ—Ä—Å–∏–π)")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            logger.info("Loading processor components...")
            
            # Image processor
            image_processor = AutoImageProcessor.from_pretrained(
                self.model_path, 
                trust_remote_code=True
            )
            
            # Tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                self.model_path, 
                trust_remote_code=True,
                use_fast=False
            )
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –æ–±—Ö–æ–¥–æ–º img_mask –ø—Ä–æ–±–ª–µ–º—ã
            class UltimateDotsOCRProcessor:
                def __init__(self, image_processor, tokenizer):
                    self.image_processor = image_processor
                    self.tokenizer = tokenizer
                
                def apply_chat_template(self, messages, **kwargs):
                    # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
                    if isinstance(messages, list) and len(messages) > 0:
                        message = messages[0]
                        if isinstance(message, dict) and 'content' in message:
                            content = message['content']
                            if isinstance(content, list):
                                for item in content:
                                    if isinstance(item, dict) and item.get('type') == 'text':
                                        return item.get('text', '')
                            elif isinstance(content, str):
                                return content
                    return kwargs.get('text', '')
                
                def __call__(self, text=None, images=None, videos=None, **kwargs):
                    result = {}
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
                    if text is not None:
                        try:
                            text_inputs = self.tokenizer(text, return_tensors='pt', **kwargs)
                            result.update(text_inputs)
                        except Exception as e:
                            logger.warning(f"Tokenizer failed: {e}")
                            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                            result['input_ids'] = torch.tensor([[1, 2, 3]])
                            result['attention_mask'] = torch.tensor([[1, 1, 1]])
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if images is not None:
                        try:
                            image_inputs = self.image_processor(images, return_tensors='pt')
                            result.update(image_inputs)
                            
                            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã
                            if 'pixel_values' in image_inputs:
                                pixel_values = image_inputs['pixel_values']
                                
                                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
                                if len(pixel_values.shape) == 2:
                                    batch_size = 1
                                    total_patches = pixel_values.shape[0]
                                elif len(pixel_values.shape) == 3:
                                    batch_size = pixel_values.shape[0] 
                                    total_patches = pixel_values.shape[1]
                                else:
                                    batch_size = 1
                                    total_patches = 256
                                
                                # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
                                device = pixel_values.device if hasattr(pixel_values, 'device') else 'cpu'
                                
                                # –ï—Å–ª–∏ image_grid_thw –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –µ–≥–æ
                                if 'image_grid_thw' not in result:
                                    # –î–ª—è 588 –ø–∞—Ç—á–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º 21x28, –¥–ª—è 256 - 16x16
                                    if total_patches == 588:
                                        h_patches, w_patches = 21, 28
                                    elif total_patches == 256:
                                        h_patches, w_patches = 16, 16
                                    else:
                                        # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
                                        import math
                                        sqrt_patches = int(math.sqrt(total_patches))
                                        h_patches = sqrt_patches
                                        w_patches = total_patches // sqrt_patches
                                        if h_patches * w_patches != total_patches:
                                            h_patches = total_patches
                                            w_patches = 1
                                    
                                    result['image_grid_thw'] = torch.tensor([[1, h_patches, w_patches]], dtype=torch.long, device=device)
                                
                                # –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º img_mask
                                result['img_mask'] = torch.ones(batch_size, total_patches, dtype=torch.bool, device=device)
                                
                                logger.info(f"üîß Ultimate fix: img_mask shape={result['img_mask'].shape}, sum={result['img_mask'].sum()}, device={result['img_mask'].device}")
                                
                        except Exception as e:
                            logger.warning(f"Image processor failed: {e}")
                    
                    return result
                
                def batch_decode(self, *args, **kwargs):
                    return self.tokenizer.batch_decode(*args, **kwargs)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            self.processor = UltimateDotsOCRProcessor(image_processor, tokenizer)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º eval
            self.model.eval()
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            if hasattr(self.processor, 'tokenizer'):
                tokenizer = self.processor.tokenizer
                
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    logger.info("Set pad_token to eos_token")
                
                logger.info(f"Tokenizer vocab size: {len(tokenizer)}")
                logger.info(f"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
                logger.info(f"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
            
            logger.info("‚úÖ dots.ocr loaded successfully with ultimate fix")
            
        except Exception as e:
            logger.error(f"Failed to load dots.ocr: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise
    
    def _preprocess_image(self, image: Image.Image) -> Image.Image:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è dots.ocr."""
        try:
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è dots.ocr
            max_size = 1120
            
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = tuple(int(dim * ratio) for dim in image.size)
                new_size = (
                    ((new_size[0] + 13) // 14) * 14,
                    ((new_size[1] + 13) // 14) * 14
                )
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                logger.info(f"Resized image to {new_size}")
            
            return image
            
        except Exception as e:
            logger.warning(f"Image preprocessing failed: {e}")
            return image
    
    def _safe_generate_ultimate(self, inputs: dict, prompt_type: str = "text_extraction") -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–±—Ö–æ–¥–æ–º img_mask –ø—Ä–æ–±–ª–µ–º—ã."""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º img_mask –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
            if 'img_mask' in inputs:
                img_mask = inputs['img_mask']
                if img_mask.sum() == 0:
                    logger.warning("üîß img_mask is zero, recreating...")
                    
                    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º img_mask –Ω–∞ –æ—Å–Ω–æ–≤–µ pixel_values
                    if 'pixel_values' in inputs:
                        pixel_values = inputs['pixel_values']
                        if len(pixel_values.shape) == 2:
                            batch_size = 1
                            total_patches = pixel_values.shape[0]
                        elif len(pixel_values.shape) == 3:
                            batch_size = pixel_values.shape[0]
                            total_patches = pixel_values.shape[1]
                        else:
                            batch_size = 1
                            total_patches = 256
                        
                        device = pixel_values.device
                        inputs['img_mask'] = torch.ones(batch_size, total_patches, dtype=torch.bool, device=device)
                        logger.info(f"üîß Recreated img_mask: sum={inputs['img_mask'].sum()}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=self.max_new_tokens,
                    do_sample=False,
                    temperature=0.1,
                    pad_token_id=self.processor.tokenizer.pad_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    use_cache=True
                )
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                input_length = inputs['input_ids'].shape[1] if 'input_ids' in inputs else 0
                generated_text = self.processor.batch_decode(
                    generated_ids[:, input_length:], 
                    skip_special_tokens=True
                )[0]
                
                return generated_text.strip()
                
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Generation failed: {error_msg}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
            if "img_mask" in error_msg:
                return f"[img_mask error: {error_msg}]"
            elif "vision_embeddings" in error_msg:
                return f"[vision_embeddings error: {error_msg}]"
            else:
                return f"[Processing error: {error_msg}]"
    
    def _process_with_prompt(self, image: Image.Image, prompt: str, prompt_type: str = "text_extraction") -> str:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø—Ä–æ–º–ø—Ç–æ–º."""
        try:
            logger.info(f"Processing with mode: {prompt_type}")
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            processed_image = self._preprocess_image(image)
            
            # –°–æ–∑–¥–∞–µ–º inputs
            try:
                # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å chat template
                messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
                formatted_prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)
            except Exception as e:
                logger.warning(f"Chat template failed: {e}, using simple text")
                formatted_prompt = prompt
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            inputs = self.processor(
                text=formatted_prompt,
                images=processed_image,
                return_tensors="pt"
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            output_text = self._safe_generate_ultimate(inputs, prompt_type)
            
            logger.info("Processing completed successfully")
            return output_text
            
        except Exception as e:
            logger.error(f"Processing failed: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return f"[Processing error: {e}]"
    
    def process_image(self, image: Union[str, Image.Image], prompt: str = None) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        if self.model is None:
            raise RuntimeError("Model not loaded")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Ç—å
        if isinstance(image, str):
            image = Image.open(image)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if prompt is None:
            prompt = self.prompts["text_extraction"]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        return self._process_with_prompt(image, prompt, "text_extraction")
