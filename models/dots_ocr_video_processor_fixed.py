"""
DOTS.OCR –° –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï–ú VIDEO_PROCESSOR –ü–†–û–ë–õ–ï–ú–´

–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –æ—à–∏–±–∫—É:
TypeError: Received a NoneType for argument video_processor, but a BaseVideoProcessor was expected.

–≠—Ç–∞ –æ—à–∏–±–∫–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ dots.ocr —Å –Ω–æ–≤—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ transformers,
–≥–¥–µ Qwen2.5-VL –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç—Ä–µ–±—É–µ—Ç video_processor –ø–∞—Ä–∞–º–µ—Ç—Ä.
"""

import os
import json
import torch
from typing import Any, Dict, List, Optional, Union
from PIL import Image
import traceback

from models.base_model import BaseModel
from utils.logger import logger


class DotsOCRVideoProcessorFixedModel(BaseModel):
    """dots.ocr —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º video_processor –ø—Ä–æ–±–ª–µ–º—ã."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.model = None
        self.processor = None
        self.max_new_tokens = config.get('max_new_tokens', 512)
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è dots.ocr
        self.prompts = {
            "text_extraction": "Extract all text from this image. Provide only the text content without any formatting or repetition.",
            "minimal": "What text is in this image?",
            "simple": "Read the text.",
            "ocr": "Perform OCR on this image."
        }
    
    def _create_video_processor_mock(self):
        """–°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è video_processor."""
        try:
            from transformers.models.qwen2_5_vl.processing_qwen2_5_vl import Qwen25VLVideoProcessor
            
            # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∑–∞–≥–ª—É—à–∫—É
            class VideoProcessorMock:
                def __init__(self):
                    pass
                
                def __call__(self, *args, **kwargs):
                    return None
                
                def preprocess(self, *args, **kwargs):
                    return None
            
            return VideoProcessorMock()
            
        except ImportError:
            # –ï—Å–ª–∏ Qwen25VLVideoProcessor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –∑–∞–≥–ª—É—à–∫—É
            class SimpleVideoProcessorMock:
                def __init__(self):
                    pass
                
                def __call__(self, *args, **kwargs):
                    return None
            
            return SimpleVideoProcessorMock()
    
    def _fix_model_dtypes(self, model):
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –º–æ–¥–µ–ª–∏."""
        try:
            logger.info("üîß –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è...")
            
            # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫ float16
            target_dtype = torch.float16
            converted_count = 0
            
            for name, param in model.named_parameters():
                if param.dtype != target_dtype:
                    logger.debug(f"Converting {name} from {param.dtype} to {target_dtype}")
                    param.data = param.data.to(target_dtype)
                    converted_count += 1
            
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –±—É—Ñ–µ—Ä—ã
            for name, buffer in model.named_buffers():
                if buffer.dtype not in [torch.int64, torch.int32, torch.bool] and buffer.dtype != target_dtype:
                    logger.debug(f"Converting buffer {name} from {buffer.dtype} to {target_dtype}")
                    buffer.data = buffer.data.to(target_dtype)
                    converted_count += 1
            
            logger.info(f"‚úÖ Dtype –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω—ã ({converted_count} –∫–æ–Ω–≤–µ—Ä—Å–∏–π)")
            return model
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å dtype: {e}")
            return model
    
    def load_model(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º video_processor –ø—Ä–æ–±–ª–µ–º—ã."""
        try:
            logger.info(f"Loading dots.ocr with video_processor fix from {self.model_path}")
            
            from transformers import AutoModelForCausalLM, AutoProcessor, AutoImageProcessor, AutoTokenizer
            
            device = self._get_device()
            logger.info(f"Using device: {device}")
            
            # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏
            load_kwargs = self._get_load_kwargs()
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è –≤—Å–µ–≥–æ
            load_kwargs.update({
                'torch_dtype': torch.float16,
                'trust_remote_code': True,
                'attn_implementation': "eager",
                'low_cpu_mem_usage': True,
                'use_safetensors': True
            })
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            logger.info("Loading model weights...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **load_kwargs
            )
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
            self.model = self._fix_model_dtypes(self.model)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º video_processor –ø—Ä–æ–±–ª–µ–º—ã
            logger.info("Loading processor with video_processor fix...")
            
            try:
                # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
                self.processor = AutoProcessor.from_pretrained(
                    self.model_path, 
                    trust_remote_code=True,
                    use_fast=False
                )
                logger.info("‚úÖ Standard processor loading successful")
                
            except TypeError as e:
                if "video_processor" in str(e):
                    logger.warning("üîß video_processor error detected, applying fix...")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
                    image_processor = AutoImageProcessor.from_pretrained(
                        self.model_path, 
                        trust_remote_code=True
                    )
                    tokenizer = AutoTokenizer.from_pretrained(
                        self.model_path, 
                        trust_remote_code=True,
                        use_fast=False
                    )
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è video_processor
                    video_processor_mock = self._create_video_processor_mock()
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    try:
                        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π video processor
                        from transformers.image_processing_utils import BaseImageProcessor
                        
                        class DummyVideoProcessor(BaseImageProcessor):
                            """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è video_processor, –Ω–∞—Å–ª–µ–¥—É—é—â–∞—è –æ—Ç BaseImageProcessor."""
                            
                            def __init__(self):
                                super().__init__()
                            
                            def preprocess(self, videos, **kwargs):
                                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –≤–∏–¥–µ–æ
                                return {}
                            
                            def __call__(self, videos=None, **kwargs):
                                if videos is None:
                                    return {}
                                return self.preprocess(videos, **kwargs)
                        
                        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
                        dummy_video_processor = DummyVideoProcessor()
                        
                        # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–∞–ø—Ä—è–º—É—é
                        class DotsOCRProcessorFixed:
                            def __init__(self, image_processor, tokenizer, video_processor=None):
                                self.image_processor = image_processor
                                self.tokenizer = tokenizer
                                self.video_processor = video_processor or dummy_video_processor
                            
                            def apply_chat_template(self, messages, **kwargs):
                                return self.tokenizer.apply_chat_template(messages, **kwargs)
                            
                            def __call__(self, text=None, images=None, videos=None, **kwargs):
                                result = {}
                                
                                if text is not None:
                                    text_inputs = self.tokenizer(text, **kwargs)
                                    result.update(text_inputs)
                                
                                if images is not None:
                                    # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è dots.ocr
                                    image_inputs = self.image_processor(
                                        images, 
                                        return_tensors=kwargs.get('return_tensors', 'pt')
                                    )
                                    result.update(image_inputs)
                                    
                                    # –°–æ–∑–¥–∞–µ–º img_mask –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                                    if 'img_mask' not in result and 'pixel_values' in image_inputs:
                                        batch_size = image_inputs['pixel_values'].shape[0]
                                        if len(image_inputs['pixel_values'].shape) == 3:
                                            # –§–æ—Ä–º–∞—Ç [batch, patches, features]
                                            total_patches = image_inputs['pixel_values'].shape[1]
                                        else:
                                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑ image_grid_thw –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                                            if 'image_grid_thw' in result:
                                                grid_thw = result['image_grid_thw']
                                                if len(grid_thw.shape) >= 2 and grid_thw.shape[1] >= 3:
                                                    _, h_patches, w_patches = grid_thw[0]
                                                    total_patches = h_patches * w_patches
                                                else:
                                                    total_patches = 256  # fallback
                                            else:
                                                total_patches = 256  # fallback
                                        
                                        import torch
                                        result['img_mask'] = torch.ones(batch_size, total_patches, dtype=torch.bool)
                                    
                                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π image_grid_thw –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å, –∏–Ω–∞—á–µ —Å–æ–∑–¥–∞–µ–º —Å–≤–æ–π
                                    if 'image_grid_thw' not in image_inputs and 'pixel_values' in image_inputs:
                                        batch_size = image_inputs['pixel_values'].shape[0]
                                        
                                        # –î–ª—è dots.ocr –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã —Å–µ—Ç–∫–∏
                                        # pixel_values –∏–º–µ–µ—Ç —Ñ–æ—Ä–º—É [batch, channels, height, width]
                                        if len(image_inputs['pixel_values'].shape) == 4:
                                            _, channels, height, width = image_inputs['pixel_values'].shape
                                            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç—á–µ–π (–æ–±—ã—á–Ω–æ 14x14 –ø–∞—Ç—á–∏)
                                            patch_size = 14  # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞ –¥–ª—è ViT
                                            h_patches = height // patch_size
                                            w_patches = width // patch_size
                                            total_patches = h_patches * w_patches
                                        else:
                                            # –ï—Å–ª–∏ —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –ø–∞—Ç—á–µ–π [batch, patches, features]
                                            total_patches = image_inputs['pixel_values'].shape[1]
                                            # –î–ª—è 588 –ø–∞—Ç—á–µ–π: –ø–æ–ø—Ä–æ–±—É–µ–º 21x28 = 588
                                            if total_patches == 588:
                                                h_patches, w_patches = 21, 28
                                            else:
                                                # –û–±—â–∏–π —Å–ª—É—á–∞–π - –Ω–∞–π–¥–µ–º –±–ª–∏–∂–∞–π—à–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
                                                import math
                                                sqrt_patches = int(math.sqrt(total_patches))
                                                h_patches = sqrt_patches
                                                w_patches = sqrt_patches
                                                # –ï—Å–ª–∏ –Ω–µ –∫–≤–∞–¥—Ä–∞—Ç–Ω–∞—è, –Ω–∞–π–¥–µ–º –ª—É—á—à–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
                                                if h_patches * w_patches != total_patches:
                                                    for i in range(sqrt_patches, 0, -1):
                                                        if total_patches % i == 0:
                                                            h_patches = i
                                                            w_patches = total_patches // i
                                                            break
                                        
                                        # –°–æ–∑–¥–∞–µ–º image_grid_thw (–≤—Ä–µ–º—è, –≤—ã—Å–æ—Ç–∞, —à–∏—Ä–∏–Ω–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
                                        # –î–ª—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—Ä–µ–º—è = 1
                                        import torch
                                        result['image_grid_thw'] = torch.tensor([[1, h_patches, w_patches]], dtype=torch.long)
                                        
                                        # –°–æ–∑–¥–∞–µ–º img_mask (–º–∞—Å–∫–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
                                        result['img_mask'] = torch.ones(batch_size, total_patches, dtype=torch.bool)
                                
                                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º videos, —Ç–∞–∫ –∫–∞–∫ dots.ocr –∏—Ö –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç
                                
                                return result
                            
                            def batch_decode(self, *args, **kwargs):
                                return self.tokenizer.batch_decode(*args, **kwargs)
                        
                        self.processor = DotsOCRProcessorFixed(
                            image_processor=image_processor,
                            tokenizer=tokenizer,
                            video_processor=dummy_video_processor
                        )
                        
                        logger.info("‚úÖ Custom processor created successfully")
                    
                    except Exception as custom_error:
                        logger.error(f"Custom processor creation failed: {custom_error}")
                        logger.error(f"Traceback: {traceback.format_exc()}")
                        
                        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - —Å–æ–∑–¥–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
                        logger.warning("Trying ultra-simple processor fallback...")
                        
                        class UltraSimpleProcessor:
                            def __init__(self, image_processor, tokenizer):
                                self.image_processor = image_processor
                                self.tokenizer = tokenizer
                            
                            def apply_chat_template(self, messages, **kwargs):
                                # –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
                                if isinstance(messages, list) and len(messages) > 0:
                                    message = messages[0]
                                    if isinstance(message, dict) and 'content' in message:
                                        content = message['content']
                                        if isinstance(content, list):
                                            for item in content:
                                                if isinstance(item, dict) and item.get('type') == 'text':
                                                    return item.get('text', '')
                                        elif isinstance(content, str):
                                            return content
                                return kwargs.get('text', '')
                            
                            def __call__(self, text=None, images=None, videos=None, **kwargs):
                                result = {}
                                
                                if text is not None:
                                    try:
                                        text_inputs = self.tokenizer(text, **kwargs)
                                        result.update(text_inputs)
                                    except Exception as e:
                                        logger.warning(f"Tokenizer failed: {e}")
                                        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                                        import torch
                                        result['input_ids'] = torch.tensor([[1, 2, 3]])  # Dummy tokens
                                        result['attention_mask'] = torch.tensor([[1, 1, 1]])
                                
                                if images is not None:
                                    try:
                                        image_inputs = self.image_processor(images, return_tensors=kwargs.get('return_tensors', 'pt'))
                                        result.update(image_inputs)
                                        
                                        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π img_mask
                                        if 'pixel_values' in image_inputs:
                                            # –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º img_mask –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
                                            pixel_values = image_inputs['pixel_values']
                                            
                                            if len(pixel_values.shape) == 2:
                                                # –§–æ—Ä–º–∞—Ç [patches, features]
                                                batch_size = 1
                                                total_patches = pixel_values.shape[0]
                                            elif len(pixel_values.shape) == 3:
                                                # –§–æ—Ä–º–∞—Ç [batch, patches, features] 
                                                batch_size = pixel_values.shape[0]
                                                total_patches = pixel_values.shape[1]
                                            else:
                                                # Fallback –Ω–∞ –æ—Å–Ω–æ–≤–µ image_grid_thw
                                                batch_size = 1
                                                if 'image_grid_thw' in result:
                                                    grid_thw = result['image_grid_thw']
                                                    if len(grid_thw.shape) >= 2 and grid_thw.shape[1] >= 3:
                                                        _, h_patches, w_patches = grid_thw[0]
                                                        total_patches = int(h_patches * w_patches)
                                                    else:
                                                        total_patches = 256
                                                else:
                                                    total_patches = 256
                                            
                                            import torch
                                            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
                                            device = pixel_values.device if hasattr(pixel_values, 'device') else 'cpu'
                                            result['img_mask'] = torch.ones(batch_size, total_patches, dtype=torch.bool, device=device)
                                            
                                            logger.info(f"üîß Created img_mask: shape={result['img_mask'].shape}, sum={result['img_mask'].sum()}, device={result['img_mask'].device}")
                                        
                                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π image_grid_thw –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å, –∏–Ω–∞—á–µ —Å–æ–∑–¥–∞–µ–º —Å–≤–æ–π
                                        if 'image_grid_thw' not in image_inputs and 'pixel_values' in image_inputs:
                                            batch_size = image_inputs['pixel_values'].shape[0]
                                            
                                            # –î–ª—è dots.ocr –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã —Å–µ—Ç–∫–∏
                                            # pixel_values –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ [batch, patches, features] –∏–ª–∏ [batch, channels, height, width]
                                            if len(image_inputs['pixel_values'].shape) == 4:
                                                _, channels, height, width = image_inputs['pixel_values'].shape
                                                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç—á–µ–π (–æ–±—ã—á–Ω–æ 14x14 –ø–∞—Ç—á–∏)
                                                patch_size = 14  # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞ –¥–ª—è ViT
                                                h_patches = height // patch_size
                                                w_patches = width // patch_size
                                                total_patches = h_patches * w_patches
                                            elif len(image_inputs['pixel_values'].shape) == 3:
                                                # –£–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –ø–∞—Ç—á–µ–π [batch, patches, features]
                                                batch_size, total_patches, features = image_inputs['pixel_values'].shape
                                                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –∫–≤–∞–¥—Ä–∞—Ç–Ω—É—é —Å–µ—Ç–∫—É –ø–∞—Ç—á–µ–π
                                                h_patches = int(total_patches ** 0.5)
                                                w_patches = h_patches
                                                # –ï—Å–ª–∏ –Ω–µ –∫–≤–∞–¥—Ä–∞—Ç–Ω–∞—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ
                                                if h_patches * w_patches != total_patches:
                                                    # –î–ª—è 588 –ø–∞—Ç—á–µ–π: 24x24 = 576, 25x24 = 600, –ø–æ–ø—Ä–æ–±—É–µ–º 21x28 = 588
                                                    if total_patches == 588:
                                                        h_patches, w_patches = 21, 28
                                                    else:
                                                        # –û–±—â–∏–π —Å–ª—É—á–∞–π - –Ω–∞–π–¥–µ–º –±–ª–∏–∂–∞–π—à–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
                                                        import math
                                                        sqrt_patches = int(math.sqrt(total_patches))
                                                        for i in range(sqrt_patches, 0, -1):
                                                            if total_patches % i == 0:
                                                                h_patches = i
                                                                w_patches = total_patches // i
                                                                break
                                            else:
                                                # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                                                total_patches = image_inputs['pixel_values'].shape[1] if len(image_inputs['pixel_values'].shape) > 1 else 1
                                                h_patches = int(total_patches ** 0.5)
                                                w_patches = h_patches
                                            
                                            import torch
                                            result['image_grid_thw'] = torch.tensor([[1, h_patches, w_patches]], dtype=torch.long)
                                            result['img_mask'] = torch.ones(batch_size, total_patches, dtype=torch.bool)
                                            
                                    except Exception as e:
                                        logger.warning(f"Image processor failed: {e}")
                                
                                return result
                            
                            def batch_decode(self, *args, **kwargs):
                                return self.tokenizer.batch_decode(*args, **kwargs)
                        
                        self.processor = UltraSimpleProcessor(image_processor, tokenizer)
                        logger.info("‚úÖ Ultra-simple processor fallback created")
                else:
                    raise e
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º eval
            self.model.eval()
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if hasattr(self.processor, 'tokenizer'):
                tokenizer = self.processor.tokenizer
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                    logger.info("Set pad_token to eos_token")
                
                logger.info(f"Tokenizer vocab size: {len(tokenizer)}")
                logger.info(f"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
                logger.info(f"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
            
            logger.info("‚úÖ dots.ocr loaded successfully with video_processor fix")
            
        except Exception as e:
            logger.error(f"Failed to load dots.ocr: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise
    
    def _preprocess_image(self, image: Image.Image) -> Image.Image:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è dots.ocr."""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è dots.ocr (–∫—Ä–∞—Ç–Ω—ã–π 14)
            max_size = 1120  # 80 * 14 - —É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            
            if max(image.size) > max_size:
                ratio = max_size / max(image.size)
                new_size = tuple(int(dim * ratio) for dim in image.size)
                # –î–µ–ª–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∫—Ä–∞—Ç–Ω—ã–º–∏ 14
                new_size = (
                    ((new_size[0] + 13) // 14) * 14,
                    ((new_size[1] + 13) // 14) * 14
                )
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                logger.info(f"Resized image to {new_size}")
            
            return image
            
        except Exception as e:
            logger.warning(f"Image preprocessing failed: {e}")
            return image
    
    def _safe_generate_improved(self, inputs: Dict, prompt_type: str = "simple") -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π."""
        try:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ float16
            device = next(self.model.parameters()).device
            target_dtype = torch.float16
            
            # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º dtype –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            for key, value in inputs.items():
                if isinstance(value, torch.Tensor) and value.dtype not in [torch.int64, torch.int32, torch.bool]:
                    if value.dtype != target_dtype:
                        logger.debug(f"Converting input {key} from {value.dtype} to {target_dtype}")
                        inputs[key] = value.to(target_dtype)
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                if isinstance(value, torch.Tensor):
                    inputs[key] = value.to(device)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –º–æ–¥–µ–ª—å
            model_inputs = {}
            valid_keys = ['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw']
            
            for key, value in inputs.items():
                if key in valid_keys:
                    model_inputs[key] = value
                else:
                    logger.debug(f"Skipping input key: {key}")
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            generation_kwargs = {
                'max_new_tokens': 256,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                'min_new_tokens': 1,    # –ú–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω
                'do_sample': True,      # –í–∫–ª—é—á–∞–µ–º sampling –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                'temperature': 0.7,     # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
                'top_p': 0.9,          # Nucleus sampling
                'top_k': 50,           # Top-k sampling
                'repetition_penalty': 1.3,  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                'no_repeat_ngram_size': 3,   # –ó–∞–ø—Ä–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è 3-–≥—Ä–∞–º–º
                'pad_token_id': self.processor.tokenizer.pad_token_id,
                'eos_token_id': self.processor.tokenizer.eos_token_id,
                'use_cache': True,
                'output_attentions': False,
                'output_hidden_states': False
            }
            
            # –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if prompt_type in ["minimal", "simple"]:
                generation_kwargs.update({
                    'max_new_tokens': 128,
                    'temperature': 0.3,
                    'repetition_penalty': 1.5
                })
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º autocast –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                with torch.autocast(device_type='cuda', dtype=target_dtype, enabled=True):
                    generated_ids = self.model.generate(
                        **model_inputs,
                        **generation_kwargs
                    )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(model_inputs['input_ids'], generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=True
            )[0]
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            output_text = output_text.strip()
            
            # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, !!!!!!)
            import re
            output_text = re.sub(r'(.)\1{5,}', r'\1', output_text)  # –£–¥–∞–ª—è–µ–º 6+ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
            output_text = re.sub(r'\s+', ' ', output_text)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
            
            return output_text
            
        except Exception as e:
            logger.error(f"Generation failed: {e}")
            raise
    
    def _create_messages(self, image: Image.Image, prompt: str) -> List[Dict]:
        """–°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è dots.ocr."""
        return [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
    
    def _process_with_prompt(self, image: Image.Image, prompt: str, prompt_type: str = "simple") -> str:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º."""
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            processed_image = self._preprocess_image(image)
            
            # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = self._create_messages(processed_image, prompt)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞
            try:
                text = self.processor.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
            except Exception as e:
                logger.warning(f"Chat template failed: {e}, using simple text")
                text = prompt
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            try:
                from qwen_vl_utils import process_vision_info
                image_inputs, video_inputs = process_vision_info(messages)
            except ImportError:
                logger.warning("qwen_vl_utils –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É")
                image_inputs = [processed_image]
                video_inputs = None
            except Exception as e:
                logger.warning(f"process_vision_info failed: {e}, using direct processing")
                image_inputs = [processed_image]
                video_inputs = None
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            try:
                inputs = self.processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt"
                )
            except Exception as e:
                logger.warning(f"Processor call with videos failed: {e}, trying without videos")
                try:
                    inputs = self.processor(
                        text=[text],
                        images=image_inputs,
                        padding=True,
                        return_tensors="pt"
                    )
                except Exception as e2:
                    logger.error(f"Processor call failed: {e2}")
                    raise e2
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            output_text = self._safe_generate_improved(inputs, prompt_type)
            
            return output_text
            
        except Exception as e:
            logger.error(f"Processing failed: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return f"[Processing error: {e}]"
    
    def process_image(self, image: Image.Image, prompt: Optional[str] = None, 
                     mode: str = "text_extraction") -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        if self.model is None:
            raise RuntimeError("Model not loaded")
        
        try:
            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç
            if prompt is None:
                prompt = self.prompts.get(mode, self.prompts["text_extraction"])
            
            logger.info(f"Processing with mode: {mode}")
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if image is None:
                raise ValueError("Image is None")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            result = self._process_with_prompt(image, prompt, mode)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if not result or result.strip() == "" or len(result.strip()) < 3:
                logger.warning("Poor result, trying alternative prompt")
                result = self._process_with_prompt(image, self.prompts["minimal"], "minimal")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
            if result and len(set(result.replace(' ', ''))) < 3:  # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
                logger.warning("Detected repetitive output, trying OCR prompt")
                result = self._process_with_prompt(image, self.prompts["ocr"], "simple")
            
            logger.info("Processing completed successfully")
            
            return result if result and len(result.strip()) > 2 else "[dots.ocr: No meaningful text detected]"
            
        except Exception as e:
            logger.error(f"Error processing image: {e}")
            return f"[dots.ocr error: {e}]"
    
    def extract_text(self, image: Image.Image) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç."""
        return self.process_image(image, mode="text_extraction")
    
    def chat(self, image: Image.Image, prompt: str, **kwargs) -> str:
        """–ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é."""
        return self.process_image(image, prompt=prompt, mode="custom")
    
    def extract_table(self, image: Image.Image) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü—ã."""
        return self.process_image(image, prompt="Extract table content from this image", mode="simple")
    
    def parse_document(self, image: Image.Image) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç."""
        try:
            result = self.process_image(image, mode="text_extraction")
            
            return {
                "success": True,
                "text": result,
                "method": "video_processor_fixed_parsing"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "text": f"Error: {e}"
            }
    
    def unload(self) -> None:
        """–í—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å."""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–∞
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                except Exception as e:
                    logger.warning(f"Warning during CUDA cleanup: {e}")
            
            logger.info("dots.ocr unloaded successfully")
            
        except Exception as e:
            logger.warning(f"Warning during model unload: {e}")