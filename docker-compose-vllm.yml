version: '3.8'

services:
  # DotsOCR - основная OCR модель (протестирована и работает)
  dots-ocr:
    image: vllm/vllm-openai:latest
    container_name: dots-ocr-fixed
    restart: "no"  # ИСПРАВЛЕНО: Отключен автоперезапуск для соблюдения принципа одного контейнера
    ports:
      - "8000:8000"
    volumes:
      # КРИТИЧНО: Монтирование кеша HuggingFace с правами на чтение и запись
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub:rw
      # Дополнительные кеши для совместимости
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:rw
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface:rw
    environment:
      # Переменные окружения для кеша
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      # Настройки CUDA
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 8g
    command: >
      --model rednote-hilab/dots.ocr
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 4096
      --gpu-memory-utilization 0.85
      --dtype bfloat16
      --enforce-eager
      --disable-log-requests
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Qwen2-VL 2B - легкая VLM модель (совместима с vLLM)
  qwen2-vl-2b:
    image: vllm/vllm-openai:latest
    container_name: qwen-qwen2-vl-2b-instruct-vllm
    restart: "no"  # ИСПРАВЛЕНО: Отключен автоперезапуск для соблюдения принципа одного контейнера
    ports:
      - "8001:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:rw
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface:rw
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 6g
    command: >
      --model Qwen/Qwen2-VL-2B-Instruct
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 4096
      --gpu-memory-utilization 0.6
      --dtype bfloat16
      --disable-log-requests
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - multi-model

  # Qwen3-VL 2B - улучшенная VLM модель (рекомендуется)
  qwen3-vl-2b:
    image: vllm/vllm-openai:latest
    container_name: qwen-qwen3-vl-2b-instruct-vllm
    restart: "no"  # ИСПРАВЛЕНО: Отключен автоперезапуск для соблюдения принципа одного контейнера
    ports:
      - "8004:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:rw
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface:rw
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 6g
    command: >
      --model Qwen/Qwen3-VL-2B-Instruct
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 4096
      --gpu-memory-utilization 0.6
      --dtype bfloat16
      --disable-log-requests
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - multi-model

  # Phi-3.5 Vision - продвинутая VLM модель (требует больше памяти)
  phi35-vision:
    image: vllm/vllm-openai:latest
    container_name: microsoft-phi-3-5-vision-instruct-vllm
    restart: "no"  # ИСПРАВЛЕНО: Отключен автоперезапуск для соблюдения принципа одного контейнера
    ports:
      - "8002:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:rw
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface:rw
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 8g
    command: >
      --model microsoft/Phi-3.5-vision-instruct
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 4096
      --gpu-memory-utilization 0.6
      --dtype bfloat16
      --enforce-eager
      --disable-log-requests
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - multi-model
      - high-memory

  # Qwen2-VL 7B - большая VLM модель (только для мощных GPU)
  qwen2-vl-7b:
    image: vllm/vllm-openai:latest
    container_name: qwen-qwen2-vl-7b-instruct-vllm
    restart: "no"  # ИСПРАВЛЕНО: Отключен автоперезапуск для соблюдения принципа одного контейнера
    ports:
      - "8003:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub:rw
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface:rw
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface:rw
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 12g
    command: >
      --model Qwen/Qwen2-VL-7B-Instruct
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 4096
      --gpu-memory-utilization 0.5
      --dtype bfloat16
      --disable-log-requests
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - high-memory

  # Nginx прокси для балансировки нагрузки (опционально)
  nginx-proxy:
    image: nginx:alpine
    container_name: vllm-proxy
    restart: "no"  # ИСПРАВЛЕНО: Отключен автоперезапуск
    ports:
      - "8080:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - dots-ocr
    profiles:
      - proxy

# Именованные тома для кеша (альтернативный подход)
volumes:
  huggingface-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/huggingface

# Сети для изоляции
networks:
  vllm-network:
    driver: bridge