version: '3.8'

services:
  # DotsOCR - основная OCR модель
  dots-ocr:
    image: vllm/vllm-openai:latest
    container_name: dots-ocr-vllm
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      # КРИТИЧНО: Монтирование кеша HuggingFace с правами на чтение и запись
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub
      # Дополнительные кеши для совместимости
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface
    environment:
      # Переменные окружения для кеша
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      # Настройки CUDA
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 8g
    command: >
      --model rednote-hilab/dots.ocr
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 2048
      --gpu-memory-utilization 0.7
      --dtype bfloat16
      --enforce-eager
      --disable-log-requests
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Qwen2-VL 2B - легкая VLM модель
  qwen2-vl-2b:
    image: vllm/vllm-openai:latest
    container_name: qwen2-vl-2b-vllm
    restart: unless-stopped
    ports:
      - "8001:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 6g
    command: >
      --model Qwen/Qwen2-VL-2B-Instruct
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 4096
      --gpu-memory-utilization 0.6
      --dtype bfloat16
      --disable-log-requests
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - multi-model

  # GOT-OCR 2.0 - альтернативная OCR модель
  got-ocr:
    image: vllm/vllm-openai:latest
    container_name: got-ocr-vllm
    restart: unless-stopped
    ports:
      - "8002:8000"
    volumes:
      - ${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ${HOME}/.cache/huggingface/hub:/home/vllm/.cache/huggingface/hub
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ${HOME}/.cache/huggingface:/home/vllm/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface/hub
      - HF_HUB_CACHE=/root/.cache/huggingface/hub
      - HF_TOKEN=${HF_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: 4g
    command: >
      --model stepfun-ai/GOT-OCR-2.0-hf
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 2048
      --gpu-memory-utilization 0.5
      --dtype bfloat16
      --enforce-eager
      --disable-log-requests
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    profiles:
      - multi-model

  # Nginx прокси для балансировки нагрузки (опционально)
  nginx-proxy:
    image: nginx:alpine
    container_name: vllm-proxy
    restart: unless-stopped
    ports:
      - "8080:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - dots-ocr
    profiles:
      - proxy

# Именованные тома для кеша (альтернативный подход)
volumes:
  huggingface-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/huggingface

# Сети для изоляции
networks:
  vllm-network:
    driver: bridge