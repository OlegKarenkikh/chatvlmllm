#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã OCR –¥–ª—è RTX 5070 Ti Blackwell
"""

import torch
import time
from transformers import AutoModelForImageTextToText, AutoProcessor
from PIL import Image
import yaml

def test_system():
    print("üß™ –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ –°–ò–°–¢–ï–ú–´ OCR")
    print("=" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    print(f"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}")
    print(f"üîß Compute Capability: {torch.cuda.get_device_capability(0)}")
    print(f"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB")
    print(f"üêç PyTorch: {torch.__version__}")
    print(f"‚ö° CUDA: {torch.version.cuda}")
    print(f"‚úÖ bfloat16: {torch.cuda.is_bf16_supported()}")
    print()
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    print("‚ö° –ü–†–ò–ú–ï–ù–ï–ù–ò–ï BLACKWELL –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ô")
    print("=" * 50)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.enable_flash_sdp(True)
    print("‚úÖ TF32 –≤–∫–ª—é—á–µ–Ω –¥–ª—è Tensor Cores")
    print("‚úÖ cuDNN benchmark –≤–∫–ª—é—á–µ–Ω")
    print("‚úÖ SDPA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
    print()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    # –¢–µ—Å—Ç Qwen2-VL (–æ—Å–Ω–æ–≤–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –º–æ–¥–µ–ª—å)
    print("üöÄ –¢–ï–°–¢ QWEN2-VL (–†–ï–ö–û–ú–ï–ù–î–£–ï–ú–ê–Ø –ú–û–î–ï–õ–¨)")
    print("=" * 50)
    
    try:
        start_time = time.time()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        model = AutoModelForImageTextToText.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            torch_dtype=torch.bfloat16,
            attn_implementation="eager",
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            trust_remote_code=True
        )
        
        load_time = time.time() - start_time
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        print(f"‚úÖ Dtype –º–æ–¥–µ–ª–∏: {model.dtype}")
        print(f"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {model.device}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ VRAM
        torch.cuda.empty_cache()
        vram_used = torch.cuda.memory_allocated() / 1024**3
        print(f"‚úÖ VRAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {vram_used:.2f}GB")
        print()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        print("üîç –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        test_image = Image.new('RGB', (800, 600), color='white')
        
        # –¢–µ—Å—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å bfloat16 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏...")
        start_time = time.time()
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π API –¥–ª—è Qwen2-VL
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": test_image},
                    {"type": "text", "text": "–û–ø–∏—à–∏—Ç–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."}
                ]
            }
        ]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template
        text_prompt = processor.apply_chat_template(
            conversation, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        inputs = processor(
            text=[text_prompt],
            images=[test_image],
            padding=True,
            return_tensors="pt"
        ).to("cuda")
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=128,
                do_sample=False,
                use_cache=True,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]
        
        inference_time = time.time() - start_time
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {inference_time:.3f}s")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {output_text[:100]}...")
        print()
        
        # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("üèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
        print("=" * 50)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {load_time:.2f}s")
        print(f"‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å: {inference_time:.3f}s")
        print(f"‚úÖ VRAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {vram_used:.2f}GB")
        print(f"‚úÖ Dtype: {model.dtype}")
        print(f"‚úÖ –°—Ç–∞—Ç—É—Å: –ü–û–õ–ù–û–°–¢–¨–Æ –†–ê–ë–û–¢–ê–ï–¢")
        print()
        
        print("üéâ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!")
        print("‚úÖ RTX 5070 Ti Blackwell –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        print("‚úÖ Qwen2-VL —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é")
        print("‚úÖ bfloat16 + eager attention = —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å + —Å–∫–æ—Ä–æ—Å—Ç—å")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

if __name__ == "__main__":
    success = test_system()
    if success:
        print("\nüöÄ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!")
    else:
        print("\n‚ùå –¢–†–ï–ë–£–ï–¢–°–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê")