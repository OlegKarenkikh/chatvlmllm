ДЕПАРТАМЕНТ ОБРАЗОВАНИЯ И НАУКИ ГОРОДА МОСКВЫ

ГОСУДАРСТВЕННОЕ БЮДЖЕТНОЕ

ОБЩЕОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ ГОРОДА МОСКВЫ

«ШКОЛА № 1474»

**Локальный чат на базе LLM/VLM**

+-----------------------------------------------------------------------+
| Участник:                                                             |
|                                                                       |
| Ученик 10 «Н» класса ГБОУ Школа                                       |
|                                                                       |
| №1474 Кареньких Владимир Олегович                                     |
+=======================================================================+
| Руководитель:                                                         |
|                                                                       |
| педагог ГБОУ Школа № 1474                                             |
|                                                                       |
| Василенок Алиса Андреевна                                             |
+-----------------------------------------------------------------------+

**Содержание**

#  {#section .TOC-Heading}

[Глава Ⅰ. Вступление [2](#_Toc1401690101)](#_Toc1401690101)

[Введение [3](#_Toc1764654013)](#_Toc1764654013)

[Цель проекта [3](#_Toc562581567)](#_Toc562581567)

[Актуальность проекта [4](#_Toc199335073)](#_Toc199335073)

[Проблематика [5](#_Toc806472731)](#_Toc806472731)

[Обзор аналогов [6](#_Toc559703059)](#_Toc559703059)

[Задачи проекта [7](#_Toc2077107103)](#_Toc2077107103)

[Глава II. Теоретические основы [8](#_Toc1372880126)](#_Toc1372880126)

[Основы работы LLM и VLM моделей [9](#_Toc349083464)](#_Toc349083464)

[Процесс работы LLM [9](#_Toc2054480561)](#_Toc2054480561)

[Vision-Language Models (VLM) [9](#_Toc596138151)](#_Toc596138151)

[Архитектура VLM [10](#_Toc1145402116)](#_Toc1145402116)

[Параметры генерации [10](#_Toc246992138)](#_Toc246992138)

[Архитектура клиент-серверных приложений
[11](#_Toc34199352)](#_Toc34199352)

[Мультимодальность в машинном обучении
[12](#_Toc1538023111)](#_Toc1538023111)

[Применение в проекте [13](#_Toc62429049)](#_Toc62429049)

[Синтез технологий и перспективы проекта
[13](#_Toc1495327267)](#_Toc1495327267)

[Глава III. Проектирование системы [14](#_Toc326285335)](#_Toc326285335)

[Архитектура решения [15](#_Toc459212235)](#_Toc459212235)

[Диаграмма архитектуры [15](#_Toc610522945)](#_Toc610522945)

[Компоненты системы [17](#_Toc802678299)](#_Toc802678299)

[Структурное проектирование проекта
[18](#_Toc1023398426)](#_Toc1023398426)

[Проектирование модульной архитектуры
[19](#_Toc1033344005)](#_Toc1033344005)

[Проектирование алгоритмов взаимодействия
[20](#_Toc1950645387)](#_Toc1950645387)

[Проектирование системы состояний
[20](#_Toc1137165461)](#_Toc1137165461)

[Проектирование обработки ошибок [21](#_Toc1409624254)](#_Toc1409624254)

[Проектирование адаптивности [21](#_Toc368312850)](#_Toc368312850)

[Глава IV. Теоретические аспекты локального развертывания LLM/VLM систем
[22](#_Toc1819658200)](#_Toc1819658200)

[Концепция локального развертывания искусственного интеллекта
[22](#_Toc1445695725)](#_Toc1445695725)

[Теория распределенных вычислений в контексте LLM
[22](#_Toc1346080975)](#_Toc1346080975)

[Теория обработки мультимодальных данных
[23](#_Toc273897726)](#_Toc273897726)

[Теория управления контекстом в диалоговых системах
[24](#_Toc1723189714)](#_Toc1723189714)

[Теория оптимизации inference-процесса
[26](#_Toc33026146)](#_Toc33026146)

[Теория безопасности локальных AI-систем
[27](#_Toc1524643496)](#_Toc1524643496)

[Теория эволюции и жизненного цикла AI-систем
[27](#_Toc880474818)](#_Toc880474818)

[Модель зрелости AI-систем [27](#_Toc1658927800)](#_Toc1658927800)

**\
**

[]{#_Toc1401690101 .anchor}**Глава Ⅰ. Вступление**

[]{#_Toc1764654013 .anchor}**Введение**

В современном мире растёт потребность в системах, способных обрабатывать
информацию автономно и безопасно. Особенно это актуально для
организаций, где важна конфиденциальность данных --- банков, страховых
компаний, исследовательских центров. Современные большие языковые и
мультимодальные модели (LLM/VLM) позволяют решать задачи понимания
текста и изображений, но их использование через облачные сервисы создаёт
риски утечки данных. Данный проект направлен на создание локального
чат-приложения, которое работает полностью на стороне пользователя и не
требует подключения к внешним серверам. Система сочетает возможности
языковых моделей (LLM) и визуальных моделей (VLM) для обработки
текстовых и графических запросов, обеспечивая удобный интерфейс
взаимодействия и расширенные функции по анализу данных и документов.

**\
**

[]{#_Toc562581567 .anchor}**Цель проекта**

Цель проекта --- Создание локального чат‑приложения на базе LLM/VLM,
которое позволяет безопасно и автономно обрабатывать текстовые и
мультимодальные запросы пользователя без зависимости от внешних облачных
сервисов.

**\
**

[]{#_Toc199335073 .anchor}**Актуальность проекта**

> Данный проект высоко актуален в современных реалиях, что обусловлено
> рядом ключевых факторов. Прежде всего, это растущие требования к
> безопасности данных: страховые и банковские компании не имеют
> возможности использовать облачные LLM-решения для работы с
> конфиденциальной информацией. Локальное развёртывание обеспечивает
> необходимый уровень надёжности и полного контроля над инфраструктурой,
> гарантируя её доступность и управляемость. Кроме того, проект отвечает
> задачам импортозамещения, поскольку использует открытые и
> отечественные модели, снижая зависимость от иностранных вендоров. Его
> функциональность расширена за счёт мультимодальности --- объединения
> обработки текста и изображений, что становится ключевым фактором для
> повышения эффективности рабочих процессов. Благодаря этой
> универсальности, решение может быть применено в самых разных областях,
> включая документооборот, контакт-центры, RAG-системы и автоматизацию
> офисных процессов.**\
> **

[]{#_Toc806472731 .anchor}**Проблематика**

> Современные решения на базе крупных языковых и мультимодальных моделей
> (LLM/VLM) преимущественно работают через облачные сервисы, что создаёт
> значительные риски и ограничения при работе с конфиденциальной
> корпоративной информацией. Для многих организаций, особенно из
> финансового, страхового, государственного и промышленного секторов,
> использование таких сервисов неприемлемо из-за невозможности
> гарантировать безопасность, контроль и автономность обработки данных.
> Кроме того, на рынке отсутствуют локальные платформы, которые
> совмещают возможности мультимодальной обработки (текста и
> изображений), поддержку популярных форматов документов, гибкую
> архитектуру и масштабируемость. Это существенно ограничивает внедрение
> интеллектуальных ассистентов и автоматизированных систем анализа
> данных внутри корпоративных инфраструктур.
>
> Ключевые проблемы:
>
> 1\. **Зависимость от внешних облачных решений**
>
> 2\. **Ограниченные возможности интеграции**
>
> 3\. **Отсутствие мультимодальных локальных решений**
>
> 4\. **Недостаток инструментов для управления и мониторинга**
>
> 5\. **Необходимость импортозамещения**

[]{#_Toc559703059 .anchor}**Обзор аналогов**

Рынок решений на базе крупных языковых моделей (LLM) и мультимодальных
систем (VLM) активно развивается, однако большинство популярных платформ
функционирует исключительно в облачной среде, что ограничивает их
применение в условиях повышенных требований к безопасности данных.
Рассмотрим существующие категории и конкретные примеры аналогов.

**Облачные LLM-платформы:**

К числу наиболее известных облачных решений относятся:

> **- OpenAI ChatGPT / GPT-4, Claude (Anthropic), Gemini (Google),
> Mistral AI Cloud** --- предоставляют высокое качество генерации
> текста, поддержку мультимодальности и интеграции через API.\
> **- Недостатки:** требуют постоянного интернет-доступа, передают
> пользовательские данные во внешние облака, не обеспечивают контроля
> над процессом обработки информации, что делает их неприемлемыми для
> компаний с повышенными требованиями к конфиденциальности.

Корпоративные решения с ограниченной локализацией

> **- Microsoft Copilot, IBM watsonx.ai, Amazon Bedrock** ---
> ориентированы на корпоративный сектор и частично поддерживают
> локальное развертывание.\
> **Недостатки:** зависимость от облачной инфраструктуры поставщика,
> закрытый исходный код, высокая стоимость лицензирования и ограниченная
> гибкость кастомизации под внутренние нужды.

**\
**

[]{#_Toc2077107103 .anchor}**Задачи проекта**

1.  Разработать архитектуру из независимых контейнеров (FastAPI,
    Streamlit) для гибкости и масштабируемости.

2.  Исследовать доступные модели со свободными лицензиями (MIT) на сайте
    : <https://huggingface.co/> , которые можно использовать на
    имеющемся оборудовании для разработки с видеокартой : NVIDIA GeForce
    RTX 5070 TI, с 12 гб памяти.

3.  Обеспечить работу с локальными моделями с поддержкой
    мультимодальности (текст + изображения)

4.  «Реализовать поддержку популярных форматов документов (PDF, JPG, TXT
    и других), включая графические файлы и вложенные изображения, с
    возможностью извлечения текста и визуальных сущностей.»

5.  Реализовать веб-интерфейс для общения с моделью, загрузки
    документов.

6.  Реализовать мониторинг загрузки доступных ресурсов и результатов
    обработки данных моделями.

7.  Реализовать возможность выбора модели для работы.

8.  Обеспечить безопасность и конфиденциальность данных за счёт
    локального развёртывания без передачи информации во внешние сервисы.

[]{#_Toc1372880126 .anchor}**Глава II. Теоретические основы**

[]{#_Toc349083464 .anchor}**Основы работы LLM и VLM моделей**

Large Language Models (LLM) --- это класс нейронных сетей, обученных на
огромных объемах текстовых данных для понимания и генерации
естественного языка. Современные LLM основаны на архитектуре
трансформеров (Transformers), предложенной в 2017 году в статье
\"Attention is All You Need\".

Ключевые характеристики LLM:

1\. Механизм внимания

Позволяет модели фокусироваться на релевантных частях входного текста
при генерации ответа.

2\. Контекстное окно

Количество токенов, которое модель может обработать за один раз.
Современные модели поддерживают от 4K до 200K токенов.

3\. Параметры модели

Количество весов в нейронной сети. Современные модели имеют от 7
миллиардов до нескольких триллионов параметров.

[]{#_Toc2054480561 .anchor}**Процесс работы LLM**

Ввод текста → Токенизация → Эмбеддинги →

→ Слои трансформера → Декодирование → Выходной текст

[]{#_Toc596138151 .anchor}**Vision-Language Models (VLM)**

[]{#_Toc1145402116 .anchor}**Архитектура VLM**

1\. Vision Encoder --- кодирует изображение в векторное представление

2\. Language Model --- обрабатывает текст и визуальные признаки

3\. Cross-modal Attention --- связывает визуальную и текстовую
информацию

Примеры задач для VLM:

\- Описание содержимого изображения

\- Оптическое распознавание символов (OCR)

\- Визуальный вопрос-ответ (Visual QA)

\- Извлечение информации из документов

\- Анализ диаграмм и графиков

Примеры задач для VLM:

\- Описание содержимого изображения

\- Оптическое распознавание символов (OCR)

\- Визуальный вопрос-ответ (Visual QA)

\- Извлечение информации из документов

\- Анализ диаграмм и графиков

**Параметры генерации**

-Описание содержимого изображения

-Оптическое распознавание символов (OCR)

-Визуальный вопрос-ответ (Visual QA)

-Извлечение информации из документов

-Анализ диаграмм и графиков

[]{#_Toc246992138 .anchor}**Параметры генерации**

Temperature (Температура)

Контролирует случайность выбора следующего токена:

\- 0.0 --- детерминированный выбор (всегда наиболее вероятный токен)

\- 0.3-0.7 --- сбалансированная генерация

\- 1.0+ --- более креативные и разнообразные ответы

Top-p (Nucleus Sampling)

Определяет порог кумулятивной вероятности для выбора токенов:

\- 1.0 --- рассматриваются все токены

\- 0.9 --- только токены с суммарной вероятностью 90%

\- Меньшие значения делают ответы более фокусированными

Max Tokens

Максимальное количество токенов в ответе. Один токен примерно равен 4
символам в английском или 2-3 символам в русском языке.

Presence Penalty

Штрафует повторение тем и понятий:

\- 0.0 --- нет штрафа

\- Положительные значения --- модель избегает повторений

\- Отрицательные значения --- модель склонна к повторениям

[]{#_Toc34199352 .anchor}**Архитектура клиент-серверных приложений**

Проект построен по классической клиент-серверной архитектуре, где:

Клиент --- веб-приложение, выполняющееся в браузере пользователя:

\- Отрисовка интерфейса

\- Обработка пользовательского ввода

\- Формирование запросов к API

\- Отображение результатов

Сервер --- локально запущенный LLM/VLM бэкенд:

\- Загрузка и управление моделями

\- Обработка запросов генерации

\- Управление памятью и ресурсами

Преимущества:

1\. Разделение ответственности

Интерфейс отделен от логики работы с моделями

2\. Гибкость

Возможность замены бэкенда без изменения клиента

3\. Масштабируемость

Легко добавить поддержку новых серверов

4\. Безопасность

Все данные остаются в локальной сети

[]{#_Toc1538023111 .anchor}**Мультимодальность в машинном обучении**

Мультимодальность --- это способность системы обрабатывать и
интегрировать информацию из различных модальностей (типов данных):
текст, изображения, аудио, видео.

Преимущества:

1.  Полнота понимания

Комбинирование разных источников информации дает более полное
представление о контексте

2.  Решение сложных задач

Возможность анализировать документы с изображениями, диаграммами,
схемами

3.  Естественность взаимодействия

Пользователи могут общаться с системой так же, как с человеком ---
показывая изображения и задавая вопросы

[]{#_Toc62429049 .anchor}**Применение в проекте**

1\. OCR (Оптическое распознавание символов ) и анализ документов

-Извлечение текста из изображений и его последующая обработка

2\. Визуальный вопрос-ответ

-Ответы на вопросы о содержимом изображения

3\. Описание изображений

-Генерация детальных текстовых описаний

4\. Извлечение сущностей

-Распознавание объектов, лиц, текста на изображении

[]{#_Toc1495327267 .anchor}**Синтез технологий и перспективы проекта**

Теоретический анализ, представленный в данной работе, демонстрирует, что
современные Large Language Models (LLM) и Vision-Language Models (VLM)
представляют собой мощный технологический фундамент для создания
универсальных систем искусственного интеллекта. Архитектура
трансформеров с механизмом внимания, лежащая в основе этих моделей,
обеспечивает глубокое контекстное понимание и генерацию текста, а их
мультимодальные расширения (VLM) эффективно объединяют анализ визуальной
и текстовой информации. Ключевым элементом, связывающим эти сложные
модели с практическими задачами, является выбранная клиент-серверная
архитектура. Она обеспечивает необходимую гибкость, безопасность и
масштабируемость, позволяя развертывать мощные нейросетевые модели
локально, что критически важно для соблюдения требований к
конфиденциальности данных и импортозамещению. Таким образом, проект
реализует полный технологический цикл: от обработки мультимодальных
входных данных (текст, изображения) через гибко настраиваемые параметры
генерации (temperature, top-p) до предоставления результата через
удобный веб-интерфейс. Это позволяет решать широкий спектр прикладных
задач --- от сложного анализа документов с помощью OCR и визуального
вопрос-ответа до построения интеллектуальных RAG-систем и автоматизации
офисных процессов. Синтез технологий LLM/VLM, мультимодального подхода и
надежной клиент-серверной архитектуры создает не просто инструмент, а
универсальную платформу. Её потенциал заключается в способности
адаптироваться к разнообразным бизнес-потребностям, обеспечивая при этом
высочайший уровень контроля над данными и инфраструктурой, что и
определяет её актуальность и долгосрочную жизнеспособность в условиях
быстро меняющегося технологического ландшафта.

[]{#_Toc326285335 .anchor}**Глава III. Проектирование системы**

[]{#_Toc459212235 .anchor}**Архитектура решения**

Архитектура проекта построена на принципах модульности, разделения
ответственности и масштабируемости.

[]{#_Toc610522945 .anchor}**Диаграмма архитектуры**

[]{#_Toc802678299 .anchor}**Компоненты системы**

**Клиентская часть (Frontend)**

Полностью автономный веб-интерфейс

Не требует серверной части для работы

Все вычисления и обработка на стороне клиента

**API-слой**

Взаимодействие с локальными LLM/VLM серверами

Поддержка множественных бэкендов

Обработка ошибок и таймаутов

**Серверная часть (Backend)**

Внешние сервисы (vLLM, Ollama)

Управление моделями

Обработка запросов генерации

[]{#_Toc1023398426 .anchor}**Структурное проектирование проекта**

Компоновка пользовательского интерфейса:

Семантическое разделение интерфейса на шесть основных блоков:

system-bar - панель системных настроек

model-bar - панель выбора модели

model-params - панель настройки параметров генерации

prompt-section - секция системного промпта

dialog - область диалога

input-area - область ввода

Принципы проектирования компонентов:

Использование CSS Grid для основной компоновки #chat-container

Применение Flexbox для внутреннего выравнивания элементов

Система CSS-переменных для унифицированного управления цветами

Разделение цветовых схем для различных типов сообщений

[]{#_Toc1033344005 .anchor}**Проектирование модульной архитектуры**

Ключевые модули системы:

Модуль инициализации (initializeApp) - обнаружение бэкендов и загрузка
моделей

Модуль управления моделями - работа со списками моделей и их параметрами

Система промптов - управление предопределенными шаблонами запросов

Модуль обработки файлов (handleFiles) - работа с мультимодальными
данными

Модуль рендеринга (render) - отображение диалога и контента

API-интеграционный модуль (sendMsg) - унифицированное взаимодействие с
бэкендами

[]{#_Toc1950645387 .anchor}**Проектирование алгоритмов взаимодействия**

Алгоритм обнаружения бэкендов:

Последовательный опрос эндпоинтов с таймаутом

Использование AbortController для контроля времени выполнения

Приоритетное определение доступного сервиса

Алгоритм обработки мультимодальных данных:

Загрузка файла → Определение типа → Преобразование в Base64 →

→ Проверка VLM-совместимости → Автоматическая отправка (для VLM)

Проектные решения для обработки изображений:

Автоматическое определение VLM-моделей по ключевым словам

Преобразование файлов в Data URL формат

Раздельная обработка для разных бэкендов (vLLM/Ollama)

[]{#_Toc1137165461 .anchor}**Проектирование системы состояний**

Управление состоянием интерфейса:

Блокировка элементов ввода во время выполнения запросов

Визуальная индикация статусов через updateStatus()

Динамическое обновление списков моделей

Управление диалоговым контекстом:

Поддержка массива сообщений context для сохранения истории

Разделение системных промптов и пользовательских сообщений

Сброс контекста при изменении системных инструкций

[]{#_Toc1409624254 .anchor}**Проектирование обработки ошибок**

Механизмы устойчивости:

Таймауты для сетевых запросов

Заглушки для неподдерживаемых функций (PDF)

Градуированная обработка различных типов ошибок

Восстановление состояния интерфейса после сбоев

[]{#_Toc368312850 .anchor}**Проектирование адаптивности**

Адаптивный дизайн:

Медиа-запросы для мобильных устройств (@media max-width: 768px)

Переключение на вертикальную компоновку на малых экранах

Масштабируемые элементы управления

**Проектирование расширяемости**

Архитектурные решения для будущего развития:

Модульная структура для добавления новых функций

Абстрагирование API-взаимодействия для поддержки новых бэкендов

Система плагинов для обработки различных типов файлов

Возможность интеграции RAG-систем и потоковой передачи

Данная система проектирования обеспечивает надежную, расширяемую и
удобную в использовании платформу для локального взаимодействия с
языковыми и мультимодальными моделями.

[]{#_Toc1819658200 .anchor}**Глава IV. Теоретические аспекты локального
развертывания LLM/VLM систем**

[]{#_Toc1445695725 .anchor}**Концепция локального развертывания
искусственного интеллекта**

Локальное развертывание систем искусственного интеллекта представляет
собой парадигму, при которой все вычислительные процессы, связанные с
обработкой данных и генерацией ответов, происходят на оборудовании,
находящемся под полным контролем организации или пользователя. В отличие
от облачных решений, где данные передаются на удаленные серверы третьих
сторон, локальная архитектура обеспечивает полную изоляцию
информационных потоков в пределах контролируемого периметра.

Принципы локального развертывания

Суверенитет данных --- концепция, согласно которой организация сохраняет
полный контроль над своими данными на всех этапах их жизненного цикла.
Данные не покидают инфраструктуру организации, что критически важно для
соблюдения требований регуляторов в финансовом, медицинском и
государственном секторах.

Детерминированность поведения --- в локальной среде организация имеет
полный контроль над версиями моделей, параметрами их работы и
вычислительными ресурсами. Это обеспечивает предсказуемость результатов
и возможность воспроизведения поведения системы.

Автономность функционирования --- система способна работать без
подключения к внешним сетям, что обеспечивает непрерывность
бизнес-процессов даже при отсутствии интернет-соединения или в условиях
ограниченной связности.

Прозрачность обработки --- возможность полного аудита всех операций,
выполняемых системой, включая логирование запросов, анализ промежуточных
результатов и отслеживание использования ресурсов.

[]{#_Toc1346080975 .anchor}**Теория распределенных вычислений в
контексте LLM**

Модель клиент-сервер для нейросетевых систем

Классическая клиент-серверная архитектура приобретает особое значение в
контексте работы с большими языковыми моделями. Разделение на клиентскую
и серверную части обусловлено фундаментальными различиями в характере
выполняемых операций и требованиях к ресурсам.

Клиентская часть выполняет операции с низкой вычислительной сложностью:
обработку пользовательского ввода, форматирование данных, управление
состоянием интерфейса. Эти операции имеют временную сложность O(n), где
n --- размер обрабатываемых данных, и могут эффективно выполняться на
устройствах с ограниченными ресурсами.

Серверная часть осуществляет операции с высокой вычислительной
сложностью: inference нейронных сетей, матричные операции, работу с
тензорами. Временная сложность этих операций составляет O(n²) или выше,
где n --- размер модели, что требует специализированного оборудования с
высокой параллельной производительностью.

Асинхронная модель взаимодействия

Взаимодействие между клиентом и сервером построено на асинхронной
модели, где запросы обрабатываются независимо от основного потока
выполнения. Это обеспечивает неблокирующее поведение интерфейса и
возможность параллельной обработки множественных запросов.

Теоретически, при наличии N клиентов и M серверов, пропускная
способность системы определяется формулой:

Throughput = min(N × R_client, M × R_server)

где R_client --- скорость генерации запросов клиентом, R_server ---
скорость обработки запросов сервером.

[]{#_Toc273897726 .anchor}**Теория обработки мультимодальных данных**

Представление информации в различных модальностях

Мультимодальные системы работают с данными, представленными в различных
формах: текст, изображения, аудио, видео. Каждая модальность имеет свое
пространство признаков и требует специфических методов обработки.

Текстовая модальность представляется в дискретном пространстве токенов,
где каждый токен соответствует определенному фрагменту текста.
Размерность этого пространства определяется размером словаря модели
(обычно 32,000 - 100,000 токенов).

Визуальная модальность представляется в непрерывном пространстве
пикселей или в дискретном пространстве визуальных токенов после
обработки vision encoder. Типичное изображение размером 224×224 пикселя
преобразуется в последовательность из 196-256 визуальных токенов.

Кросс-модальное выравнивание

Ключевой задачей мультимодальных систем является выравнивание
(alignment) различных модальностей в едином семантическом пространстве.
Это достигается через механизмы cross-attention, которые позволяют
модели находить соответствия между визуальными и текстовыми признаками.

Математически, cross-attention между визуальными признаками V и
текстовыми признаками T может быть представлен как: Attention(V, T) =
softmax(V × T\^T / √d) × T , где d --- размерность признакового
пространства.

[]{#_Toc1723189714 .anchor}**Теория управления контекстом в диалоговых
системах**

Контекстное окно и его ограничения

Контекстное окно (context window) определяет максимальное количество
токенов, которое модель может обработать за один запрос. Это
фундаментальное ограничение обусловлено квадратичной сложностью
механизма внимания: O(n²), где n --- длина последовательности.

Для модели с контекстным окном в 4096 токенов, механизм self-attention
требует вычисления матрицы размером 4096×4096, что составляет \~16
миллионов операций. При увеличении окна до 32,768 токенов количество
операций возрастает до \~1 миллиарда.

Стратегии управления контекстом

Скользящее окно (Sliding Window) --- сохранение только последних N
токенов диалога, где N определяется размером контекстного окна. Это
простой, но эффективный метод, обеспечивающий постоянное потребление
памяти.

Иерархическое сжатие (Hierarchical Compression) --- сжатие старых
сообщений в более компактные представления с сохранением ключевой
информации. Позволяет сохранить больше истории при ограниченном
контексте.

Селективное сохранение (Selective Retention) --- сохранение наиболее
релевантных фрагментов диалога на основе их важности для текущего
запроса. Требует дополнительных вычислений для оценки релевантности.

[]{#_Toc33026146 .anchor}**Теория оптимизации inference-процесса**

Квантизация моделей

Квантизация представляет собой процесс снижения точности представления
весов нейронной сети с целью уменьшения требований к памяти и ускорения
вычислений. Стандартные модели используют 32-битные числа с плавающей
точкой (FP32), в то время как квантизованные версии могут использовать
16-битные (FP16), 8-битные (INT8) или даже 4-битные (INT4)
представления.

Теоретически, квантизация с FP32 до INT8 снижает требования к памяти в 4
раза и может ускорить вычисления в 2-4 раза на поддерживающем
оборудовании. Однако это сопровождается некоторой потерей точности,
которая обычно составляет 1-3% для качественно выполненной квантизации.

Батчинг и параллелизация

Батчинг (batching) --- техника обработки нескольких запросов
одновременно, что позволяет эффективнее использовать параллельные
вычислительные возможности GPU. При обработке батча из B запросов,
эффективная пропускная способность может возрасти в k раз, где k \< B
из-за накладных расходов.

Оптимальный размер батча определяется балансом между латентностью
(временем ответа на один запрос) и пропускной способностью (количеством
запросов в единицу времени).

KV-кэширование

Key-Value кэширование --- техника оптимизации, при которой промежуточные
результаты вычислений (ключи и значения в механизме внимания)
сохраняются между итерациями генерации. Это позволяет избежать повторных
вычислений для уже обработанных токенов.

Для последовательности длиной N токенов, KV-кэш требует памяти размером:

Memory = 2 × N × L × H × D

где L --- количество слоев, H --- количество голов внимания, D ---
размерность головы.

[]{#_Toc1524643496 .anchor}**Теория безопасности локальных AI-систем**

Модель угроз для локальных развертываний

Локальные системы искусственного интеллекта подвержены специфическому
набору угроз, отличному от облачных решений:

Физический доступ --- возможность несанкционированного доступа к
оборудованию, на котором развернута система. Требует физической защиты
серверных помещений и контроля доступа.

Инсайдерские угрозы --- риски, связанные с действиями сотрудников
организации, имеющих легитимный доступ к системе. Требует разграничения
прав доступа и аудита действий.

Сетевые атаки --- попытки несанкционированного доступа через локальную
сеть. Требует сегментации сети и межсетевого экранирования.

Утечка через побочные каналы --- возможность извлечения информации через
анализ времени выполнения, энергопотребления или электромагнитного
излучения.

Принципы защиты данных

Минимизация поверхности атаки --- ограничение количества открытых
сетевых портов, отключение неиспользуемых сервисов, минимизация
установленного программного обеспечения.

Глубокая защита (Defense in Depth) --- применение множественных уровней
защиты, так что компрометация одного уровня не приводит к полной
компрометации системы.

Принцип наименьших привилегий --- предоставление каждому компоненту
системы минимально необходимых прав для выполнения своих функций.

Изоляция компонентов --- разделение системы на изолированные компоненты
с ограниченным взаимодейств

[]{#_Toc880474818 .anchor}**Теория эволюции и жизненного цикла
AI-систем**

[]{#_Toc1658927800 .anchor}**Модель зрелости AI-систем**

Уровень 1: Экспериментальный --- система используется для пилотных
проектов и proof-of-concept. Характеризуется ручной настройкой,
отсутствием автоматизации, ограниченным числом пользователей.

Уровень 2: Операционный --- система внедрена в производственную среду
для решения конкретных задач. Присутствует базовый мониторинг,
документация, процедуры обновления.

Уровень 3: Оптимизированный --- система интегрирована в бизнес-процессы,
автоматизированы процессы развертывания и обновления, ведется сбор
метрик эффективности.

Уровень 4: Адаптивный --- система способна к самообучению и адаптации
под изменяющиеся требования, присутствует автоматическая оптимизация
параметров, интеграция с множественными системами.

Стратегии обновления моделей

Полная замена --- старая версия модели полностью заменяется новой.
Простой подход, но может привести к резкому изменению поведения системы.

Постепенный переход (Canary Deployment) --- новая версия сначала
развертывается для небольшой части пользователей, затем постепенно
расширяется при отсутствии проблем.

A/B тестирование --- параллельная работа двух версий модели с
распределением пользователей между ними для сравнения эффективности.

Ансамблевый подход --- одновременное использование нескольких версий
моделей с агрегацией их результатов для повышения надежности.

**Заключение по главе**

Теоретические основы локального развертывания LLM/VLM систем охватывают
широкий спектр дисциплин: от распределенных вычислений и теории
информации до когнитивной психологии и организационного управления.
Понимание этих теоретических аспектов критически важно для создания
эффективных, безопасных и масштабируемых решений, способных
удовлетворить требования современных организаций к конфиденциальности,
производительности и надежности. Представленные концепции формируют
фундамент для практической реализации систем, способных конкурировать с
облачными решениями при сохранении полного контроля над данными и
инфраструктурой.
