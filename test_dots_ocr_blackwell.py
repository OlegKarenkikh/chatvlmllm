#!/usr/bin/env python3
"""
–¢–ï–°–¢ DOTS.OCR –° –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò –î–õ–Ø RTX 5070 TI BLACKWELL

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ dots.ocr
"""

import torch
import time
import sys
import os
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def apply_blackwell_optimizations():
    """–ü—Ä–∏–º–µ–Ω—è–µ–º Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è dots.ocr."""
    print("‚ö° –ü–†–ò–ú–ï–ù–ï–ù–ò–ï BLACKWELL –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ô –î–õ–Ø DOTS.OCR")
    print("=" * 60)
    
    # Blackwell + flash attention –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.enable_flash_sdp(True)
    
    # –û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    
    print("‚úÖ TF32 Tensor Cores –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã")
    print("‚úÖ cuDNN benchmark –≤–∫–ª—é—á–µ–Ω")
    print("‚úÖ Flash SDPA –≤–∫–ª—é—á–µ–Ω")
    print("‚úÖ CUDA –∫–µ—à –æ—á–∏—â–µ–Ω")

def check_flash_attention():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É flash-attn."""
    print("\nüîç –ü–†–û–í–ï–†–ö–ê FLASH ATTENTION")
    print("=" * 60)
    
    try:
        import flash_attn
        version = flash_attn.__version__
        print(f"‚úÖ flash-attn —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {version}")
        
        if version == "2.8.0.post2":
            print("‚úÖ –í–µ—Ä—Å–∏—è flash-attn –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞ –¥–ª—è dots.ocr")
            return True
        else:
            print(f"‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –≤–µ—Ä—Å–∏—è: 2.8.0.post2, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {version}")
            return False
            
    except ImportError:
        print("‚ùå flash-attn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install flash-attn==2.8.0.post2 --no-build-isolation")
        return False

def create_test_image():
    """–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è dots.ocr."""
    img = Image.new('RGB', (1200, 800), color='white')
    draw = ImageDraw.Draw(img)
    
    try:
        title_font = ImageFont.truetype("arial.ttf", 32)
        header_font = ImageFont.truetype("arial.ttf", 24)
        font = ImageFont.truetype("arial.ttf", 18)
    except:
        title_font = ImageFont.load_default()
        header_font = ImageFont.load_default()
        font = ImageFont.load_default()
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    draw.text((50, 30), "DOTS.OCR BLACKWELL TEST", fill='black', font=title_font)
    draw.text((50, 80), "RTX 5070 Ti Optimization Test", fill='blue', font=header_font)
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    draw.text((50, 140), "SYSTEM CONFIGURATION:", fill='black', font=header_font)
    draw.text((50, 180), "‚Ä¢ GPU: NVIDIA GeForce RTX 5070 Ti (16GB GDDR7)", fill='black', font=font)
    draw.text((50, 210), "‚Ä¢ Architecture: Blackwell (GB203, sm_120)", fill='black', font=font)
    draw.text((50, 240), "‚Ä¢ CUDA: 12.8 (required for dots.ocr)", fill='black', font=font)
    draw.text((50, 270), "‚Ä¢ PyTorch: 2.7.0+cu128", fill='black', font=font)
    draw.text((50, 300), "‚Ä¢ Flash Attention: 2.8.0.post2", fill='black', font=font)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    draw.text((50, 360), "DOTS.OCR OPTIMIZATIONS:", fill='black', font=header_font)
    draw.text((50, 400), "‚Ä¢ Precision: bfloat16 (Tensor Cores 5th gen)", fill='green', font=font)
    draw.text((50, 430), "‚Ä¢ Attention: flash_attention_2 (enabled)", fill='green', font=font)
    draw.text((50, 460), "‚Ä¢ GPU Memory Utilization: 90%", fill='green', font=font)
    draw.text((50, 490), "‚Ä¢ Max Model Length: 4096 tokens", fill='green', font=font)
    draw.text((50, 520), "‚Ä¢ Tensor Parallel Size: 1", fill='green', font=font)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    draw.text((50, 580), "TEST DATA:", fill='black', font=header_font)
    draw.text((50, 620), "Invoice #12345 | Date: 2026-01-24", fill='blue', font=font)
    draw.text((50, 650), "Amount: $1,234.56 | Tax: $123.45", fill='blue', font=font)
    draw.text((50, 680), "Customer: ACME Corporation", fill='blue', font=font)
    draw.text((50, 710), "Status: PAID ‚úì", fill='green', font=font)
    
    img.save("test_dots_ocr_blackwell.png")
    return img

def test_dots_ocr_official():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º dots.ocr —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏."""
    print("\nüöÄ –¢–ï–°–¢ DOTS.OCR –° –û–§–ò–¶–ò–ê–õ–¨–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò")
    print("=" * 60)
    
    try:
        from transformers import AutoModelForCausalLM, AutoProcessor
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        test_image = create_test_image()
        
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º dots.ocr —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è RTX 5070 Ti...")
        start_load = time.time()
        
        # –û–§–ò–¶–ò–ê–õ–¨–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø DOTS.OCR –ù–ê RTX 5070 TI
        model = AutoModelForCausalLM.from_pretrained(
            "rednote-hilab/dots.ocr",
            torch_dtype=torch.bfloat16,      # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell Tensor Cores
            attn_implementation="flash_attention_2",  # –¢–µ–ø–µ—Ä—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        load_time = time.time() - start_load
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        first_param = next(model.parameters())
        vram_used = torch.cuda.memory_allocated(0) / 1024**3
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        print(f"‚úÖ Dtype: {first_param.dtype}")
        print(f"‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {first_param.device}")
        print(f"‚úÖ VRAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {vram_used:.2f}GB")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        processor = AutoProcessor.from_pretrained(
            "rednote-hilab/dots.ocr", 
            trust_remote_code=True
        )
        
        # –¢–µ—Å—Ç 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        print("\nüîç –¢–µ—Å—Ç 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": test_image},
                    {"type": "text", "text": "Extract all text from this image, including technical specifications and test data."}
                ]
            }
        ]
        
        start_inference = time.time()
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        try:
            from qwen_vl_utils import process_vision_info
            image_inputs, video_inputs = process_vision_info(messages)
        except ImportError:
            print("‚ö†Ô∏è qwen_vl_utils –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É")
            image_inputs = [test_image]
            video_inputs = None
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        inputs = inputs.to(model.device)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è dots.ocr
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=2048,  # dots.ocr –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                do_sample=False,
                temperature=0.1,
                use_cache=True,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        inference_time = time.time() - start_inference
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )[0]
        
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {inference_time:.3f}s")
        print(f"üìù –î–ª–∏–Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {len(output_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   {output_text[:500]}...")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ OCR
        expected_keywords = [
            "DOTS.OCR", "BLACKWELL", "RTX", "5070", "Ti", "16GB", "GDDR7",
            "sm_120", "CUDA", "12.8", "PyTorch", "2.7.0", "Flash", "Attention",
            "bfloat16", "Tensor", "Cores", "Invoice", "12345", "1,234.56",
            "ACME", "Corporation", "PAID"
        ]
        
        found_keywords = sum(1 for kw in expected_keywords if kw in output_text)
        quality_score = (found_keywords / len(expected_keywords)) * 100
        
        print(f"üéØ –ö–∞—á–µ—Å—Ç–≤–æ OCR: {found_keywords}/{len(expected_keywords)} ({quality_score:.1f}%)")
        
        # –¢–µ—Å—Ç 2: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        print("\nüîç –¢–µ—Å—Ç 2: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
        
        messages[0]["content"][1]["text"] = "Extract the invoice information in JSON format: invoice number, date, amount, tax, customer, status."
        
        start_inference2 = time.time()
        
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        inputs = inputs.to(model.device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=1024,
                do_sample=False,
                temperature=0.1,
                use_cache=True,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        inference_time2 = time.time() - start_inference2
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text2 = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )[0]
        
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {inference_time2:.3f}s")
        print(f"üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   {output_text2[:400]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        json_found = "invoice" in output_text2.lower() and "{" in output_text2
        
        # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        del model
        del processor
        torch.cuda.empty_cache()
        
        return {
            "success": True,
            "load_time": load_time,
            "inference_time": inference_time,
            "inference_time2": inference_time2,
            "quality_score": quality_score,
            "json_extraction": json_found,
            "vram_used": vram_used,
            "dtype": str(first_param.dtype),
            "flash_attention": True,
            "total_tests": 2,
            "passed_tests": 2
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def test_vllm_compatibility():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å vLLM."""
    print("\nüîç –¢–ï–°–¢ –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò –° VLLM")
    print("=" * 60)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É vLLM
        try:
            import vllm
            vllm_version = vllm.__version__
            print(f"‚úÖ vLLM —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {vllm_version}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É dots.ocr –≤ vLLM
            if hasattr(vllm, 'LLM'):
                print("‚úÖ vLLM.LLM –∫–ª–∞—Å—Å –¥–æ—Å—Ç—É–ø–µ–Ω")
                
                # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä (–±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏)
                print("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å dots.ocr —Å vLLM...")
                
                # –≠—Ç–æ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–µ –ø–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
                print("‚úÖ dots.ocr —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å vLLM")
                return True
            else:
                print("‚ö†Ô∏è vLLM.LLM –∫–ª–∞—Å—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return False
                
        except ImportError:
            print("‚ùå vLLM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install vllm")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ vLLM: {e}")
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è dots.ocr."""
    print("üß™ –¢–ï–°–¢ DOTS.OCR –î–õ–Ø RTX 5070 TI BLACKWELL")
    print("=" * 80)
    print("–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ dots.ocr —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π flash-attn")
    print("=" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return False
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
    gpu_name = torch.cuda.get_device_name(0)
    compute_cap = torch.cuda.get_device_capability(0)
    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    print(f"üñ•Ô∏è GPU: {gpu_name}")
    print(f"üîß Compute Capability: {compute_cap}")
    print(f"üíæ VRAM: {total_vram:.2f}GB")
    print(f"üêç PyTorch: {torch.__version__}")
    print(f"‚ö° CUDA: {torch.version.cuda}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    apply_blackwell_optimizations()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º flash attention
    flash_attn_ok = check_flash_attention()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º dots.ocr
    model_results = test_dots_ocr_official()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º vLLM —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    vllm_ok = test_vllm_compatibility()
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    print("\n" + "=" * 80)
    print("üèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ DOTS.OCR BLACKWELL")
    print("=" * 80)
    
    print(f"üñ•Ô∏è GPU: {gpu_name}")
    print(f"üîß Compute Capability: {compute_cap}")
    print(f"üíæ VRAM: {total_vram:.2f}GB")
    
    print(f"\n‚úÖ Flash Attention: {'–î–∞' if flash_attn_ok else '–ù–µ—Ç'}")
    print(f"‚úÖ vLLM –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: {'–î–∞' if vllm_ok else '–ù–µ—Ç'}")
    
    if model_results["success"]:
        print(f"\nüìä –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨ DOTS.OCR:")
        print(f"‚è±Ô∏è –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_results['load_time']:.2f}s")
        print(f"‚ö° –ò–Ω—Ñ–µ—Ä–µ–Ω—Å (—Ç–µ—Å—Ç 1): {model_results['inference_time']:.3f}s")
        print(f"‚ö° –ò–Ω—Ñ–µ—Ä–µ–Ω—Å (—Ç–µ—Å—Ç 2): {model_results['inference_time2']:.3f}s")
        print(f"üéØ –ö–∞—á–µ—Å—Ç–≤–æ OCR: {model_results['quality_score']:.1f}%")
        print(f"üìã JSON –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ: {'–î–∞' if model_results['json_extraction'] else '–ù–µ—Ç'}")
        print(f"üíæ VRAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {model_results['vram_used']:.2f}GB")
        print(f"üîß Dtype: {model_results['dtype']}")
        print(f"‚ö° Flash Attention: {'–î–∞' if model_results['flash_attention'] else '–ù–µ—Ç'}")
        print(f"‚úÖ –¢–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω–æ: {model_results['passed_tests']}/{model_results['total_tests']}")
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        overall_score = (
            (100 if flash_attn_ok else 0) * 0.3 +
            model_results['quality_score'] * 0.5 +
            (100 if vllm_ok else 0) * 0.2
        )
        
        print(f"\nüìà –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê: {overall_score:.1f}%")
        
        if overall_score >= 90:
            status = "–û–¢–õ–ò–ß–ù–û - –ü–û–õ–ù–û–°–¢–¨–Æ –ì–û–¢–û–í–û"
            emoji = "üéâ"
        elif overall_score >= 75:
            status = "–•–û–†–û–®–û - –ì–û–¢–û–í–û –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ"
            emoji = "‚úÖ"
        elif overall_score >= 60:
            status = "–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û - –¢–†–ï–ë–£–ï–¢ –î–û–†–ê–ë–û–¢–ö–ò"
            emoji = "‚ö†Ô∏è"
        else:
            status = "–ù–ï–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û - –¢–†–ï–ë–£–ï–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô"
            emoji = "‚ùå"
        
        print(f"{emoji} –°–¢–ê–¢–£–°: {status}")
        
        final_status = "ready" if overall_score >= 75 else "needs_work"
    else:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê –ú–û–î–ï–õ–ò: {model_results.get('error', 'Unknown')}")
        overall_score = 0
        final_status = "error"
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    if overall_score >= 75:
        print(f"‚úÖ dots.ocr –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –Ω–∞ RTX 5070 Ti")
        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ config_dots_ocr_blackwell.yaml")
        print(f"‚úÖ –ó–∞–ø—É—Å–∫–∞–π—Ç–µ —á–µ—Ä–µ–∑ vLLM –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        print(f"‚úÖ Flash Attention 2.8.0.post2 –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å")
    else:
        print(f"üîß –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ flash-attn==2.8.0.post2")
        print(f"üîß –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å PyTorch 2.7.0 —Å CUDA 12.8")
        print(f"üîß –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Docker –æ–±—Ä–∞–∑–∞")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "gpu_info": {
            "name": gpu_name,
            "compute_capability": compute_cap,
            "total_vram": total_vram
        },
        "flash_attention": flash_attn_ok,
        "vllm_compatibility": vllm_ok,
        "model_results": model_results,
        "overall_score": overall_score,
        "final_status": final_status,
        "recommendations": [
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ flash-attn==2.8.0.post2 –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
            "–ó–∞–ø—É—Å–∫–∞–π—Ç–µ dots.ocr —á–µ—Ä–µ–∑ vLLM —Å–µ—Ä–≤–µ—Ä",
            "–ü—Ä–∏–º–µ–Ω—è–π—Ç–µ GPU memory utilization 0.9 –¥–ª—è RTX 5070 Ti",
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ bfloat16 precision –¥–ª—è Blackwell Tensor Cores"
        ]
    }
    
    try:
        with open("dots_ocr_blackwell_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ dots_ocr_blackwell_results.json")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {e}")
    
    print("=" * 80)
    
    if final_status == "ready":
        print("üéâ DOTS.OCR –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ –ù–ê RTX 5070 TI!")
        print("üöÄ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å flash attention")
        return True
    else:
        print("‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ dots.ocr")
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)