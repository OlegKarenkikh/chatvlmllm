#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è 5 –º–æ–¥–µ–ª–µ–π vLLM
"""

import json
import subprocess
import time
import requests
import base64
import os
from pathlib import Path
from typing import Dict, List, Any

class RemainingModelsTester:
    def __init__(self):
        self.cache_path = str(os.path.expanduser("~/.cache/huggingface/hub")).replace('\\', '/')
        self.test_image = "test_documents/01_simple_text.png"
        
        # –¢–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å
        self.models_to_test = {
            "Qwen/Qwen2-VL-2B-Instruct": {
                "container_name": "qwen2-vl-2b-instruct-vllm",
                "port": 8011,
                "vllm_params": {
                    "max_model_len": 4096,
                    "gpu_memory_utilization": 0.7,
                    "trust_remote_code": True,
                    "enforce_eager": False
                }
            },
            "Qwen/Qwen2-VL-7B-Instruct": {
                "container_name": "qwen2-vl-7b-instruct-vllm",
                "port": 8013,
                "vllm_params": {
                    "max_model_len": 4096,
                    "gpu_memory_utilization": 0.6,
                    "trust_remote_code": True,
                    "enforce_eager": False
                }
            },
            "microsoft/Phi-3.5-vision-instruct": {
                "container_name": "phi-3-5-vision-instruct-vllm",
                "port": 8014,
                "vllm_params": {
                    "max_model_len": 4096,
                    "gpu_memory_utilization": 0.6,
                    "trust_remote_code": True,
                    "enforce_eager": False
                }
            },
            "stepfun-ai/GOT-OCR-2.0-hf": {
                "container_name": "got-ocr-2-0-hf-vllm",
                "port": 8002,
                "vllm_params": {
                    "max_model_len": 2048,
                    "gpu_memory_utilization": 0.7,
                    "trust_remote_code": True,
                    "enforce_eager": True
                }
            },
            "vikhyatk/moondream2": {
                "container_name": "moondream2-vllm",
                "port": 8023,
                "vllm_params": {
                    "max_model_len": 2048,
                    "gpu_memory_utilization": 0.6,
                    "trust_remote_code": True,
                    "enforce_eager": True
                }
            }
        }
        
    def run_command(self, command):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
        try:
            result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
            return True, result.stdout.strip()
        except subprocess.CalledProcessError as e:
            return False, e.stderr.strip() if e.stderr else str(e)
    
    def check_gpu_memory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU"""
        success, output = self.run_command("nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv,noheader,nounits")
        
        if success:
            lines = output.strip().split('\n')
            for i, line in enumerate(lines):
                parts = line.split(', ')
                if len(parts) == 3:
                    total, used, free = map(int, parts)
                    return {
                        'total_mb': total,
                        'used_mb': used,
                        'free_mb': free,
                        'usage_percent': round((used / total) * 100, 1)
                    }
        return None
    
    def cleanup_containers(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ vLLM"""
        print("üßπ –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤...")
        success, output = self.run_command("docker ps -a --filter ancestor=vllm/vllm-openai:latest --format {{.Names}}")
        
        if success and output:
            container_names = output.strip().split('\n')
            for container_name in container_names:
                if container_name:
                    self.run_command(f"docker stop {container_name}")
                    self.run_command(f"docker rm {container_name}")
                    print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω {container_name}")
    
    def encode_image(self, image_path: str) -> str:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None
    
    def test_model_launch(self, model_name: str, config: Dict, timeout: int = 180) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"\nüß™ –¢–ï–°–¢ –ó–ê–ü–£–°–ö–ê: {model_name}")
        print("-" * 50)
        
        container_name = config['container_name']
        port = config['port']
        vllm_params = config['vllm_params']
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        gpu_info = self.check_gpu_memory()
        if gpu_info:
            print(f"üíæ GPU –¥–æ –∑–∞–ø—É—Å–∫–∞: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã Docker
        docker_command = f"""
        docker run -d \
            --gpus all \
            --name {container_name} \
            -p {port}:{port} \
            -v {self.cache_path}:/root/.cache/huggingface/hub:ro \
            --shm-size=8g \
            vllm/vllm-openai:latest \
            --model {model_name} \
            --trust-remote-code \
            --max-model-len {vllm_params['max_model_len']} \
            --gpu-memory-utilization {vllm_params['gpu_memory_utilization']} \
            --host 0.0.0.0 \
            --port {port} \
            --disable-log-requests
        """.strip().replace('\n', ' ').replace('\\', '')
        
        if vllm_params.get('enforce_eager'):
            docker_command += " --enforce-eager"
        
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ {container_name}...")
        
        # –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        success, output = self.run_command(docker_command)
        
        if not success:
            return {
                "launch_success": False,
                "error": f"Container start failed: {output}",
                "error_type": "container_start_error"
            }
        
        print(f"üì¶ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω, –æ–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏...")
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ª–æ–≥–æ–≤
        start_time = time.time()
        last_log_check = start_time
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=5)
                if response.status_code == 200:
                    launch_time = time.time() - start_time
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∑–∞ {int(launch_time)} —Å–µ–∫—É–Ω–¥!")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞
                    gpu_info = self.check_gpu_memory()
                    if gpu_info:
                        print(f"üíæ GPU –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
                    
                    return {
                        "launch_success": True,
                        "launch_time": launch_time,
                        "gpu_memory_after": gpu_info
                    }
                    
            except requests.exceptions.ConnectionError:
                pass
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            current_time = time.time()
            if current_time - last_log_check > 30:
                elapsed = int(current_time - start_time)
                print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {elapsed}/{timeout}—Å...")
                
                # –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ—à–∏–±–æ–∫
                success_log, logs = self.run_command(f"docker logs {container_name} --tail 10")
                if success_log and logs:
                    # –ü–æ–∏—Å–∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
                    if "ImportError" in logs:
                        error_lines = [line for line in logs.split('\n') if 'ImportError' in line]
                        return {
                            "launch_success": False,
                            "error": f"Import error: {error_lines[-1] if error_lines else 'Unknown import error'}",
                            "error_type": "import_error",
                            "logs": logs
                        }
                    elif "ModuleNotFoundError" in logs:
                        error_lines = [line for line in logs.split('\n') if 'ModuleNotFoundError' in line]
                        return {
                            "launch_success": False,
                            "error": f"Module not found: {error_lines[-1] if error_lines else 'Unknown module error'}",
                            "error_type": "module_error",
                            "logs": logs
                        }
                    elif "CUDA out of memory" in logs:
                        return {
                            "launch_success": False,
                            "error": "CUDA out of memory",
                            "error_type": "memory_error",
                            "logs": logs
                        }
                
                last_log_check = current_time
            
            time.sleep(10)
        
        # –¢–∞–π–º–∞—É—Ç - –ø–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ª–æ–≥–æ–≤
        success_log, logs = self.run_command(f"docker logs {container_name} --tail 20")
        
        return {
            "launch_success": False,
            "error": f"Timeout after {timeout} seconds",
            "error_type": "timeout",
            "logs": logs if success_log else "No logs available"
        }
    
    def test_model_functionality(self, model_name: str, port: int) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        print(f"üîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...")
        
        results = {}
        
        # 1. –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ç–µ—Å—Ç
        try:
            payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": "Hello, how are you?"}],
                "max_tokens": 50,
                "temperature": 0.1
            }
            
            start_time = time.time()
            response = requests.post(
                f"http://localhost:{port}/v1/chat/completions",
                json=payload,
                timeout=30
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                text = result["choices"][0]["message"]["content"]
                
                results["text_test"] = {
                    "success": True,
                    "response": text,
                    "processing_time": round(processing_time, 2),
                    "usage": result.get("usage", {})
                }
                print(f"   ‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ç–µ—Å—Ç: {text[:50]}...")
            else:
                results["text_test"] = {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text[:200]}"
                }
                print(f"   ‚ùå –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ç–µ—Å—Ç –Ω–µ—É–¥–∞—á–µ–Ω")
                
        except Exception as e:
            results["text_test"] = {
                "success": False,
                "error": str(e)
            }
            print(f"   ‚ùå –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ç–µ—Å—Ç –æ—à–∏–±–∫–∞: {e}")
        
        # 2. –¢–µ—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if os.path.exists(self.test_image):
            try:
                image_base64 = self.encode_image(self.test_image)
                if image_base64:
                    payload = {
                        "model": model_name,
                        "messages": [
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": "What do you see in this image?"},
                                    {
                                        "type": "image_url",
                                        "image_url": {"url": f"data:image/png;base64,{image_base64}"}
                                    }
                                ]
                            }
                        ],
                        "max_tokens": 100,
                        "temperature": 0.1
                    }
                    
                    start_time = time.time()
                    response = requests.post(
                        f"http://localhost:{port}/v1/chat/completions",
                        json=payload,
                        timeout=60
                    )
                    processing_time = time.time() - start_time
                    
                    if response.status_code == 200:
                        result = response.json()
                        text = result["choices"][0]["message"]["content"]
                        
                        results["vision_test"] = {
                            "success": True,
                            "response": text,
                            "processing_time": round(processing_time, 2),
                            "usage": result.get("usage", {})
                        }
                        print(f"   ‚úÖ Vision —Ç–µ—Å—Ç: {text[:50]}...")
                    else:
                        results["vision_test"] = {
                            "success": False,
                            "error": f"HTTP {response.status_code}: {response.text[:200]}"
                        }
                        print(f"   ‚ùå Vision —Ç–µ—Å—Ç –Ω–µ—É–¥–∞—á–µ–Ω")
                        
            except Exception as e:
                results["vision_test"] = {
                    "success": False,
                    "error": str(e)
                }
                print(f"   ‚ùå Vision —Ç–µ—Å—Ç –æ—à–∏–±–∫–∞: {e}")
        
        return results
    
    def cleanup_model(self, container_name: str):
        """–û—á–∏—Å—Ç–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"üßπ –û—á–∏—Å—Ç–∫–∞ {container_name}...")
        self.run_command(f"docker stop {container_name}")
        self.run_command(f"docker rm {container_name}")
    
    def run_tests(self):
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üî¨ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–°–¢–ê–í–®–ò–•–°–Ø 5 –ú–û–î–ï–õ–ï–ô")
        print("=" * 50)
        
        # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        self.cleanup_containers()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
        gpu_info = self.check_gpu_memory()
        if gpu_info:
            print(f"üéÆ GPU: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
        
        all_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "models": {},
            "summary": {
                "total_tested": 0,
                "successful": 0,
                "failed": 0
            }
        }
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for i, (model_name, config) in enumerate(self.models_to_test.items(), 1):
            print(f"\n{'='*70}")
            print(f"üîÑ –ú–û–î–ï–õ–¨ {i}/{len(self.models_to_test)}: {model_name}")
            print(f"{'='*70}")
            
            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞
            launch_result = self.test_model_launch(model_name, config)
            all_results["summary"]["total_tested"] += 1
            
            model_result = {
                "config": config,
                "launch_result": launch_result,
                "functionality_result": None
            }
            
            if launch_result["launch_success"]:
                print(f"‚úÖ –ó–∞–ø—É—Å–∫ —É—Å–ø–µ—à–µ–Ω!")
                
                # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                functionality_result = self.test_model_functionality(model_name, config['port'])
                model_result["functionality_result"] = functionality_result
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
                text_success = functionality_result.get("text_test", {}).get("success", False)
                vision_success = functionality_result.get("vision_test", {}).get("success", False)
                
                if text_success or vision_success:
                    print(f"‚úÖ –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç!")
                    all_results["summary"]["successful"] += 1
                    model_result["status"] = "working"
                else:
                    print(f"‚ùå –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
                    all_results["summary"]["failed"] += 1
                    model_result["status"] = "launch_ok_function_fail"
                
                # –û—á–∏—Å—Ç–∫–∞
                self.cleanup_model(config['container_name'])
                
            else:
                print(f"‚ùå –ó–∞–ø—É—Å–∫ –Ω–µ—É–¥–∞—á–µ–Ω: {launch_result['error']}")
                all_results["summary"]["failed"] += 1
                model_result["status"] = "launch_failed"
                
                # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                self.cleanup_model(config['container_name'])
            
            all_results["models"][model_name] = model_result
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
            if i < len(self.models_to_test):
                print(f"\n‚è∏Ô∏è –ü–∞—É–∑–∞ 5 —Å–µ–∫—É–Ω–¥...")
                time.sleep(5)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        self.cleanup_containers()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = f"remaining_models_test_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        # –ü–æ–∫–∞–∑ –∏—Ç–æ–≥–æ–≤
        self.show_final_summary(all_results)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_file}")
        
        return all_results
    
    def show_final_summary(self, results: Dict[str, Any]):
        """–ü–æ–∫–∞–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        summary = results["summary"]
        
        print(f"\nüèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
        print("=" * 30)
        print(f"üìä –í—Å–µ–≥–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {summary['total_tested']}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç: {summary['successful']}")
        print(f"‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞—é—Ç: {summary['failed']}")
        
        # –°–ø–∏—Å–æ–∫ —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
        working_models = [name for name, result in results["models"].items() 
                         if result.get("status") == "working"]
        
        if working_models:
            print(f"\nüéâ –†–ê–ë–û–¢–ê–Æ–©–ò–ï –ú–û–î–ï–õ–ò:")
            for model_name in working_models:
                config = results["models"][model_name]["config"]
                print(f"   ‚úÖ {model_name} (–ø–æ—Ä—Ç {config['port']})")
        
        # –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        failed_models = [name for name, result in results["models"].items() 
                        if result.get("status") in ["launch_failed", "launch_ok_function_fail"]]
        
        if failed_models:
            print(f"\n‚ùå –ü–†–û–ë–õ–ï–ú–ù–´–ï –ú–û–î–ï–õ–ò:")
            for model_name in failed_models:
                result = results["models"][model_name]
                error = result["launch_result"].get("error", "Unknown error")
                print(f"   ‚ùå {model_name}: {error[:100]}...")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    tester = RemainingModelsTester()
    
    print("üöÄ –ó–ê–ü–£–°–ö –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –û–°–¢–ê–í–®–ò–•–°–Ø –ú–û–î–ï–õ–ï–ô")
    print("=" * 45)
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    results = tester.run_tests()
    
    print(f"\nüéâ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")

if __name__ == "__main__":
    main()