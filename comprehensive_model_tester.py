#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π vLLM –ø–æ –æ—á–µ—Ä–µ–¥–∏
"""

import json
import subprocess
import time
import requests
import base64
import os
from pathlib import Path
from typing import Dict, List, Any

class ComprehensiveModelTester:
    def __init__(self):
        self.cache_path = str(os.path.expanduser("~/.cache/huggingface/hub")).replace('\\', '/')
        self.test_images = [
            "test_documents/01_simple_text.png",
            "test_documents/02_table.png", 
            "test_documents/04_numbers.png"
        ]
        self.text_prompts = [
            "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?",
            "–û–ø–∏—à–∏ –ø—Ä–æ—Ü–µ—Å—Å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫—Ä–∞—Ç–∫–æ"
        ]
        self.results = {}
        
    def run_command(self, command):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
        try:
            result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
            return True, result.stdout.strip()
        except subprocess.CalledProcessError as e:
            return False, e.stderr.strip() if e.stderr else str(e)
    
    def check_gpu_memory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU"""
        success, output = self.run_command("nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv,noheader,nounits")
        
        if success:
            lines = output.strip().split('\n')
            for i, line in enumerate(lines):
                parts = line.split(', ')
                if len(parts) == 3:
                    total, used, free = map(int, parts)
                    return {
                        'total_mb': total,
                        'used_mb': used,
                        'free_mb': free,
                        'usage_percent': round((used / total) * 100, 1)
                    }
        return None
    
    def launch_model(self, model_name, config, timeout=300):
        """–ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏"""
        print(f"\nüöÄ –ó–ê–ü–£–°–ö –ú–û–î–ï–õ–ò: {model_name}")
        print("=" * 50)
        
        container_name = config['container_name']
        port = config['port']
        vllm_params = config['vllm_params']
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        self.run_command(f"docker stop {container_name}")
        self.run_command(f"docker rm {container_name}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
        gpu_info = self.check_gpu_memory()
        if gpu_info:
            print(f"üíæ GPU –ø–∞–º—è—Ç—å: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
            print(f"üíæ –°–≤–æ–±–æ–¥–Ω–æ: {gpu_info['free_mb']} –ú–ë")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã Docker
        docker_command = f"""
        docker run -d \
            --gpus all \
            --name {container_name} \
            -p {port}:{port} \
            -v {self.cache_path}:/root/.cache/huggingface/hub:ro \
            --shm-size=8g \
            vllm/vllm-openai:latest \
            --model {model_name} \
            --trust-remote-code \
            --max-model-len {vllm_params['max_model_len']} \
            --gpu-memory-utilization {vllm_params['gpu_memory_utilization']} \
            --host 0.0.0.0 \
            --port {port} \
            --disable-log-requests
        """.strip().replace('\n', ' ').replace('\\', '')
        
        if vllm_params.get('enforce_eager'):
            docker_command += " --enforce-eager"
        
        print(f"üì¶ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä: {container_name}")
        print(f"üåê –ü–æ—Ä—Ç: {port}")
        print(f"üíæ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {config['size_gb']} –ì–ë")
        print(f"‚öôÔ∏è Max tokens: {vllm_params['max_model_len']}")
        print(f"üéÆ GPU utilization: {vllm_params['gpu_memory_utilization']}")
        
        # –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        success, output = self.run_command(docker_command)
        
        if not success:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: {output}")
            return False
        
        print(f"‚úÖ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω, –æ–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏...")
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        start_time = time.time()
        last_log_time = start_time
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=5)
                if response.status_code == 200:
                    launch_time = time.time() - start_time
                    print(f"üéâ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∑–∞ {int(launch_time)} —Å–µ–∫—É–Ω–¥!")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞
                    gpu_info = self.check_gpu_memory()
                    if gpu_info:
                        print(f"üíæ GPU –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
                    
                    return True
                    
            except requests.exceptions.ConnectionError:
                pass
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            
            # –ü–æ–∫–∞–∑ –ª–æ–≥–æ–≤ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            current_time = time.time()
            if current_time - last_log_time > 30:
                elapsed = int(current_time - start_time)
                print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {elapsed}/{timeout}—Å...")
                
                # –ü–æ–∫–∞–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ª–æ–≥–æ–≤
                success_log, logs = self.run_command(f"docker logs {container_name} --tail 3")
                if success_log and logs:
                    print(f"üìã –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏: {logs.split()[-1] if logs.split() else '–Ω–µ—Ç –ª–æ–≥–æ–≤'}")
                
                last_log_time = current_time
            
            time.sleep(10)
        
        # –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª–∞—Å—å
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –≥–æ—Ç–æ–≤–∞ –∑–∞ {timeout} —Å–µ–∫—É–Ω–¥")
        print(f"üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤...")
        
        success_log, logs = self.run_command(f"docker logs {container_name} --tail 20")
        if success_log:
            print("–ü–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏:")
            print(logs)
        
        return False
    
    def test_model_ocr(self, model_name, port, image_path):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ OCR"""
        if not Path(image_path).exists():
            return {"success": False, "error": f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {image_path}"}
        
        try:
            with open(image_path, "rb") as f:
                image_data = f.read()
            
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            payload = {
                "model": model_name,
                "messages": [{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Extract all text from this image"},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}
                    ]
                }],
                "max_tokens": 1000,
                "temperature": 0.1
            }
            
            start_time = time.time()
            response = requests.post(
                f"http://localhost:{port}/v1/chat/completions",
                json=payload,
                timeout=120
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                text = result["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "text": text,
                    "processing_time": round(processing_time, 2),
                    "word_count": len(text.split()),
                    "char_count": len(text),
                    "usage": result.get("usage", {}),
                    "image": Path(image_path).name
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text[:200]}"
                }
                
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def test_model_text(self, model_name, port, prompt):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
        try:
            payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 500,
                "temperature": 0.1
            }
            
            start_time = time.time()
            response = requests.post(
                f"http://localhost:{port}/v1/chat/completions",
                json=payload,
                timeout=60
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                text = result["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "text": text,
                    "processing_time": round(processing_time, 2),
                    "word_count": len(text.split()),
                    "usage": result.get("usage", {}),
                    "prompt": prompt[:30] + "..."
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text[:200]}"
                }
                
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def test_single_model(self, model_name, config):
        """–ü–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print(f"\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò: {model_name}")
        print("=" * 60)
        
        # –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏
        if not self.launch_model(model_name, config):
            return {
                "model_name": model_name,
                "launch_success": False,
                "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å",
                "ocr_tests": [],
                "text_tests": [],
                "performance": {}
            }
        
        port = config['port']
        model_results = {
            "model_name": model_name,
            "launch_success": True,
            "config": config,
            "ocr_tests": [],
            "text_tests": [],
            "performance": {}
        }
        
        # OCR —Ç–µ—Å—Ç—ã
        print(f"\nüì∑ OCR –¢–ï–°–¢–´")
        print("-" * 15)
        
        ocr_times = []
        ocr_successes = 0
        
        for image_path in self.test_images:
            if Path(image_path).exists():
                image_name = Path(image_path).name
                print(f"   üñºÔ∏è {image_name}...", end=" ")
                
                result = self.test_model_ocr(model_name, port, image_path)
                model_results["ocr_tests"].append(result)
                
                if result["success"]:
                    ocr_successes += 1
                    ocr_times.append(result["processing_time"])
                    print(f"‚úÖ {result['processing_time']}—Å, {result['word_count']} —Å–ª–æ–≤")
                else:
                    print(f"‚ùå {result['error'][:50]}...")
        
        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–µ—Å—Ç—ã
        print(f"\nüí¨ –¢–ï–ö–°–¢–û–í–´–ï –¢–ï–°–¢–´")
        print("-" * 20)
        
        text_times = []
        text_successes = 0
        
        for prompt in self.text_prompts:
            print(f"   üìù {prompt[:30]}...", end=" ")
            
            result = self.test_model_text(model_name, port, prompt)
            model_results["text_tests"].append(result)
            
            if result["success"]:
                text_successes += 1
                text_times.append(result["processing_time"])
                print(f"‚úÖ {result['processing_time']}—Å, {result['word_count']} —Å–ª–æ–≤")
            else:
                print(f"‚ùå {result['error'][:50]}...")
        
        # –†–∞—Å—á–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        model_results["performance"] = {
            "ocr_success_rate": round((ocr_successes / len(self.test_images)) * 100, 1) if self.test_images else 0,
            "ocr_avg_time": round(sum(ocr_times) / len(ocr_times), 2) if ocr_times else 0,
            "ocr_avg_words": round(sum(r["word_count"] for r in model_results["ocr_tests"] if r["success"]) / len([r for r in model_results["ocr_tests"] if r["success"]]), 1) if [r for r in model_results["ocr_tests"] if r["success"]] else 0,
            "text_success_rate": round((text_successes / len(self.text_prompts)) * 100, 1),
            "text_avg_time": round(sum(text_times) / len(text_times), 2) if text_times else 0,
            "text_avg_words": round(sum(r["word_count"] for r in model_results["text_tests"] if r["success"]) / len([r for r in model_results["text_tests"] if r["success"]]), 1) if [r for r in model_results["text_tests"] if r["success"]] else 0,
            "total_tests": len(self.test_images) + len(self.text_prompts),
            "total_successes": ocr_successes + text_successes
        }
        
        # –ü–æ–∫–∞–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        perf = model_results["performance"]
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   üì∑ OCR: {perf['ocr_success_rate']}% —É—Å–ø–µ—Ö, {perf['ocr_avg_time']}—Å —Å—Ä–µ–¥–Ω–µ–µ, {perf['ocr_avg_words']} —Å–ª–æ–≤")
        print(f"   üí¨ –¢–µ–∫—Å—Ç: {perf['text_success_rate']}% —É—Å–ø–µ—Ö, {perf['text_avg_time']}—Å —Å—Ä–µ–¥–Ω–µ–µ, {perf['text_avg_words']} —Å–ª–æ–≤")
        print(f"   üéØ –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {perf['total_successes']}/{perf['total_tests']}")
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏
        container_name = config['container_name']
        print(f"\nüõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ {container_name}...")
        self.run_command(f"docker stop {container_name}")
        self.run_command(f"docker rm {container_name}")
        
        return model_results
    
    def run_comprehensive_test(self):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üî¨ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–°–ï–• –ú–û–î–ï–õ–ï–ô vLLM")
        print("=" * 55)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
        try:
            with open('vllm_models_config.json', 'r', encoding='utf-8') as f:
                configs = json.load(f)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {e}")
            return
        
        # –ü–æ—Ä—è–¥–æ–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–æ—Ç –ª–µ–≥–∫–∏—Ö –∫ —Ç—è–∂–µ–ª—ã–º)
        test_order = [
            "rednote-hilab/dots.ocr",
            "Qwen/Qwen3-VL-2B-Instruct",
            "Qwen/Qwen2-VL-2B-Instruct", 
            "Qwen/Qwen2.5-VL-7B-Instruct",
            "microsoft/Phi-3.5-vision-instruct",
            "Qwen/Qwen2-VL-7B-Instruct"
        ]
        
        print(f"üìã –ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len([m for m in test_order if m in configs])} –º–æ–¥–µ–ª–µ–π")
        print(f"‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: {len([m for m in test_order if m in configs]) * 10} –º–∏–Ω—É—Ç")
        
        all_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "models": {},
            "summary": {}
        }
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for i, model_name in enumerate(test_order, 1):
            if model_name not in configs:
                print(f"‚ö†Ô∏è {model_name} - –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
                continue
            
            print(f"\n{'='*70}")
            print(f"üîÑ –ú–û–î–ï–õ–¨ {i}/{len([m for m in test_order if m in configs])}: {model_name}")
            print(f"{'='*70}")
            
            config = configs[model_name]
            result = self.test_single_model(model_name, config)
            all_results["models"][model_name] = result
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏
            if i < len([m for m in test_order if m in configs]):
                print(f"\n‚è∏Ô∏è –ü–∞—É–∑–∞ 10 —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –º–æ–¥–µ–ª—å—é...")
                time.sleep(10)
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.analyze_all_results(all_results)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"comprehensive_vllm_test_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filename}")
        
        return all_results
    
    def analyze_all_results(self, results):
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print(f"\nüèÜ –ò–¢–û–ì–û–í–´–ô –ê–ù–ê–õ–ò–ó –í–°–ï–• –ú–û–î–ï–õ–ï–ô")
        print("=" * 40)
        
        successful_models = []
        failed_models = []
        
        for model_name, result in results["models"].items():
            if result["launch_success"]:
                successful_models.append((model_name, result))
            else:
                failed_models.append(model_name)
        
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(successful_models)} –º–æ–¥–µ–ª–µ–π")
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å: {len(failed_models)} –º–æ–¥–µ–ª–µ–π")
        
        if failed_models:
            print(f"\n‚ùå –ù–ï–£–î–ê–ß–ù–´–ï –ú–û–î–ï–õ–ò:")
            for model_name in failed_models:
                print(f"   ‚Ä¢ {model_name}")
        
        if successful_models:
            print(f"\nüìä –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô:")
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –æ–±—â–µ–º—É —É—Å–ø–µ—Ö—É
            successful_models.sort(key=lambda x: x[1]["performance"]["total_successes"], reverse=True)
            
            print(f"\nüéØ –ü–û –û–ë–©–ï–ú–£ –£–°–ü–ï–•–£:")
            for i, (model_name, result) in enumerate(successful_models, 1):
                perf = result["performance"]
                print(f"   {i}. {model_name}")
                print(f"      –£—Å–ø–µ—Ö: {perf['total_successes']}/{perf['total_tests']}")
                print(f"      OCR: {perf['ocr_success_rate']}% ({perf['ocr_avg_time']}—Å)")
                print(f"      –¢–µ–∫—Å—Ç: {perf['text_success_rate']}% ({perf['text_avg_time']}—Å)")
            
            # –õ—É—á—à–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            best_ocr = max(successful_models, key=lambda x: x[1]["performance"]["ocr_success_rate"])
            fastest_ocr = min([m for m in successful_models if m[1]["performance"]["ocr_avg_time"] > 0], 
                            key=lambda x: x[1]["performance"]["ocr_avg_time"])
            best_text = max(successful_models, key=lambda x: x[1]["performance"]["text_avg_words"])
            
            print(f"\nüèÖ –õ–£–ß–®–ò–ï –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
            print(f"   üì∑ –õ—É—á—à–∏–π OCR: {best_ocr[0]} ({best_ocr[1]['performance']['ocr_success_rate']}%)")
            print(f"   ‚ö° –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π OCR: {fastest_ocr[0]} ({fastest_ocr[1]['performance']['ocr_avg_time']}—Å)")
            print(f"   üí¨ –õ—É—á—à–∏–π —Ç–µ–∫—Å—Ç: {best_text[0]} ({best_text[1]['performance']['text_avg_words']} —Å–ª–æ–≤)")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏
            results["summary"] = {
                "total_tested": len(successful_models),
                "total_failed": len(failed_models),
                "best_ocr_model": best_ocr[0],
                "fastest_ocr_model": fastest_ocr[0],
                "best_text_model": best_text[0]
            }

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    tester = ComprehensiveModelTester()
    
    print("üöÄ –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 40)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    gpu_info = tester.check_gpu_memory()
    if gpu_info:
        print(f"üéÆ GPU –ø–∞–º—è—Ç—å: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
        print(f"üíæ –°–≤–æ–±–æ–¥–Ω–æ: {gpu_info['free_mb']} –ú–ë")
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    results = tester.run_comprehensive_test()
    
    if results:
        print(f"\nüéâ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"üìä –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(results['models'])}")
    else:
        print(f"\n‚ùå –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")

if __name__ == "__main__":
    main()