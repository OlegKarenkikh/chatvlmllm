#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π vLLM —Å –æ—á–∏—Å—Ç–∫–æ–π –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö
"""

import json
import subprocess
import time
import requests
import base64
import os
import shutil
from pathlib import Path
from typing import Dict, List, Any

class AutomatedModelTester:
    def __init__(self, config_file: str = "full_vllm_models_config.json"):
        self.config_file = config_file
        self.configs = self.load_configs()
        self.cache_path = str(os.path.expanduser("~/.cache/huggingface/hub")).replace('\\', '/')
        self.test_images = [
            "test_documents/01_simple_text.png",
            "test_documents/02_table.png", 
            "test_documents/04_numbers.png"
        ]
        self.results = {}
        self.incompatible_models = []
        
    def load_configs(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–µ–π"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {e}")
            return {}
    
    def run_command(self, command):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
        try:
            result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
            return True, result.stdout.strip()
        except subprocess.CalledProcessError as e:
            return False, e.stderr.strip() if e.stderr else str(e)
    
    def check_gpu_memory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU"""
        success, output = self.run_command("nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv,noheader,nounits")
        
        if success:
            lines = output.strip().split('\n')
            for i, line in enumerate(lines):
                parts = line.split(', ')
                if len(parts) == 3:
                    total, used, free = map(int, parts)
                    return {
                        'total_mb': total,
                        'used_mb': used,
                        'free_mb': free,
                        'usage_percent': round((used / total) * 100, 1)
                    }
        return None
    
    def cleanup_containers(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ vLLM"""
        print("üßπ –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤...")
        success, output = self.run_command("docker ps -a --filter ancestor=vllm/vllm-openai:latest --format {{.Names}}")
        
        if success and output:
            container_names = output.strip().split('\n')
            for container_name in container_names:
                if container_name:
                    self.run_command(f"docker stop {container_name}")
                    self.run_command(f"docker rm {container_name}")
                    print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω {container_name}")
    
    def test_model_launch(self, model_name: str, config: Dict, timeout: int = 180) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"\nüß™ –¢–ï–°–¢ –ó–ê–ü–£–°–ö–ê: {model_name}")
        print("-" * 50)
        
        container_name = config['container_name']
        port = config['port']
        vllm_params = config['vllm_params']
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        gpu_info = self.check_gpu_memory()
        if gpu_info:
            print(f"üíæ GPU: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã Docker
        docker_command = f"""
        docker run -d \
            --gpus all \
            --name {container_name} \
            -p {port}:{port} \
            -v {self.cache_path}:/root/.cache/huggingface/hub:ro \
            --shm-size=8g \
            vllm/vllm-openai:latest \
            --model {model_name} \
            --trust-remote-code \
            --max-model-len {vllm_params['max_model_len']} \
            --gpu-memory-utilization {vllm_params['gpu_memory_utilization']} \
            --host 0.0.0.0 \
            --port {port} \
            --disable-log-requests
        """.strip().replace('\n', ' ').replace('\\', '')
        
        if vllm_params.get('enforce_eager'):
            docker_command += " --enforce-eager"
        
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ {container_name}...")
        
        # –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        success, output = self.run_command(docker_command)
        
        if not success:
            return {
                "launch_success": False,
                "error": f"Container start failed: {output}",
                "error_type": "container_start_error"
            }
        
        print(f"üì¶ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω, –æ–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏...")
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ª–æ–≥–æ–≤
        start_time = time.time()
        last_log_check = start_time
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=5)
                if response.status_code == 200:
                    launch_time = time.time() - start_time
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∑–∞ {int(launch_time)} —Å–µ–∫—É–Ω–¥!")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞
                    gpu_info = self.check_gpu_memory()
                    if gpu_info:
                        print(f"üíæ GPU –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
                    
                    return {
                        "launch_success": True,
                        "launch_time": launch_time,
                        "gpu_memory_after": gpu_info
                    }
                    
            except requests.exceptions.ConnectionError:
                pass
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            current_time = time.time()
            if current_time - last_log_check > 30:
                elapsed = int(current_time - start_time)
                print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {elapsed}/{timeout}—Å...")
                
                # –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ—à–∏–±–æ–∫
                success_log, logs = self.run_command(f"docker logs {container_name} --tail 10")
                if success_log and logs:
                    # –ü–æ–∏—Å–∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
                    if "ImportError" in logs:
                        error_lines = [line for line in logs.split('\n') if 'ImportError' in line]
                        return {
                            "launch_success": False,
                            "error": f"Import error: {error_lines[-1] if error_lines else 'Unknown import error'}",
                            "error_type": "import_error",
                            "logs": logs
                        }
                    elif "ModuleNotFoundError" in logs:
                        error_lines = [line for line in logs.split('\n') if 'ModuleNotFoundError' in line]
                        return {
                            "launch_success": False,
                            "error": f"Module not found: {error_lines[-1] if error_lines else 'Unknown module error'}",
                            "error_type": "module_error",
                            "logs": logs
                        }
                    elif "CUDA out of memory" in logs:
                        return {
                            "launch_success": False,
                            "error": "CUDA out of memory",
                            "error_type": "memory_error",
                            "logs": logs
                        }
                
                last_log_check = current_time
            
            time.sleep(10)
        
        # –¢–∞–π–º–∞—É—Ç - –ø–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ª–æ–≥–æ–≤
        success_log, logs = self.run_command(f"docker logs {container_name} --tail 20")
        
        return {
            "launch_success": False,
            "error": f"Timeout after {timeout} seconds",
            "error_type": "timeout",
            "logs": logs if success_log else "No logs available"
        }
    
    def test_model_functionality(self, model_name: str, port: int) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        print(f"üîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...")
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ç–µ—Å—Ç
        try:
            payload = {
                "model": model_name,
                "messages": [{"role": "user", "content": "Hello, how are you?"}],
                "max_tokens": 50,
                "temperature": 0.1
            }
            
            start_time = time.time()
            response = requests.post(
                f"http://localhost:{port}/v1/chat/completions",
                json=payload,
                timeout=30
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                text = result["choices"][0]["message"]["content"]
                
                return {
                    "text_test_success": True,
                    "text_response": text,
                    "text_processing_time": round(processing_time, 2),
                    "usage": result.get("usage", {})
                }
            else:
                return {
                    "text_test_success": False,
                    "text_error": f"HTTP {response.status_code}: {response.text[:200]}"
                }
                
        except Exception as e:
            return {
                "text_test_success": False,
                "text_error": str(e)
            }
    
    def cleanup_model(self, container_name: str):
        """–û—á–∏—Å—Ç–∫–∞ –º–æ–¥–µ–ª–∏"""
        print(f"üßπ –û—á–∏—Å—Ç–∫–∞ {container_name}...")
        self.run_command(f"docker stop {container_name}")
        self.run_command(f"docker rm {container_name}")
    
    def remove_incompatible_model_cache(self, model_name: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ –∫–µ—à–∞ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –º–æ–¥–µ–ª–∏"""
        cache_dir_name = f"models--{model_name.replace('/', '--')}"
        cache_dir_path = Path(self.cache_path) / cache_dir_name
        
        if cache_dir_path.exists():
            try:
                shutil.rmtree(cache_dir_path)
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –∫–µ—à –º–æ–¥–µ–ª–∏: {model_name}")
                return True
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –∫–µ—à–∞ {model_name}: {e}")
                return False
        else:
            print(f"‚ö†Ô∏è –ö–µ—à –º–æ–¥–µ–ª–∏ {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
    
    def run_comprehensive_test(self):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üî¨ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –í–°–ï–• –ú–û–î–ï–õ–ï–ô")
        print("=" * 50)
        
        # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        self.cleanup_containers()
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        sorted_models = sorted(
            self.configs.items(), 
            key=lambda x: (x[1]['priority'], x[1]['size_gb'])
        )
        
        print(f"üìã –ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len(sorted_models)} –º–æ–¥–µ–ª–µ–π")
        
        all_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "models": {},
            "summary": {
                "total_tested": 0,
                "successful": 0,
                "failed": 0,
                "incompatible": 0,
                "removed_from_cache": 0
            }
        }
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for i, (model_name, config) in enumerate(sorted_models, 1):
            print(f"\n{'='*70}")
            print(f"üîÑ –ú–û–î–ï–õ–¨ {i}/{len(sorted_models)}: {model_name}")
            print(f"üìä –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {config['priority']}, –†–∞–∑–º–µ—Ä: {config['size_gb']} –ì–ë")
            print(f"üìã –°—Ç–∞—Ç—É—Å: {config['status']}")
            print(f"{'='*70}")
            
            # –ü—Ä–æ–ø—É—Å–∫ –∑–∞–≤–µ–¥–æ–º–æ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π
            if config['status'] == 'known_incompatible':
                print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∑–∞–≤–µ–¥–æ–º–æ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π –º–æ–¥–µ–ª–∏")
                
                # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —É–¥–∞–ª–∏—Ç—å –∏–∑ –∫–µ—à–∞
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –∫–µ—à–∞...")
                if self.remove_incompatible_model_cache(model_name):
                    all_results["summary"]["removed_from_cache"] += 1
                
                all_results["models"][model_name] = {
                    "status": "skipped_incompatible",
                    "reason": "Known incompatible model"
                }
                all_results["summary"]["incompatible"] += 1
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫ –∑–∞–≤–µ–¥–æ–º–æ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            if config['status'] == 'likely_broken':
                print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ —Å–ª–æ–º–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
                
                # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —É–¥–∞–ª–∏—Ç—å –∏–∑ –∫–µ—à–∞
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –∫–µ—à–∞...")
                if self.remove_incompatible_model_cache(model_name):
                    all_results["summary"]["removed_from_cache"] += 1
                
                all_results["models"][model_name] = {
                    "status": "skipped_broken",
                    "reason": "Likely broken model (zero size)"
                }
                all_results["summary"]["incompatible"] += 1
                continue
            
            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞
            launch_result = self.test_model_launch(model_name, config)
            all_results["summary"]["total_tested"] += 1
            
            model_result = {
                "config": config,
                "launch_result": launch_result,
                "functionality_result": None
            }
            
            if launch_result["launch_success"]:
                print(f"‚úÖ –ó–∞–ø—É—Å–∫ —É—Å–ø–µ—à–µ–Ω!")
                
                # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                functionality_result = self.test_model_functionality(model_name, config['port'])
                model_result["functionality_result"] = functionality_result
                
                if functionality_result.get("text_test_success"):
                    print(f"‚úÖ –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç!")
                    print(f"üìù –û—Ç–≤–µ—Ç: {functionality_result['text_response'][:100]}...")
                    all_results["summary"]["successful"] += 1
                    model_result["status"] = "working"
                else:
                    print(f"‚ùå –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {functionality_result.get('text_error', 'Unknown error')}")
                    all_results["summary"]["failed"] += 1
                    model_result["status"] = "launch_ok_function_fail"
                
                # –û—á–∏—Å—Ç–∫–∞
                self.cleanup_model(config['container_name'])
                
            else:
                print(f"‚ùå –ó–∞–ø—É—Å–∫ –Ω–µ—É–¥–∞—á–µ–Ω: {launch_result['error']}")
                all_results["summary"]["failed"] += 1
                model_result["status"] = "launch_failed"
                
                # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞ –æ—à–∏–±–∫–∏
                error_type = launch_result.get("error_type", "unknown")
                
                if error_type in ["import_error", "module_error"]:
                    print(f"üóëÔ∏è –ú–æ–¥–µ–ª—å –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞, —É–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –∫–µ—à–∞...")
                    if self.remove_incompatible_model_cache(model_name):
                        all_results["summary"]["removed_from_cache"] += 1
                        model_result["cache_removed"] = True
                
                # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                self.cleanup_model(config['container_name'])
            
            all_results["models"][model_name] = model_result
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
            if i < len(sorted_models):
                print(f"\n‚è∏Ô∏è –ü–∞—É–∑–∞ 5 —Å–µ–∫—É–Ω–¥...")
                time.sleep(5)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        self.cleanup_containers()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = f"automated_test_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.create_updated_config(all_results)
        
        # –ü–æ–∫–∞–∑ –∏—Ç–æ–≥–æ–≤
        self.show_final_summary(all_results)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_file}")
        
        return all_results
    
    def create_updated_config(self, results: Dict[str, Any]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        working_models = {}
        
        for model_name, result in results["models"].items():
            if result.get("status") == "working":
                config = result["config"].copy()
                config["status"] = "tested_working"
                config["last_tested"] = results["timestamp"]
                working_models[model_name] = config
        
        if working_models:
            with open('working_vllm_models.json', 'w', encoding='utf-8') as f:
                json.dump(working_models, f, ensure_ascii=False, indent=2)
            
            print(f"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: working_vllm_models.json")
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(working_models)} —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π")
    
    def show_final_summary(self, results: Dict[str, Any]):
        """–ü–æ–∫–∞–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        summary = results["summary"]
        
        print(f"\nüèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
        print("=" * 30)
        print(f"üìä –í—Å–µ–≥–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {summary['total_tested']}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç: {summary['successful']}")
        print(f"‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞—é—Ç: {summary['failed']}")
        print(f"‚è≠Ô∏è –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ: {summary['incompatible']}")
        print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∏–∑ –∫–µ—à–∞: {summary['removed_from_cache']}")
        
        # –°–ø–∏—Å–æ–∫ —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
        working_models = [name for name, result in results["models"].items() 
                         if result.get("status") == "working"]
        
        if working_models:
            print(f"\nüéâ –†–ê–ë–û–¢–ê–Æ–©–ò–ï –ú–û–î–ï–õ–ò:")
            for model_name in working_models:
                config = results["models"][model_name]["config"]
                print(f"   ‚úÖ {model_name} (–ø–æ—Ä—Ç {config['port']}, {config['category']})")
        
        # –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        failed_models = [name for name, result in results["models"].items() 
                        if result.get("status") in ["launch_failed", "launch_ok_function_fail"]]
        
        if failed_models:
            print(f"\n‚ùå –ü–†–û–ë–õ–ï–ú–ù–´–ï –ú–û–î–ï–õ–ò:")
            for model_name in failed_models:
                result = results["models"][model_name]
                error = result["launch_result"].get("error", "Unknown error")
                print(f"   ‚ùå {model_name}: {error[:100]}...")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    tester = AutomatedModelTester()
    
    if not tester.configs:
        print("‚ùå –ù–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        return
    
    print("üöÄ –ó–ê–ü–£–°–ö –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 45)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    gpu_info = tester.check_gpu_memory()
    if gpu_info:
        print(f"üéÆ GPU: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    results = tester.run_comprehensive_test()
    
    print(f"\nüéâ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")

if __name__ == "__main__":
    main()