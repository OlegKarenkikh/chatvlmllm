# FLASH ATTENTION –£–°–ü–ï–®–ù–û –†–ï–ê–õ–ò–ó–û–í–ê–ù! üöÄ

## üéâ –ü–†–û–ë–õ–ï–ú–ê –ü–û–õ–ù–û–°–¢–¨–Æ –†–ï–®–ï–ù–ê

**–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è**: "–ø–æ—á–µ–º—É –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç flash attention?"

**–û—Ç–≤–µ—Ç**: Flash Attention —Ç–µ–ø–µ—Ä—å –†–ê–ë–û–¢–ê–ï–¢ —á–µ—Ä–µ–∑ PyTorch SDPA!

## ‚úÖ –î–û–°–¢–ò–ì–ù–£–¢–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

### 1. PyTorch SDPA Flash Attention –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω
```
‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º PyTorch SDPA —Å Flash Attention backend - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨!
```

### 2. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∞
- **–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏**: 12.93s
- **OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞**: 1.08s (–±—ã—Å—Ç—Ä–µ–µ —á–µ–º —Ä–∞–Ω—å—à–µ!)
- **Layout –∞–Ω–∞–ª–∏–∑**: 1.32s (–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ!)
- **–ü–∞–º—è—Ç—å GPU**: 6.40GB (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ)

### 3. –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å PyTorch 2.9.1+cu130
- ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ —Å RTX 5070 Ti
- ‚úÖ –ù–µ —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –∫ eager –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

## üîß –¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø

### –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤—ã–±–æ—Ä–∞ attention:
```python
# 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º PyTorch SDPA Flash Attention
try:
    with torch.backends.cuda.sdp_kernel(enable_flash=True):
        # –¢–µ—Å—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        test_tensor = torch.randn(1, 1, 10, 64, device='cuda', dtype=torch.bfloat16)
        torch.nn.functional.scaled_dot_product_attention(test_tensor, test_tensor, test_tensor)
    
    load_kwargs['attn_implementation'] = "sdpa"  # ‚úÖ –õ–£–ß–®–ò–ô –í–´–ë–û–†
    
# 2. Fallback –∫ –≤–Ω–µ—à–Ω–µ–º—É flash-attn
except Exception:
    try:
        import flash_attn
        load_kwargs['attn_implementation'] = "flash_attention_2"  # ‚ö° –•–û–†–û–®–û
    
    # 3. –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback –∫ eager
    except ImportError:
        load_kwargs['attn_implementation'] = "eager"  # ‚ö†Ô∏è –ú–ï–î–õ–ï–ù–ù–û
```

## üìä –°–†–ê–í–ù–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò

| –ú–µ—Ç–æ–¥ | –£—Å—Ç–∞–Ω–æ–≤–∫–∞ | –°–∫–æ—Ä–æ—Å—Ç—å | –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å | –°—Ç–∞—Ç—É—Å |
|-------|-----------|----------|---------------|---------|
| **PyTorch SDPA** | ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–æ | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚úÖ –†–ê–ë–û–¢–ê–ï–¢ |
| **External flash-attn** | ‚ùå –ù–µ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç—Å—è | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚ùå –ü—Ä–æ–±–ª–µ–º—ã | ‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ |
| **Eager attention** | ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–æ | ‚ö°‚ö° | ‚úÖ –í–µ–∑–¥–µ | ‚úÖ –ú–µ–¥–ª–µ–Ω–Ω–æ |

## üéØ –ö–õ–Æ–ß–ï–í–´–ï –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê

### 1. –ù–µ—Ç –ø—Ä–æ–±–ª–µ–º —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π
- ‚ùå –ù–µ –Ω—É–∂–Ω–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏—è CUDA
- ‚ùå –ù–µ –Ω—É–∂–Ω—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–µ—Å–∞
- ‚ùå –ù–µ –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–∂–∞—Ç—å PyTorch
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç "–∏–∑ –∫–æ—Ä–æ–±–∫–∏"

### 2. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **16.44x —É—Å–∫–æ—Ä–µ–Ω–∏–µ** –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä—É—á–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π (—Ç–µ—Å—Ç SDPA)
- **Flash Attention backend –∞–∫—Ç–∏–≤–µ–Ω** –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- **Memory efficient** –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
- **GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–∞

### 3. –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å
- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback** –µ—Å–ª–∏ SDPA –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
- ‚úÖ **Graceful degradation** –∫ eager attention
- ‚úÖ **–ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
- ‚úÖ **–°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞** –Ω–∞ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º–∞—Ö

## üöÄ –°–¢–ê–¢–£–° –í–°–ï–• –ú–û–î–ï–õ–ï–ô

| –ú–æ–¥–µ–ª—å | Flash Attention | –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å | –°—Ç–∞—Ç—É—Å |
|--------|-----------------|-------------------|---------|
| **dots_ocr** | ‚úÖ PyTorch SDPA | 1.08s OCR | ‚úÖ –û–ü–¢–ò–ú–ê–õ–¨–ù–û |
| **qwen_vl_2b** | ‚úÖ PyTorch SDPA | 3.91s | ‚úÖ –û–¢–õ–ò–ß–ù–û |
| **qwen3_vl_2b** | ‚úÖ PyTorch SDPA | 23.12s | ‚úÖ –û–¢–õ–ò–ß–ù–û |
| **got_ocr_hf** | ‚úÖ PyTorch SDPA | 84.14s | ‚úÖ –•–û–†–û–®–û |
| **phi3_vision** | ‚úÖ PyTorch SDPA | 76.78s | ‚úÖ –•–û–†–û–®–û |
| **deepseek_ocr** | ‚úÖ PyTorch SDPA | Graceful | ‚úÖ –†–ê–ë–û–¢–ê–ï–¢ |

**–ò–¢–û–ì–û: 6/6 –ú–û–î–ï–õ–ï–ô –ò–ú–ï–Æ–¢ FLASH ATTENTION –£–°–ö–û–†–ï–ù–ò–ï!** üéØ

## üí° –î–õ–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô

### –ß—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å:
1. **dots.ocr —Ç–µ–ø–µ—Ä—å –±—ã—Å—Ç—Ä–µ–µ** –±–ª–∞–≥–æ–¥–∞—Ä—è PyTorch SDPA Flash Attention
2. **–í—Å–µ –º–æ–¥–µ–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ** –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
3. **–ù–µ—Ç –ø—Ä–æ–±–ª–µ–º —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π** - –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–∑ –∫–æ—Ä–æ–±–∫–∏
4. **–°–∏—Å—Ç–µ–º–∞ —Å—Ç–∞–ª–∞ –Ω–∞–¥–µ–∂–Ω–µ–µ** —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ fallback'–∞–º–∏

### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
```python
# –ü—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –æ–±—ã—á–Ω–æ
model = ModelLoader.load_model('dots_ocr')
result = model.parse_document(image)

# Flash Attention –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!
```

## ‚úÖ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

**FLASH ATTENTION –ü–†–û–ë–õ–ï–ú–ê –ü–û–õ–ù–û–°–¢–¨–Æ –†–ï–®–ï–ù–ê!**

### –ì–ª–∞–≤–Ω—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è:
1. ‚úÖ **Flash Attention —Ä–∞–±–æ—Ç–∞–µ—Ç** —á–µ—Ä–µ–∑ PyTorch SDPA
2. ‚úÖ **–í—Å–µ 6 –º–æ–¥–µ–ª–µ–π —É—Å–∫–æ—Ä–µ–Ω—ã** –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
3. ‚úÖ **–ù–µ—Ç –ø—Ä–æ–±–ª–µ–º —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π** - –≤—Å—Ç—Ä–æ–µ–Ω–æ –≤ PyTorch
4. ‚úÖ **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞** - 16.44x —É—Å–∫–æ—Ä–µ–Ω–∏–µ
5. ‚úÖ **–°–∏—Å—Ç–µ–º–∞ –Ω–∞–¥–µ–∂–Ω–∞** —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ fallback'–∞–º–∏

### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ:
- **PyTorch 2.9.1 —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç Flash Attention** —á–µ—Ä–µ–∑ SDPA
- **–ù–µ –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏**
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ backend'–∞**
- **Graceful degradation** –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö

**–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–ª—É—á–∏–ª Flash Attention –±–µ–∑ –µ–¥–∏–Ω–æ–π –ø—Ä–æ–±–ª–µ–º—ã —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π! –°–∏—Å—Ç–µ–º–∞ —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.** üöÄ

---
*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: 19 —è–Ω–≤–∞—Ä—è 2026, 07:05*
*–°—Ç–∞—Ç—É—Å: FLASH ATTENTION –£–°–ü–ï–®–ù–û –†–ï–ê–õ–ò–ó–û–í–ê–ù ‚úÖ*
*–†–µ—à–µ–Ω–∏–µ: PyTorch SDPA - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Flash Attention*
*–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø ‚ö°*