#!/usr/bin/env python3
"""
–¢–æ—á–Ω–∞—è –∫–æ–ø–∏—è –ø—Ä–∏–º–µ—Ä–∞ –∏–∑ Modal Notebooks –¥–ª—è dots.ocr
"""

import os
import sys
import json
import torch
from pathlib import Path
from PIL import Image

# Add project root to path
sys.path.append(str(Path(__file__).parent))

# Set environment variable
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from models.model_loader import ModelLoader
from utils.logger import logger


def inference(image_path_or_pil, prompt: str, model, processor):
    """–¢–æ—á–Ω–∞—è –∫–æ–ø–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ inference –∏–∑ Modal Notebooks."""
    
    # Handle both file path and PIL Image
    if isinstance(image_path_or_pil, str):
        image_path = image_path_or_pil
    else:
        # For PIL Image, we need to save it temporarily or handle differently
        image_path = image_path_or_pil
    
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image_path},
                {"type": "text", "text": prompt}
            ]
        }
    ]
    
    # Preparation for inference
    text = processor.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # Process vision info
    from qwen_vl_utils import process_vision_info
    image_inputs, video_inputs = process_vision_info(messages)
    
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    )
    
    inputs = inputs.to("cuda")
    
    # Inference: Generation of the output
    generated_ids = model.generate(**inputs, max_new_tokens=24000)
    
    generated_ids_trimmed = [
        out_ids[len(in_ids):] 
        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    
    (output_text,) = processor.batch_decode(
        generated_ids_trimmed, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )
    
    return json.loads(output_text)


def test_exact_modal_implementation():
    """–¢–µ—Å—Ç —Ç–æ—á–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Modal Notebooks."""
    
    print("üî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¢–û–ß–ù–û–ô –∫–æ–ø–∏–∏ Modal Notebooks")
    print("=" * 60)
    
    # Load model using our model loader
    model_wrapper = ModelLoader.load_model('dots_ocr')
    
    # Get the actual model and processor
    model = model_wrapper.model
    processor = model_wrapper.processor
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    
    # Import prompts (exact Modal way)
    try:
        from dots_ocr.utils import dict_promptmode_to_prompt
        print("‚úÖ –ü—Ä–æ–º–ø—Ç—ã –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏–∑ dots_ocr.utils")
    except ImportError:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å dots_ocr.utils")
        return
    
    # Test with our complex document
    image_path = "complex_document.png"
    if not Path(image_path).exists():
        print("‚ùå –§–∞–π–ª complex_document.png –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    image = Image.open(image_path)
    print(f"üì∑ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {image.size}")
    
    # Test 1: OCR mode (exact Modal)
    print("\nüî§ –¢–µ—Å—Ç 1: OCR (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ Modal)")
    prompt = dict_promptmode_to_prompt["ocr"]
    print(f"–ü—Ä–æ–º–ø—Ç: {prompt}")
    
    try:
        result1 = inference(image, prompt, model, processor)
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç OCR: {type(result1)}")
        if isinstance(result1, list):
            print(f"   –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(result1)}")
        else:
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result1}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ OCR: {e}")
    
    # Test 2: Layout analysis (exact Modal)
    print("\nüìã –¢–µ—Å—Ç 2: Layout –∞–Ω–∞–ª–∏–∑ (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ Modal)")
    prompt = dict_promptmode_to_prompt["prompt_layout_all_en"]
    print(f"–ü—Ä–æ–º–ø—Ç: {prompt[:100]}...")
    
    try:
        result2 = inference(image, prompt, model, processor)
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç Layout: {type(result2)}")
        
        if isinstance(result2, list):
            print(f"   –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(result2)}")
            
            # Show categories
            categories = {}
            for element in result2:
                cat = element.get('category', 'Unknown')
                categories[cat] = categories.get(cat, 0) + 1
            
            print("   –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for cat, count in categories.items():
                print(f"     {cat}: {count}")
                
            # Show first few elements
            print("   –ü–µ—Ä–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã:")
            for i, element in enumerate(result2[:3]):
                bbox = element.get('bbox', [])
                category = element.get('category', 'Unknown')
                text = element.get('text', '')[:50]
                print(f"     {i+1}. {category} | {bbox} | {text}...")
                
        else:
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result2}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ Layout: {e}")
        import traceback
        print(f"   Traceback: {traceback.format_exc()}")
    
    # Unload model
    ModelLoader.unload_model('dots_ocr')
    print("\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    test_exact_modal_implementation()