#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö CUDA –æ—à–∏–±–æ–∫ –≤ —Å–∏—Å—Ç–µ–º–µ ChatVLMLLM
–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –ø–æ–∫–∞–∑–∞–ª —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
"""

import json
import yaml
from datetime import datetime
import os

def analyze_log_errors():
    """–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏–∑ –ª–æ–≥–∞"""
    
    errors_found = {
        "critical_cuda_errors": [
            {
                "error": "CUDA error: device-side assert triggered",
                "frequency": "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è (–º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏)",
                "severity": "–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø",
                "models_affected": ["qwen_vl_2b", "qwen3_vl_2b", "dots_ocr"],
                "impact": "–ü–æ–ª–Ω—ã–π –æ—Ç–∫–∞–∑ —Å–∏—Å—Ç–µ–º—ã –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                "context": "–ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–µ–π"
            }
        ],
        "flash_attention_errors": [
            {
                "error": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed",
                "frequency": "–í—ã—Å–æ–∫–∞—è",
                "severity": "–í–´–°–û–ö–ê–Ø", 
                "models_affected": ["qwen_vl_2b", "qwen3_vl_2b"],
                "impact": "–ú–æ–¥–µ–ª–∏ –Ω–µ –º–æ–≥—É—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è —Å Flash Attention",
                "solution": "–û—Ç–∫–ª—é—á–∏—Ç—å Flash Attention –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"
            }
        ],
        "quantization_errors": [
            {
                "error": "Qwen3VLForConditionalGeneration.__init__() got an unexpected keyword argument 'load_in_8bit'",
                "frequency": "–°—Ä–µ–¥–Ω—è—è",
                "severity": "–í–´–°–û–ö–ê–Ø",
                "models_affected": ["qwen3_vl_2b", "dots_ocr"],
                "impact": "–ú–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é",
                "solution": "–û—Ç–∫–ª—é—á–∏—Ç—å load_in_8bit –¥–ª—è —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π"
            }
        ],
        "transformers_version_errors": [
            {
                "error": "transformers library with Qwen2-VL support is required. Install with: pip install transformers>=4.37.0",
                "frequency": "–°—Ä–µ–¥–Ω—è—è",
                "severity": "–°–†–ï–î–ù–Ø–Ø",
                "models_affected": ["qwen_vl_2b"],
                "impact": "–ú–æ–¥–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è –∏–∑-–∑–∞ –≤–µ—Ä—Å–∏–∏ transformers",
                "solution": "–û–±–Ω–æ–≤–∏—Ç—å transformers –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback"
            }
        ]
    }
    
    return errors_found

def create_emergency_config():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞–≤–∞—Ä–∏–π–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫"""
    
    emergency_config = {
        "models": {
            "qwen_vl_2b": {
                "name": "Qwen2-VL 2B (Emergency Mode)",
                "model_path": "Qwen/Qwen2-VL-2B-Instruct",
                "precision": "fp16",
                "attn_implementation": "eager",  # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û eager –≤–º–µ—Å—Ç–æ flash_attention
                "use_flash_attention": False,    # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "device_map": "auto",
                "trust_remote_code": True,
                "max_new_tokens": 2048,         # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                "context_length": 4096,
                "load_in_8bit": False,          # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "load_in_4bit": False,          # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "torch_dtype": "float16"
            },
            "qwen3_vl_2b": {
                "name": "Qwen3-VL 2B (Emergency Mode)",
                "model_path": "Qwen/Qwen3-VL-2B-Instruct", 
                "precision": "fp16",
                "attn_implementation": "eager",  # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û eager
                "use_flash_attention": False,    # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "device_map": "auto",
                "trust_remote_code": True,
                "max_new_tokens": 2048,         # –£–º–µ–Ω—å—à–µ–Ω–æ
                "context_length": 4096,
                "load_in_8bit": False,          # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "load_in_4bit": False,          # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "torch_dtype": "float16"
            },
            "dots_ocr": {
                "name": "dots.ocr (Emergency Mode)",
                "model_path": "rednote-hilab/dots.ocr",
                "precision": "fp16",
                "attn_implementation": "eager",  # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û eager
                "use_flash_attention": False,    # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "device_map": "auto",
                "trust_remote_code": True,
                "max_new_tokens": 1024,         # –°–∏–ª—å–Ω–æ —É–º–µ–Ω—å—à–µ–Ω–æ
                "context_length": 2048,
                "load_in_8bit": False,          # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "load_in_4bit": False,          # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "torch_dtype": "float16"
            }
        },
        "performance": {
            "blackwell_optimizations": {
                "enable_tf32": False,           # –û–¢–ö–õ–Æ–ß–ï–ù–û –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                "enable_cudnn_benchmark": False, # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "use_bfloat16": False,          # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "enable_sdpa": False,           # –û–¢–ö–õ–Æ–ß–ï–ù–û
                "force_eager_attention": True   # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û
            },
            "generation_settings": {
                "default_max_tokens": 1024,    # –°–∏–ª—å–Ω–æ —É–º–µ–Ω—å—à–µ–Ω–æ
                "max_context_length": 2048,    # –£–º–µ–Ω—å—à–µ–Ω–æ
                "temperature": 0.7,
                "top_p": 0.9,
                "repetition_penalty": 1.1
            },
            "memory_management": {
                "clear_cache_before_load": True,
                "force_gc_collection": True,
                "max_memory_per_gpu": "8GB"    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
            }
        },
        "emergency_mode": {
            "enabled": True,
            "reason": "Critical CUDA errors detected in logs",
            "timestamp": datetime.now().isoformat(),
            "disabled_features": [
                "flash_attention",
                "8bit_quantization", 
                "4bit_quantization",
                "tf32_optimization",
                "cudnn_benchmark",
                "sdpa_attention"
            ]
        }
    }
    
    return emergency_config

def create_cuda_recovery_script():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è CUDA"""
    
    recovery_script = """#!/usr/bin/env python3
'''
CUDA Recovery Script - –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ GPU —Å–æ—Å—Ç–æ—è–Ω–∏—è
'''

import torch
import gc
import os
import time

def emergency_cuda_recovery():
    '''–≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ CUDA'''
    
    print("üö® –≠–ö–°–¢–†–ï–ù–ù–û–ï –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï CUDA...")
    
    try:
        # 1. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö CUDA –∫–µ—à–µ–π
        if torch.cuda.is_available():
            print("üîÑ –û—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–µ–π...")
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            torch.cuda.ipc_collect()
            
            # –°–±—Ä–æ—Å –≤—Å–µ—Ö CUDA –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
        
        # 2. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
        print("üóëÔ∏è –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞...")
        for _ in range(3):
            gc.collect()
            time.sleep(0.5)
        
        # 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print("üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Ç–ª–∞–¥–æ—á–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        os.environ['TORCH_USE_CUDA_DSA'] = '1'
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è GPU
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            print(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {device_count}")
            
            for i in range(device_count):
                props = torch.cuda.get_device_properties(i)
                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
                memory_total = props.total_memory / 1024**3
                
                print(f"GPU {i}: {props.name}")
                print(f"  –ü–∞–º—è—Ç—å: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ, {memory_total:.2f}GB –≤—Å–µ–≥–æ")
                
                # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä
                try:
                    test_tensor = torch.randn(100, 100, device=f'cuda:{i}')
                    del test_tensor
                    torch.cuda.empty_cache()
                    print(f"  ‚úÖ GPU {i} —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                except Exception as e:
                    print(f"  ‚ùå GPU {i} –æ—à–∏–±–∫–∞: {e}")
        
        print("‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ CUDA –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è CUDA: {e}")
        return False

if __name__ == "__main__":
    emergency_cuda_recovery()
"""
    
    return recovery_script

def create_model_loader_fixes():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    
    fixes = {
        "model_loader_patches": {
            "force_eager_attention": True,
            "disable_flash_attention": True,
            "disable_quantization": True,
            "enable_cuda_recovery": True,
            "max_retries": 3,
            "fallback_to_cpu": True
        },
        "error_handling": {
            "cuda_device_assert": {
                "action": "clear_cache_and_retry",
                "max_retries": 2,
                "fallback": "cpu_mode"
            },
            "flash_attention_error": {
                "action": "disable_flash_attention",
                "fallback_attention": "eager"
            },
            "quantization_error": {
                "action": "disable_quantization",
                "fallback_precision": "fp16"
            },
            "transformers_version_error": {
                "action": "use_compatible_model",
                "fallback_model": "qwen3_vl_2b"
            }
        }
    }
    
    return fixes

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫"""
    
    print("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –û–®–ò–ë–ö–ò –û–ë–ù–ê–†–£–ñ–ï–ù–´ –í –õ–û–ì–ê–•!")
    print("=" * 60)
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    errors = analyze_log_errors()
    
    print("üìä –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö:")
    for category, error_list in errors.items():
        print(f"\nüî¥ {category.upper()}:")
        for error in error_list:
            print(f"  ‚Ä¢ {error['error'][:80]}...")
            print(f"    –ß–∞—Å—Ç–æ—Ç–∞: {error['frequency']}")
            print(f"    –ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å: {error['severity']}")
            print(f"    –ú–æ–¥–µ–ª–∏: {', '.join(error['models_affected'])}")
    
    print("\n" + "=" * 60)
    print("üîß –°–û–ó–î–ê–ù–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≤–∞—Ä–∏–π–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    emergency_config = create_emergency_config()
    with open("config_emergency.yaml", "w", encoding="utf-8") as f:
        yaml.dump(emergency_config, f, default_flow_style=False, allow_unicode=True)
    print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∞–≤–∞—Ä–∏–π–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: config_emergency.yaml")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è CUDA
    recovery_script = create_cuda_recovery_script()
    with open("cuda_emergency_recovery.py", "w", encoding="utf-8") as f:
        f.write(recovery_script)
    print("‚úÖ –°–æ–∑–¥–∞–Ω —Å–∫—Ä–∏–ø—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è CUDA: cuda_emergency_recovery.py")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
    fixes = create_model_loader_fixes()
    with open("model_loader_emergency_fixes.json", "w", encoding="utf-8") as f:
        json.dump(fixes, f, indent=2, ensure_ascii=False)
    print("‚úÖ –°–æ–∑–¥–∞–Ω—ã –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞: model_loader_emergency_fixes.json")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report = {
        "timestamp": datetime.now().isoformat(),
        "status": "–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –û–®–ò–ë–ö–ò –û–ë–ù–ê–†–£–ñ–ï–ù–´",
        "errors_analyzed": errors,
        "fixes_created": [
            "config_emergency.yaml",
            "cuda_emergency_recovery.py", 
            "model_loader_emergency_fixes.json"
        ],
        "immediate_actions_required": [
            "1. –ó–∞–ø—É—Å—Ç–∏—Ç—å cuda_emergency_recovery.py",
            "2. –ó–∞–º–µ–Ω–∏—Ç—å config.yaml –Ω–∞ config_emergency.yaml",
            "3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –≤ –∞–≤–∞—Ä–∏–π–Ω–æ–º —Ä–µ–∂–∏–º–µ",
            "4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ –æ–¥–Ω–æ–π",
            "5. –û–±–Ω–æ–≤–∏—Ç—å –¥—Ä–∞–π–≤–µ—Ä—ã CUDA –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ"
        ],
        "root_causes": [
            "CUDA device-side assert - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ GPU",
            "Flash Attention –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º",
            "8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—è–º–∏",
            "–í–µ—Ä—Å–∏—è transformers –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"
        ]
    }
    
    with open("CRITICAL_ERRORS_ANALYSIS_REPORT.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 60)
    print("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ß–ï–¢ –°–û–ó–î–ê–ù!")
    print("üìÑ –§–∞–π–ª: CRITICAL_ERRORS_ANALYSIS_REPORT.json")
    print("\n‚ö†Ô∏è  –ù–ï–ú–ï–î–õ–ï–ù–ù–´–ï –î–ï–ô–°–¢–í–ò–Ø:")
    print("1. üîß –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python cuda_emergency_recovery.py")
    print("2. üìù –ó–∞–º–µ–Ω–∏—Ç–µ config.yaml –Ω–∞ config_emergency.yaml")
    print("3. üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∏—Å—Ç–µ–º—É")
    print("4. üß™ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ")
    print("\nüí° –°–∏—Å—Ç–µ–º–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏!")
    print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∞–≤–∞—Ä–∏–π–Ω—ã–π —Ä–µ–∂–∏–º!")

if __name__ == "__main__":
    main()