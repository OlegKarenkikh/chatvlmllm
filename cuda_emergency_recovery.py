#!/usr/bin/env python3
'''
CUDA Recovery Script - –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ GPU —Å–æ—Å—Ç–æ—è–Ω–∏—è
'''

import torch
import gc
import os
import time

def emergency_cuda_recovery():
    '''–≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ CUDA'''
    
    print("üö® –≠–ö–°–¢–†–ï–ù–ù–û–ï –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï CUDA...")
    
    try:
        # 1. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö CUDA –∫–µ—à–µ–π
        if torch.cuda.is_available():
            print("üîÑ –û—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–µ–π...")
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            torch.cuda.ipc_collect()
            
            # –°–±—Ä–æ—Å –≤—Å–µ—Ö CUDA –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
        
        # 2. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
        print("üóëÔ∏è –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞...")
        for _ in range(3):
            gc.collect()
            time.sleep(0.5)
        
        # 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print("üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Ç–ª–∞–¥–æ—á–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö...")
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        os.environ['TORCH_USE_CUDA_DSA'] = '1'
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è GPU
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            print(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {device_count}")
            
            for i in range(device_count):
                props = torch.cuda.get_device_properties(i)
                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
                memory_total = props.total_memory / 1024**3
                
                print(f"GPU {i}: {props.name}")
                print(f"  –ü–∞–º—è—Ç—å: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ, {memory_total:.2f}GB –≤—Å–µ–≥–æ")
                
                # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä
                try:
                    test_tensor = torch.randn(100, 100, device=f'cuda:{i}')
                    del test_tensor
                    torch.cuda.empty_cache()
                    print(f"  ‚úÖ GPU {i} —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
                except Exception as e:
                    print(f"  ‚ùå GPU {i} –æ—à–∏–±–∫–∞: {e}")
        
        print("‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ CUDA –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è CUDA: {e}")
        return False

if __name__ == "__main__":
    emergency_cuda_recovery()
