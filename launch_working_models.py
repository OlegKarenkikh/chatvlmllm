#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π –ª–∞—É–Ω—á–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö vLLM –º–æ–¥–µ–ª–µ–π
"""

import json
import subprocess
import time
import requests
import os
from typing import Dict, Any

class WorkingModelsLauncher:
    def __init__(self):
        self.cache_path = str(os.path.expanduser("~/.cache/huggingface/hub")).replace('\\', '/')
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
        try:
            with open('final_working_models.json', 'r', encoding='utf-8') as f:
                self.working_models = json.load(f)
        except FileNotFoundError:
            # Fallback –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            self.working_models = {
                "rednote-hilab/dots.ocr": {
                    "container_name": "dots-ocr-production",
                    "port": 8000,
                    "vllm_params": {
                        "max_model_len": 1024,
                        "gpu_memory_utilization": 0.85,
                        "trust_remote_code": True,
                        "enforce_eager": True
                    }
                },
                "Qwen/Qwen3-VL-2B-Instruct": {
                    "container_name": "qwen3-vl-2b-production",
                    "port": 8010,
                    "vllm_params": {
                        "max_model_len": 2048,
                        "gpu_memory_utilization": 0.7,
                        "trust_remote_code": True,
                        "enforce_eager": False
                    }
                }
            }
    
    def run_command(self, command):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
        try:
            result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
            return True, result.stdout.strip()
        except subprocess.CalledProcessError as e:
            return False, e.stderr.strip() if e.stderr else str(e)
    
    def check_gpu_memory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU"""
        success, output = self.run_command("nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv,noheader,nounits")
        
        if success:
            lines = output.strip().split('\n')
            for i, line in enumerate(lines):
                parts = line.split(', ')
                if len(parts) == 3:
                    total, used, free = map(int, parts)
                    return {
                        'total_mb': total,
                        'used_mb': used,
                        'free_mb': free,
                        'usage_percent': round((used / total) * 100, 1)
                    }
        return None
    
    def cleanup_containers(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ vLLM"""
        print("üßπ –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤...")
        success, output = self.run_command("docker ps -a --filter ancestor=vllm/vllm-openai:latest --format {{.Names}}")
        
        if success and output:
            container_names = output.strip().split('\n')
            for container_name in container_names:
                if container_name:
                    self.run_command(f"docker stop {container_name}")
                    self.run_command(f"docker rm {container_name}")
                    print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω {container_name}")
    
    def launch_model(self, model_name: str, config: Dict[str, Any]) -> bool:
        """–ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print(f"\nüöÄ –ó–ê–ü–£–°–ö: {model_name}")
        print("-" * 50)
        
        container_name = config['container_name']
        port = config['port']
        vllm_params = config['vllm_params']
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        gpu_info = self.check_gpu_memory()
        if gpu_info:
            print(f"üíæ GPU: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã Docker
        docker_command = f"""
        docker run -d \
            --gpus all \
            --name {container_name} \
            -p {port}:{port} \
            -v {self.cache_path}:/root/.cache/huggingface/hub:ro \
            --shm-size=8g \
            vllm/vllm-openai:latest \
            --model {model_name} \
            --trust-remote-code \
            --max-model-len {vllm_params['max_model_len']} \
            --gpu-memory-utilization {vllm_params['gpu_memory_utilization']} \
            --host 0.0.0.0 \
            --port {port} \
            --disable-log-requests
        """.strip().replace('\n', ' ').replace('\\', '')
        
        if vllm_params.get('enforce_eager'):
            docker_command += " --enforce-eager"
        
        print(f"üì¶ –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ {container_name} –Ω–∞ –ø–æ—Ä—Ç—É {port}...")
        
        # –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        success, output = self.run_command(docker_command)
        
        if not success:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {output}")
            return False
        
        print(f"‚úÖ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω: {output[:12]}...")
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (—Å —Ç–∞–π–º–∞—É—Ç–æ–º)
        timeout = 300 if "dots.ocr" in model_name else 360  # 5-6 –º–∏–Ω—É—Ç
        start_time = time.time()
        
        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (—Ç–∞–π–º–∞—É—Ç {timeout}—Å)...")
        
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=5)
                if response.status_code == 200:
                    launch_time = time.time() - start_time
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∑–∞ {int(launch_time)} —Å–µ–∫—É–Ω–¥!")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞
                    gpu_info = self.check_gpu_memory()
                    if gpu_info:
                        print(f"üíæ GPU –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")
                    
                    return True
                    
            except requests.exceptions.ConnectionError:
                pass
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            
            # –ü–æ–∫–∞–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            elapsed = int(time.time() - start_time)
            if elapsed % 30 == 0 and elapsed > 0:
                print(f"   ‚è≥ {elapsed}/{timeout}—Å...")
            
            time.sleep(10)
        
        print(f"‚ùå –¢–∞–π–º–∞—É—Ç {timeout}—Å - –º–æ–¥–µ–ª—å –Ω–µ –≥–æ—Ç–æ–≤–∞")
        return False
    
    def list_models(self):
        """–ü–æ–∫–∞–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üìã –î–û–°–¢–£–ü–ù–´–ï –ú–û–î–ï–õ–ò:")
        print("=" * 30)
        
        for i, (model_name, config) in enumerate(self.working_models.items(), 1):
            port = config['port']
            container = config['container_name']
            print(f"{i}. {model_name}")
            print(f"   –ü–æ—Ä—Ç: {port}")
            print(f"   –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä: {container}")
            print()
    
    def launch_single_model(self, model_choice: str):
        """–ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–π –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_names = list(self.working_models.keys())
        
        if model_choice.isdigit():
            choice = int(model_choice) - 1
            if 0 <= choice < len(model_names):
                model_name = model_names[choice]
            else:
                print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä: {model_choice}")
                return False
        else:
            # –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏
            matching_models = [name for name in model_names if model_choice.lower() in name.lower()]
            if len(matching_models) == 1:
                model_name = matching_models[0]
            elif len(matching_models) > 1:
                print(f"‚ùå –ù–∞–π–¥–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π: {matching_models}")
                return False
            else:
                print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_choice}")
                return False
        
        # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        self.cleanup_containers()
        
        # –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏
        config = self.working_models[model_name]
        success = self.launch_model(model_name, config)
        
        if success:
            port = config['port']
            print(f"\nüéâ –ú–û–î–ï–õ–¨ –ì–û–¢–û–í–ê –ö –†–ê–ë–û–¢–ï!")
            print(f"üåê API: http://localhost:{port}")
            print(f"üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:{port}/docs")
            print(f"‚ù§Ô∏è Health: http://localhost:{port}/health")
            
            # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            print(f"\nüí° –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø:")
            print(f"curl -X POST http://localhost:{port}/v1/chat/completions \\")
            print(f'  -H "Content-Type: application/json" \\')
            print(f'  -d \'{{"model": "{model_name}", "messages": [{{"role": "user", "content": "Hello!"}}]}}\'')
        
        return success
    
    def launch_all_models(self):
        """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üöÄ –ó–ê–ü–£–°–ö –í–°–ï–• –†–ê–ë–û–¢–ê–Æ–©–ò–• –ú–û–î–ï–õ–ï–ô")
        print("=" * 40)
        
        # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        self.cleanup_containers()
        
        success_count = 0
        
        for model_name, config in self.working_models.items():
            success = self.launch_model(model_name, config)
            if success:
                success_count += 1
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—É—Å–∫–∞–º–∏
            if model_name != list(self.working_models.keys())[-1]:
                print(f"\n‚è∏Ô∏è –ü–∞—É–∑–∞ 10 —Å–µ–∫—É–Ω–¥...")
                time.sleep(10)
        
        print(f"\nüèÜ –ò–¢–û–ì: {success_count}/{len(self.working_models)} –º–æ–¥–µ–ª–µ–π –∑–∞–ø—É—â–µ–Ω–æ")
        
        if success_count > 0:
            print(f"\nüåê –î–û–°–¢–£–ü–ù–´–ï API:")
            for model_name, config in self.working_models.items():
                port = config['port']
                print(f"‚Ä¢ {model_name}: http://localhost:{port}")
        
        return success_count > 0
    
    def show_status(self):
        """–ü–æ–∫–∞–∑ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
        print("üìä –°–¢–ê–¢–£–° –ö–û–ù–¢–ï–ô–ù–ï–†–û–í:")
        print("=" * 25)
        
        success, output = self.run_command("docker ps --filter ancestor=vllm/vllm-openai:latest --format 'table {{.Names}}\\t{{.Ports}}\\t{{.Status}}'")
        
        if success and output:
            print(output)
        else:
            print("–ù–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ vLLM")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
        gpu_info = self.check_gpu_memory()
        if gpu_info:
            print(f"\nüíæ GPU: {gpu_info['used_mb']}/{gpu_info['total_mb']} –ú–ë ({gpu_info['usage_percent']}%)")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    launcher = WorkingModelsLauncher()
    
    print("ü§ñ –õ–ê–£–ù–ß–ï–† –†–ê–ë–û–¢–ê–Æ–©–ò–• vLLM –ú–û–î–ï–õ–ï–ô")
    print("=" * 40)
    
    if len(launcher.working_models) == 0:
        print("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–ø—É—Å–∫–∞")
        return
    
    while True:
        print(f"\nüìã –ú–ï–ù–Æ:")
        print("1. –ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        print("2. –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–¥–Ω—É –º–æ–¥–µ–ª—å")
        print("3. –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏")
        print("4. –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å")
        print("5. –û—á–∏—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã")
        print("0. –í—ã—Ö–æ–¥")
        
        choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ: ").strip()
        
        if choice == "1":
            launcher.list_models()
        
        elif choice == "2":
            launcher.list_models()
            model_choice = input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: ").strip()
            if model_choice:
                launcher.launch_single_model(model_choice)
        
        elif choice == "3":
            launcher.launch_all_models()
        
        elif choice == "4":
            launcher.show_status()
        
        elif choice == "5":
            launcher.cleanup_containers()
            print("‚úÖ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –æ—á–∏—â–µ–Ω—ã")
        
        elif choice == "0":
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")

if __name__ == "__main__":
    main()