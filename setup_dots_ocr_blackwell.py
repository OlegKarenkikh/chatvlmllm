#!/usr/bin/env python3
"""
–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –£–°–¢–ê–ù–û–í–ö–ê DOTS.OCR –î–õ–Ø RTX 5070 TI BLACKWELL

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ dots.ocr
"""

import subprocess
import sys
import os
import torch
from pathlib import Path

def check_system_requirements():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è."""
    print("üîç –ü–†–û–í–ï–†–ö–ê –°–ò–°–¢–ï–ú–ù–´–• –¢–†–ï–ë–û–í–ê–ù–ò–ô")
    print("=" * 50)
    
    # GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        compute_cap = torch.cuda.get_device_capability(0)
        
        print(f"‚úÖ GPU: {gpu_name}")
        print(f"‚úÖ VRAM: {gpu_memory:.2f}GB")
        print(f"‚úÖ Compute Capability: {compute_cap}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º RTX 5070 Ti
        if "5070 Ti" in gpu_name:
            print("‚úÖ RTX 5070 Ti –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞")
            if gpu_memory >= 15.0:  # 16GB GDDR7
                print("‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM –¥–ª—è dots.ocr")
            else:
                print("‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16GB+ VRAM –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Blackwell –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
        if compute_cap >= (12, 0):  # sm_120
            print("‚úÖ Blackwell –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        else:
            print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è Blackwell –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (sm_120+)")
            return False
            
    else:
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return False
    
    # PyTorch –≤–µ—Ä—Å–∏—è
    pytorch_version = torch.__version__
    cuda_version = torch.version.cuda
    
    print(f"üì¶ PyTorch: {pytorch_version}")
    print(f"‚ö° CUDA: {cuda_version}")
    
    return True

def install_pytorch_cuda128():
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch —Å CUDA 12.8 –¥–ª—è dots.ocr."""
    print("\nüì¶ –£–°–¢–ê–ù–û–í–ö–ê PYTORCH –° CUDA 12.8")
    print("=" * 50)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –≤–µ—Ä—Å–∏—é
        current_cuda = torch.version.cuda
        if current_cuda == "12.8":
            print("‚úÖ PyTorch —Å CUDA 12.8 —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            return True
        
        print(f"üîÑ –¢–µ–∫—É—â–∞—è CUDA –≤–µ—Ä—Å–∏—è: {current_cuda}")
        print("üîÑ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch 2.7.0 —Å CUDA 12.8...")
        
        # –ö–æ–º–∞–Ω–¥–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ PyTorch 2.7.0 —Å CUDA 12.8
        install_cmd = [
            sys.executable, "-m", "pip", "install", 
            "torch==2.7.0", 
            "torchvision==0.22.0", 
            "torchaudio==2.7.0",
            "--index-url", "https://download.pytorch.org/whl/cu128",
            "--force-reinstall"
        ]
        
        print(f"–í—ã–ø–æ–ª–Ω—è–µ–º: {' '.join(install_cmd)}")
        result = subprocess.run(install_cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ PyTorch 2.7.0 —Å CUDA 12.8 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ PyTorch: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ PyTorch: {e}")
        return False

def install_flash_attention():
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º flash-attn==2.8.0.post2 –¥–ª—è dots.ocr."""
    print("\n‚ö° –£–°–¢–ê–ù–û–í–ö–ê FLASH ATTENTION 2.8.0.post2")
    print("=" * 50)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ flash-attn
        try:
            import flash_attn
            current_version = flash_attn.__version__
            if current_version == "2.8.0.post2":
                print("‚úÖ flash-attn 2.8.0.post2 —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                return True
            else:
                print(f"üîÑ –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è flash-attn: {current_version}")
        except ImportError:
            print("üì¶ flash-attn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        print("üîÑ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º flash-attn==2.8.0.post2...")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ flash-attn —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π
        install_cmd = [
            sys.executable, "-m", "pip", "install", 
            "flash-attn==2.8.0.post2",
            "--no-build-isolation"
        ]
        
        print(f"–í—ã–ø–æ–ª–Ω—è–µ–º: {' '.join(install_cmd)}")
        result = subprocess.run(install_cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ flash-attn 2.8.0.post2 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ flash-attn: {result.stderr}")
            print("üîÑ –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É...")
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ pip –±–µ–∑ –∫–µ—à–∞
            alt_cmd = [
                sys.executable, "-m", "pip", "install", 
                "flash-attn==2.8.0.post2",
                "--no-cache-dir",
                "--no-build-isolation",
                "--force-reinstall"
            ]
            
            result2 = subprocess.run(alt_cmd, capture_output=True, text=True)
            if result2.returncode == 0:
                print("‚úÖ flash-attn —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —á–µ—Ä–µ–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥")
                return True
            else:
                print(f"‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å: {result2.stderr}")
                return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ flash-attn: {e}")
        return False

def clone_dots_ocr_repo():
    """–ö–ª–æ–Ω–∏—Ä—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π dots.ocr."""
    print("\nüìÇ –ö–õ–û–ù–ò–†–û–í–ê–ù–ò–ï –†–ï–ü–û–ó–ò–¢–û–†–ò–Ø DOTS.OCR")
    print("=" * 50)
    
    repo_path = Path("dots.ocr")
    
    if repo_path.exists():
        print("‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π dots.ocr —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return str(repo_path)
    
    try:
        clone_cmd = [
            "git", "clone", 
            "https://github.com/rednote-hilab/dots.ocr.git"
        ]
        
        print(f"–í—ã–ø–æ–ª–Ω—è–µ–º: {' '.join(clone_cmd)}")
        result = subprocess.run(clone_cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π dots.ocr –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            return str(repo_path)
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {result.stderr}")
            return None
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return None

def install_dots_ocr(repo_path):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º dots.ocr –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞."""
    print("\nüöÄ –£–°–¢–ê–ù–û–í–ö–ê DOTS.OCR")
    print("=" * 50)
    
    if not repo_path or not Path(repo_path).exists():
        print("‚ùå –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π dots.ocr –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return False
    
    try:
        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        original_cwd = os.getcwd()
        os.chdir(repo_path)
        
        print(f"üìÅ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ {repo_path}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º dots.ocr –≤ —Ä–µ–∂–∏–º–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
        install_cmd = [sys.executable, "-m", "pip", "install", "-e", "."]
        
        print(f"–í—ã–ø–æ–ª–Ω—è–µ–º: {' '.join(install_cmd)}")
        result = subprocess.run(install_cmd, capture_output=True, text=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –≤ –∏—Å—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        os.chdir(original_cwd)
        
        if result.returncode == 0:
            print("‚úÖ dots.ocr —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ dots.ocr: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ dots.ocr: {e}")
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤–µ—Ä–Ω—É–ª–∏—Å—å –≤ –∏—Å—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        try:
            os.chdir(original_cwd)
        except:
            pass
        return False

def install_additional_dependencies():
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏."""
    print("\nüì¶ –£–°–¢–ê–ù–û–í–ö–ê –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô")
    print("=" * 50)
    
    dependencies = [
        "transformers>=4.50.0",
        "accelerate>=1.2.0",
        "qwen-vl-utils",
        "pillow",
        "numpy",
        "opencv-python",
        "requests"
    ]
    
    try:
        for dep in dependencies:
            print(f"üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º {dep}...")
            install_cmd = [sys.executable, "-m", "pip", "install", dep]
            result = subprocess.run(install_cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"‚úÖ {dep} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            else:
                print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ {dep}: {result.stderr}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {e}")
        return False

def test_dots_ocr_installation():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É dots.ocr."""
    print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –£–°–¢–ê–ù–û–í–ö–ò DOTS.OCR")
    print("=" * 50)
    
    try:
        # –¢–µ—Å—Ç 1: –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π
        print("üîç –¢–µ—Å—Ç 1: –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π...")
        
        import torch
        print(f"‚úÖ PyTorch: {torch.__version__}")
        print(f"‚úÖ CUDA: {torch.version.cuda}")
        print(f"‚úÖ GPU –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
        
        try:
            import flash_attn
            print(f"‚úÖ flash-attn: {flash_attn.__version__}")
        except ImportError:
            print("‚ùå flash-attn –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è")
            return False
        
        # –¢–µ—Å—Ç 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        print("\nüîç –¢–µ—Å—Ç 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ dots.ocr...")
        
        from transformers import AutoModelForCausalLM, AutoProcessor
        
        model_path = "rednote-hilab/dots.ocr"
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        processor = AutoProcessor.from_pretrained(
            model_path, 
            trust_remote_code=True
        )
        print("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (—Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞, –Ω–µ –ø–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="cpu",  # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞ CPU –¥–ª—è —Ç–µ—Å—Ç–∞
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ (CPU —Ç–µ—Å—Ç)")
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            del model
            del processor
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        
        print("\n‚úÖ –ë–∞–∑–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–π–¥–µ–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_dots_ocr_config():
    """–°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è dots.ocr –Ω–∞ RTX 5070 Ti."""
    print("\n‚öôÔ∏è –°–û–ó–î–ê–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò DOTS.OCR")
    print("=" * 50)
    
    config_content = """# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø DOTS.OCR –î–õ–Ø RTX 5070 TI BLACKWELL

models:
  dots_ocr_blackwell:
    name: "dots.ocr (RTX 5070 Ti Optimized)"
    model_path: "rednote-hilab/dots.ocr"
    precision: "bf16"  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell Tensor Cores
    attn_implementation: "flash_attention_2"  # –¢–µ–ø–µ—Ä—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π
    use_flash_attention: true  # –í–∫–ª—é—á–∞–µ–º flash attention
    device_map: "auto"
    trust_remote_code: true
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è RTX 5070 Ti
    gpu_memory_utilization: 0.9  # 90% –æ—Ç 16GB VRAM
    tensor_parallel_size: 1
    max_model_len: 4096
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Blackwell
    enable_tf32: true
    enable_cudnn_benchmark: true
    use_bfloat16: true

performance:
  blackwell_optimizations:
    enable_tf32: true
    enable_cudnn_benchmark: true
    use_bfloat16: true
    enable_flash_attention: true  # –¢–µ–ø–µ—Ä—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    gpu_memory_utilization: 0.9
    
gpu_requirements:
  rtx_5070_ti:
    compute_capability: "sm_120"
    cuda_version: "12.8"
    pytorch_version: "2.7.0"
    flash_attention_version: "2.8.0.post2"
    recommended_precision: "bf16"
    tensor_cores: "5th_gen"
    vram_gb: 16

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è vLLM (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Å–ø–æ—Å–æ–± –∑–∞–ø—É—Å–∫–∞)
vllm:
  gpu_memory_utilization: 0.95
  tensor_parallel_size: 1
  max_model_len: 4096
  trust_remote_code: true
  async_scheduling: true
"""
    
    try:
        with open("config_dots_ocr_blackwell.yaml", "w", encoding="utf-8") as f:
            f.write(config_content)
        
        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ config_dots_ocr_blackwell.yaml")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        return False

def create_vllm_launch_script():
    """–°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ —á–µ—Ä–µ–∑ vLLM."""
    print("\nüöÄ –°–û–ó–î–ê–ù–ò–ï –°–ö–†–ò–ü–¢–ê –ó–ê–ü–£–°–ö–ê VLLM")
    print("=" * 50)
    
    script_content = """#!/usr/bin/env python3
\"\"\"
–ó–ê–ü–£–°–ö DOTS.OCR –ß–ï–†–ï–ó VLLM –î–õ–Ø RTX 5070 TI

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—É—Å–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Blackwell –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
\"\"\"

import subprocess
import sys
import os

def launch_dots_ocr_vllm():
    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ–º dots.ocr —á–µ—Ä–µ–∑ vLLM —Å–µ—Ä–≤–µ—Ä.\"\"\"
    print("üöÄ –ó–ê–ü–£–°–ö DOTS.OCR –ß–ï–†–ï–ó VLLM")
    print("=" * 50)
    
    # –ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞ vLLM —Å–µ—Ä–≤–µ—Ä–∞
    vllm_cmd = [
        "vllm", "serve", "rednote-hilab/dots.ocr",
        "--trust-remote-code",
        "--async-scheduling",
        "--gpu-memory-utilization", "0.95",
        "--tensor-parallel-size", "1",
        "--max-model-len", "4096",
        "--host", "0.0.0.0",
        "--port", "8000"
    ]
    
    print(f"–ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞: {' '.join(vllm_cmd)}")
    print("üåê –°–µ—Ä–≤–µ—Ä –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:8000")
    print("üìã –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")
    print()
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º vLLM —Å–µ—Ä–≤–µ—Ä
        subprocess.run(vllm_cmd)
        
    except KeyboardInterrupt:
        print("\\n‚èπÔ∏è –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ vLLM: {e}")

def launch_dots_ocr_docker():
    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ–º dots.ocr —á–µ—Ä–µ–∑ Docker.\"\"\"
    print("üê≥ –ó–ê–ü–£–°–ö DOTS.OCR –ß–ï–†–ï–ó DOCKER")
    print("=" * 50)
    
    # –ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
    docker_cmd = [
        "docker", "run", "--gpus", "all",
        "-e", "VLLM_GPU_MEMORY_UTILIZATION=0.9",
        "-e", "VLLM_TENSOR_PARALLEL_SIZE=1", 
        "-e", "VLLM_MAX_MODEL_LEN=4096",
        "-p", "8000:8000",
        "rednotehilab/dots.ocr:vllm-openai-v0.9.1"
    ]
    
    print(f"–ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞: {' '.join(docker_cmd)}")
    print("üåê –°–µ—Ä–≤–µ—Ä –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ http://localhost:8000")
    print("üìã –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")
    print()
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
        subprocess.run(docker_cmd)
        
    except KeyboardInterrupt:
        print("\\n‚èπÔ∏è Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Docker: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "docker":
        launch_dots_ocr_docker()
    else:
        launch_dots_ocr_vllm()
"""
    
    try:
        with open("launch_dots_ocr.py", "w", encoding="utf-8") as f:
            f.write(script_content)
        
        print("‚úÖ –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ launch_dots_ocr.py")
        print("üìã –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("   python launch_dots_ocr.py        # –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ vLLM")
        print("   python launch_dots_ocr.py docker # –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Docker")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞: {e}")
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ dots.ocr –¥–ª—è RTX 5070 Ti."""
    print("üöÄ –£–°–¢–ê–ù–û–í–ö–ê DOTS.OCR –î–õ–Ø RTX 5070 TI BLACKWELL")
    print("=" * 80)
    print("–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ dots.ocr")
    print("=" * 80)
    
    success_steps = 0
    total_steps = 8
    
    # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    if check_system_requirements():
        success_steps += 1
        print("‚úÖ –®–∞–≥ 1/8: –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã")
    else:
        print("‚ùå –®–∞–≥ 1/8: –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã")
        return False
    
    # –®–∞–≥ 2: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å CUDA 12.8
    if install_pytorch_cuda128():
        success_steps += 1
        print("‚úÖ –®–∞–≥ 2/8: PyTorch —Å CUDA 12.8 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    else:
        print("‚ùå –®–∞–≥ 2/8: –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ PyTorch")
        print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π PyTorch...")
    
    # –®–∞–≥ 3: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ flash-attn
    if install_flash_attention():
        success_steps += 1
        print("‚úÖ –®–∞–≥ 3/8: flash-attn 2.8.0.post2 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    else:
        print("‚ùå –®–∞–≥ 3/8: –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ flash-attn")
        print("‚ö†Ô∏è dots.ocr –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ flash-attn")
    
    # –®–∞–≥ 4: –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
    repo_path = clone_dots_ocr_repo()
    if repo_path:
        success_steps += 1
        print("‚úÖ –®–∞–≥ 4/8: –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π dots.ocr –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω")
    else:
        print("‚ùå –®–∞–≥ 4/8: –û—à–∏–±–∫–∞ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è")
    
    # –®–∞–≥ 5: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ dots.ocr
    if repo_path and install_dots_ocr(repo_path):
        success_steps += 1
        print("‚úÖ –®–∞–≥ 5/8: dots.ocr —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    else:
        print("‚ùå –®–∞–≥ 5/8: –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ dots.ocr")
    
    # –®–∞–≥ 6: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    if install_additional_dependencies():
        success_steps += 1
        print("‚úÖ –®–∞–≥ 6/8: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    else:
        print("‚ùå –®–∞–≥ 6/8: –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
    
    # –®–∞–≥ 7: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if create_dots_ocr_config():
        success_steps += 1
        print("‚úÖ –®–∞–≥ 7/8: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")
    else:
        print("‚ùå –®–∞–≥ 7/8: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    
    # –®–∞–≥ 8: –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –∑–∞–ø—É—Å–∫–∞
    if create_vllm_launch_script():
        success_steps += 1
        print("‚úÖ –®–∞–≥ 8/8: –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞ —Å–æ–∑–¥–∞–Ω")
    else:
        print("‚ùå –®–∞–≥ 8/8: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞")
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\n" + "=" * 80)
    print("üß™ –§–ò–ù–ê–õ–¨–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï")
    print("=" * 80)
    
    if test_dots_ocr_installation():
        print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–π–¥–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        test_success = True
    else:
        print("‚ùå –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–æ")
        test_success = False
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 80)
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –£–°–¢–ê–ù–û–í–ö–ò")
    print("=" * 80)
    
    success_rate = (success_steps / total_steps) * 100
    print(f"üìà –£—Å–ø–µ—à–Ω–æ—Å—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {success_steps}/{total_steps} ({success_rate:.1f}%)")
    
    if success_steps >= 6 and test_success:
        print("üéâ –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print("\nüí° –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print("1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ dots.ocr —á–µ—Ä–µ–∑ vLLM:")
        print("   python launch_dots_ocr.py")
        print("2. –ò–ª–∏ —á–µ—Ä–µ–∑ Docker:")
        print("   python launch_dots_ocr.py docker")
        print("3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é config_dots_ocr_blackwell.yaml")
        return True
    else:
        print("‚ö†Ô∏è –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –° –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø–ú–ò")
        print("\nüîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É CUDA 12.8")
        print("2. –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ flash-attn")
        print("3. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Docker –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É")
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)