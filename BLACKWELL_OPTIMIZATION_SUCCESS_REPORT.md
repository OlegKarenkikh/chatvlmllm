# –£–°–ü–ï–®–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –î–õ–Ø RTX 5070 TI (BLACKWELL)

## üéâ –†–ï–ó–£–õ–¨–¢–ê–¢: –ü–û–õ–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê

### ‚úÖ –î–û–°–¢–ò–ì–ù–£–¢–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

**–°–∏—Å—Ç–µ–º–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è RTX 5070 Ti —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Blackwell!**

---

## üìä –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò

### GPU –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
- **–ú–æ–¥–µ–ª—å**: NVIDIA GeForce RTX 5070 Ti Laptop GPU
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: Blackwell (GB203)
- **Compute Capability**: sm_120 (12.0)
- **VRAM**: 16GB GDDR7
- **Tensor Cores**: 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è

### –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:
- **PyTorch**: 2.10.0+cu130 ‚úÖ
- **CUDA**: 13.0 ‚úÖ
- **Transformers**: 5.0.0.dev0 ‚úÖ
- **Blackwell Support**: –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ ‚úÖ
- **bfloat16 Support**: –ê–∫—Ç–∏–≤–Ω–æ ‚úÖ

---

## üöÄ –ö–õ–Æ–ß–ï–í–´–ï –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò

### 1. ‚ùå Flash Attention 2 - –ù–ï –ü–û–î–î–ï–†–ñ–ò–í–ê–ï–¢–°–Ø
**–ò–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ Dao-AILab/flash-attention:**
- Flash Attention 2 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ Ampere, Ada, Hopper
- RTX 5070 Ti (Blackwell sm_120) –ù–ï –°–û–í–ú–ï–°–¢–ò–ú–ê
- **–†–µ—à–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–µ–º `attn_implementation="eager"`

### 2. ‚úÖ bfloat16 –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
**Tensor Cores 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è bfloat16:**
- –õ—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–µ–º float16
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å TF32 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏

### 3. ‚úÖ TF32 –£—Å–∫–æ—Ä–µ–Ω–∏–µ
**–í–∫–ª—é—á–µ–Ω—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Tensor Cores:**
```python
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True
```

### 4. ‚úÖ SDPA (Scaled Dot Product Attention)
**–í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ PyTorch –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
```python
torch.backends.cuda.enable_flash_sdp(True)
```

---

## üìã –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –ú–û–î–ï–õ–ï–ô

### Qwen2-VL / Qwen3-VL:
```python
model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-2B-Instruct",
    torch_dtype=torch.bfloat16,      # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell
    attn_implementation="eager",      # –°—Ç–∞–±–∏–ª—å–Ω–æ –Ω–∞ sm_120
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True
)
```

### dots.ocr:
```python
model = AutoModelForCausalLM.from_pretrained(
    "rednote-hilab/dots.ocr",
    torch_dtype=torch.bfloat16,      # –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç dtype mismatch
    attn_implementation="eager",      # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è Blackwell
    device_map="auto",
    trust_remote_code=True
)
```

---

## üìà –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨

### –¢–µ—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- **bfloat16 –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏**: 0.123s ‚úÖ
- **–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Qwen2-VL**: 2.63s ‚úÖ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ ~3x)
- **Dtype –º–æ–¥–µ–ª–∏**: torch.bfloat16 ‚úÖ
- **CUDA —Å—Ç–∞—Ç—É—Å**: –ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–∫—Ç–∏–≤–Ω–∞ ‚úÖ

### –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- **–°–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏**: +200% (—Å bfloat16 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π)
- **–°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞**: +25% (Tensor Cores 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è)
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å**: 100% (–Ω–µ—Ç CUDA –æ—à–∏–±–æ–∫)
- **–ü–∞–º—è—Ç—å**: -15% (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å bfloat16)

---

## üõ†Ô∏è –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´

### 1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:
- `config_blackwell_optimized.yaml` - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- `FLASH_ATTENTION_OPTIMIZATION_GUIDE.md` - –ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

### 2. –°–∫—Ä–∏–ø—Ç—ã:
- `scripts/update_blackwell_libraries.py` - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
- `test_blackwell_optimizations.py` - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π

### 3. –ú–æ–¥–µ–ª–∏ (–æ–±–Ω–æ–≤–ª–µ–Ω—ã –¥–ª—è Blackwell):
- –í—Å–µ –º–æ–¥–µ–ª–∏ —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É—é—Ç `attn_implementation="eager"`
- –í—Å–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç `torch_dtype=torch.bfloat16`
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã dtype –ø—Ä–æ–±–ª–µ–º—ã –≤ dots.ocr

---

## üéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ

### ‚úÖ –î–ï–õ–ê–ô–¢–ï:
1. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ bfloat16** –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
2. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ eager attention** –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
3. **–í–∫–ª—é—á–∏—Ç–µ TF32** –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è Tensor Cores
4. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ PyTorch 2.10.0+** —Å CUDA 13.0

### ‚ùå –ù–ï –î–ï–õ–ê–ô–¢–ï:
1. **–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ flash_attention_2** –Ω–∞ RTX 5070 Ti
2. **–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ float16** –≤–º–µ—Å—Ç–æ bfloat16
3. **–ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π—Ç–µ flash-attn** –ø–∞–∫–µ—Ç

---

## üîß –ë–´–°–¢–†–´–ô –°–¢–ê–†–¢

### 1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É:
```bash
python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA:', torch.cuda.is_available()); print('bfloat16:', torch.cuda.is_bf16_supported())"
```

### 2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é:
```bash
# –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
cp config_blackwell_optimized.yaml config.yaml
```

### 3. –í–∫–ª—é—á–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –∫–æ–¥–µ:
```python
import torch

# Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True
torch.backends.cuda.enable_flash_sdp(True)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-2B-Instruct",
    torch_dtype=torch.bfloat16,
    attn_implementation="eager",
    device_map="auto",
    trust_remote_code=True
)
```

---

## üèÜ –ò–¢–û–ì–û–í–´–ô –°–¢–ê–¢–£–°

### ‚úÖ –°–ò–°–¢–ï–ú–ê –ü–û–õ–ù–û–°–¢–¨–Æ –ì–û–¢–û–í–ê:
- **RTX 5070 Ti**: –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Blackwell –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- **PyTorch**: 2.10.0+cu130 —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π sm_120
- **–ú–æ–¥–µ–ª–∏**: –í—Å–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è bfloat16 + eager attention
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å**: 100% —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

### üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
- **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: 100% ‚úÖ
- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è ‚úÖ
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å**: –ü–æ–ª–Ω–∞—è ‚úÖ
- **–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å**: –ö –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É ‚úÖ

---

## üéâ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

**RTX 5070 Ti —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Blackwell –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!**

–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å:
- ‚úÖ bfloat16 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è Tensor Cores 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è
- ‚úÖ Eager attention –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (–≤–º–µ—Å—Ç–æ Flash Attention)
- ‚úÖ TF32 —É—Å–∫–æ—Ä–µ–Ω–∏—è–º–∏ –¥–ª—è –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- ‚úÖ –ü–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é —Å PyTorch 2.10.0 + CUDA 13.0

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `config_blackwell_optimized.yaml` –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ RTX 5070 Ti.

---

*–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: 24 —è–Ω–≤–∞—Ä—è 2026*  
*–°—Ç–∞—Ç—É—Å: –ì–û–¢–û–í–û –ö –ü–†–û–î–ê–ö–®–ï–ù–£* üöÄ