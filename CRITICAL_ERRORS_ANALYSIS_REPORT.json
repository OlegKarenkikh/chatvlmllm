{
  "timestamp": "2026-01-24T22:53:33.045352",
  "status": "КРИТИЧЕСКИЕ ОШИБКИ ОБНАРУЖЕНЫ",
  "errors_analyzed": {
    "critical_cuda_errors": [
      {
        "error": "CUDA error: device-side assert triggered",
        "frequency": "Очень высокая (множественные записи)",
        "severity": "КРИТИЧЕСКАЯ",
        "models_affected": [
          "qwen_vl_2b",
          "qwen3_vl_2b",
          "dots_ocr"
        ],
        "impact": "Полный отказ системы при обработке изображений",
        "context": "Происходит при попытке инференса моделей"
      }
    ],
    "flash_attention_errors": [
      {
        "error": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed",
        "frequency": "Высокая",
        "severity": "ВЫСОКАЯ",
        "models_affected": [
          "qwen_vl_2b",
          "qwen3_vl_2b"
        ],
        "impact": "Модели не могут загрузиться с Flash Attention",
        "solution": "Отключить Flash Attention в конфигурации"
      }
    ],
    "quantization_errors": [
      {
        "error": "Qwen3VLForConditionalGeneration.__init__() got an unexpected keyword argument 'load_in_8bit'",
        "frequency": "Средняя",
        "severity": "ВЫСОКАЯ",
        "models_affected": [
          "qwen3_vl_2b",
          "dots_ocr"
        ],
        "impact": "Модели не поддерживают 8-bit квантизацию",
        "solution": "Отключить load_in_8bit для этих моделей"
      }
    ],
    "transformers_version_errors": [
      {
        "error": "transformers library with Qwen2-VL support is required. Install with: pip install transformers>=4.37.0",
        "frequency": "Средняя",
        "severity": "СРЕДНЯЯ",
        "models_affected": [
          "qwen_vl_2b"
        ],
        "impact": "Модель не может загрузиться из-за версии transformers",
        "solution": "Обновить transformers или использовать fallback"
      }
    ]
  },
  "fixes_created": [
    "config_emergency.yaml",
    "cuda_emergency_recovery.py",
    "model_loader_emergency_fixes.json"
  ],
  "immediate_actions_required": [
    "1. Запустить cuda_emergency_recovery.py",
    "2. Заменить config.yaml на config_emergency.yaml",
    "3. Перезапустить систему в аварийном режиме",
    "4. Протестировать модели по одной",
    "5. Обновить драйверы CUDA если необходимо"
  ],
  "root_causes": [
    "CUDA device-side assert - критическая ошибка GPU",
    "Flash Attention не установлен или несовместим",
    "8-bit квантизация не поддерживается моделями",
    "Версия transformers несовместима с некоторыми моделями"
  ]
}