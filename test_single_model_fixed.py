#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
"""

import time
import torch
from PIL import Image, ImageDraw, ImageFont
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def create_simple_document():
    """–°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –¥–æ–∫—É–º–µ–Ω—Ç"""
    img = Image.new('RGB', (400, 200), color='white')
    draw = ImageDraw.Draw(img)
    
    try:
        font = ImageFont.truetype("arial.ttf", 14)
    except:
        font = ImageFont.load_default()
    
    draw.text((20, 20), "–¢–ï–°–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô", fill='black', font=font)
    draw.text((20, 50), "–ú–æ–¥–µ–ª—å: qwen3_vl_2b", fill='black', font=font)
    draw.text((20, 80), "Precision: fp16", fill='black', font=font)
    draw.text((20, 110), "Flash Attention: OFF", fill='black', font=font)
    draw.text((20, 140), "–°—Ç–∞—Ç—É—Å: –ò–°–ü–†–ê–í–õ–ï–ù–û", fill='black', font=font)
    
    return img

def test_single_model():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å"""
    print("üîß –¢–ï–°–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô –ú–û–î–ï–õ–ò")
    print("=" * 40)
    
    # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gpu_name = torch.cuda.get_device_name(0)
        vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        vram_allocated = torch.cuda.memory_allocated(0) / 1024**3
        print(f"‚úÖ GPU: {gpu_name}")
        print(f"‚úÖ VRAM: {vram_allocated:.2f}GB / {vram_total:.2f}GB")
    
    try:
        from models.model_loader import ModelLoader
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º qwen3_vl_2b (–æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–ª–∞ –≤ API)
        model_name = "qwen3_vl_2b"
        
        print(f"\nüì• –ó–∞–≥—Ä—É–∂–∞–µ–º {model_name}...")
        start_load = time.time()
        
        model = ModelLoader.load_model(model_name)
        load_time = time.time() - start_load
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º VRAM
        if torch.cuda.is_available():
            vram_used = torch.cuda.memory_allocated(0) / 1024**3
            print(f"üíæ VRAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {vram_used:.2f}GB")
        
        # –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = create_simple_document()
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç OCR
        print(f"\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º OCR...")
        start_ocr = time.time()
        
        result = model.process_image(image, "–ò–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        ocr_time = time.time() - start_ocr
        
        print(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {ocr_time:.2f}s")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        keywords = ["–¢–ï–°–¢", "–ò–°–ü–†–ê–í–õ–ï–ù–ò–ô", "qwen3_vl_2b", "fp16", "–ò–°–ü–†–ê–í–õ–ï–ù–û"]
        found = sum(1 for kw in keywords if kw.upper() in result.upper())
        quality = (found / len(keywords)) * 100
        
        print(f"üéØ –ö–∞—á–µ—Å—Ç–≤–æ: {found}/{len(keywords)} ({quality:.0f}%)")
        
        # –ù–ï –≤—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print(f"üîÑ –ú–æ–¥–µ–ª—å –æ—Å—Ç–∞–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏")
        
        if quality >= 60:
            print(f"üéâ –¢–ï–°–¢ –ü–†–û–ô–î–ï–ù!")
            return True
        else:
            print(f"‚ö†Ô∏è –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ OCR")
            return False
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_single_model()
    exit(0 if success else 1)