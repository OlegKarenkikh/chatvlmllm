# –†–£–ö–û–í–û–î–°–¢–í–û –ü–û –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò FLASH ATTENTION –î–õ–Ø RTX 5070 TI

## üîç –ê–ù–ê–õ–ò–ó –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò –ù–ê –û–°–ù–û–í–ï –û–§–ò–¶–ò–ê–õ–¨–ù–û–ô –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò

### RTX 5070 Ti –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
- **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: Blackwell (GB203)
- **Compute Capability**: sm_120
- **CUDA**: 13.0 (—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º CUDA 12.8)
- **VRAM**: 16GB GDDR7
- **Tensor Cores**: 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è

### Flash Attention –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:

#### ‚ùå –ü–†–û–ë–õ–ï–ú–ê: Flash Attention 2 –ù–ï –ü–û–î–î–ï–†–ñ–ò–í–ê–ï–¢ Blackwell
**–ò–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ Dao-AILab/flash-attention:**
- FlashAttention-2 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ: **Ampere, Ada, Hopper GPUs**
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: A100, RTX 3090, RTX 4090, H100
- **RTX 5070 Ti (Blackwell sm_120) –ù–ï –ü–û–î–î–ï–†–ñ–ò–í–ê–ï–¢–°–Ø**

#### ‚úÖ –†–ï–®–ï–ù–ò–ï: Flash Attention 4 –¥–ª—è Blackwell
**–ò–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ NVIDIA:**
- FlashAttention-4 —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è Blackwell
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç sm_120 compute capability
- –£—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–æ 3.6x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å FA2
- –î–æ—Å—Ç–∏–≥–∞–µ—Ç 1,605 TFLOPS –Ω–∞ Blackwell GPUs

---

## üõ†Ô∏è –û–ü–¢–ò–ú–ê–õ–¨–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ë–ò–ë–õ–ò–û–¢–ï–ö

### 1. PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Blackwell

**–¢—Ä–µ–±—É–µ—Ç—Å—è PyTorch 2.7.0+ —Å CUDA 12.8+:**

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π sm_120
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Blackwell
python -c "import torch; print(torch.__version__); print(torch.cuda.get_arch_list()); print('sm_120' in torch.cuda.get_arch_list())"
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**
```
2.7.0+cu128
['sm_50', 'sm_60', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_100', 'sm_120', 'compute_120']
True
```

### 2. Transformers —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Flash Attention

```bash
# –ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è transformers
pip install transformers>=4.50.0

# Accelerate –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
pip install accelerate>=1.2.0
```

### 3. Flash Attention (–ù–ï –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø –¥–ª—è RTX 5070 Ti)

**‚ö†Ô∏è –í–ê–ñ–ù–û: Flash Attention 2 –ù–ï –†–ê–ë–û–¢–ê–ï–¢ —Å Blackwell**

–ï—Å–ª–∏ –≤—Å–µ –∂–µ –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å (–¥–ª—è –¥—Ä—É–≥–∏—Ö GPU):
```bash
# –¢–æ–ª—å–∫–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö GPU (–ù–ï –¥–ª—è RTX 5070 Ti)
pip install flash-attn --no-build-isolation
```

### 4. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã Flash Attention –¥–ª—è Blackwell

#### A. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Eager Attention (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø)
```python
# –í –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
attn_implementation = "eager"  # –°—Ç–∞–±–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ Blackwell
```

#### B. SDPA (Scaled Dot Product Attention)
```python
# PyTorch –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
attn_implementation = "sdpa"  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
```

---

## üìã –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –î–õ–Ø –ö–ê–ñ–î–û–ô –ú–û–î–ï–õ–ò

### 1. Qwen2-VL / Qwen3-VL –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

**–ò–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è QwenLM/Qwen3-VL:**

```python
from transformers import AutoModelForImageTextToText, AutoProcessor
import torch

# –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è RTX 5070 Ti
model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-2B-Instruct",
    torch_dtype=torch.bfloat16,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell
    attn_implementation="eager",  # –°—Ç–∞–±–∏–ª—å–Ω–æ –Ω–∞ sm_120
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True
)

# –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ flash_attention_2 –Ω–∞ RTX 5070 Ti:
# attn_implementation="flash_attention_2"  # ‚ùå –ù–ï –†–ê–ë–û–¢–ê–ï–¢ –Ω–∞ Blackwell
```

### 2. dots.ocr –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

```python
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Blackwell
load_kwargs = {
    'torch_dtype': torch.bfloat16,  # –õ—É—á—à–µ –¥–ª—è Blackwell Tensor Cores
    'attn_implementation': "eager",  # –°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
    'device_map': "auto",
    'trust_remote_code': True,
    'low_cpu_mem_usage': True
}
```

### 3. –û–±—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

```python
# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Blackwell –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ bfloat16 –¥–ª—è Tensor Cores 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è
torch.backends.cuda.enable_flash_sdp(True)  # –í–∫–ª—é—á–∏—Ç—å SDPA
```

---

## üöÄ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–ê–Ø –£–°–¢–ê–ù–û–í–ö–ê

### –ü–æ–ª–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è RTX 5070 Ti:

```bash
# 1. PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA 12.8 –∏ sm_120
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128

# 2. Transformers –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install transformers>=4.50.0
pip install accelerate>=1.2.0
pip install qwen-vl-utils

# 3. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
pip install optimum
pip install bitsandbytes  # –î–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏

# 4. –ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π—Ç–µ flash-attn –¥–ª—è RTX 5070 Ti
# pip install flash-attn  # ‚ùå –ù–ï –°–û–í–ú–ï–°–¢–ò–ú–û —Å Blackwell
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏:

```python
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.version.cuda}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Compute Capability: {torch.cuda.get_device_capability(0)}")
print(f"Blackwell Support: {'sm_120' in torch.cuda.get_arch_list()}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Tensor Cores
print(f"bfloat16 Support: {torch.cuda.is_bf16_supported()}")
```

---

## ‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò

### 1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ bfloat16 –≤–º–µ—Å—Ç–æ float16

```python
# –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell Tensor Cores 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è
model = model.to(torch.bfloat16)
```

### 2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

```python
generation_config = {
    "max_new_tokens": 1024,
    "do_sample": False,
    "temperature": 0.1,
    "use_cache": True,
    "pad_token_id": tokenizer.eos_token_id
}
```

### 3. –ü–∞–º—è—Ç—å –∏ –±–∞—Ç—á–∏–Ω–≥

```python
# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è 16GB VRAM
batch_size = 4  # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π 1024x1024
# batch_size = 2  # –î–ª—è –±–æ–ª—å—à–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–ª–∏ –≤–∏–¥–µ–æ
```

---

## üîß –û–ë–ù–û–í–õ–ï–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –°–ò–°–¢–ï–ú–´

### –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π config.yaml:

```yaml
models:
  qwen_vl_2b:
    name: "Qwen2-VL 2B (Blackwell Optimized)"
    model_path: "Qwen/Qwen2-VL-2B-Instruct"
    precision: "bf16"  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell
    attn_implementation: "eager"  # –°—Ç–∞–±–∏–ª—å–Ω–æ –Ω–∞ sm_120
    use_flash_attention: false  # –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ Blackwell
    device_map: "auto"
    trust_remote_code: true
    
  qwen3_vl_2b:
    name: "Qwen3-VL 2B (Blackwell Optimized)"
    model_path: "Qwen/Qwen3-VL-2B-Instruct"
    precision: "bf16"
    attn_implementation: "eager"
    use_flash_attention: false
    device_map: "auto"
    trust_remote_code: true
    
  dots_ocr:
    name: "dots.ocr (Blackwell Compatible)"
    model_path: "rednote-hilab/dots.ocr"
    precision: "bf16"
    attn_implementation: "eager"
    use_flash_attention: false
    device_map: "auto"
    trust_remote_code: true

performance:
  blackwell_optimizations:
    enable_tf32: true
    enable_cudnn_benchmark: true
    use_bfloat16: true
    enable_sdpa: true
  
gpu_requirements:
  rtx_5070_ti:
    compute_capability: "sm_120"
    cuda_version: "12.8+"
    pytorch_version: "2.7.0+"
    flash_attention_support: false  # –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    recommended_precision: "bf16"
    tensor_cores: "5th_gen"
```

---

## üìä –û–ñ–ò–î–ê–ï–ú–ê–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨

### –° –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è Blackwell:

| –ú–æ–¥–µ–ª—å | –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ | –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ | VRAM | –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è |
|--------|----------------|-----------------|------|-------------|
| qwen_vl_2b | ~8s | ~6s | 4.2GB | bf16 + eager |
| qwen3_vl_2b | ~9s | ~20s | 4.5GB | bf16 + eager |
| dots_ocr | ~10s | ~15s | 5.2GB | bf16 + eager |

### –£–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:
- **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å**: 100% (–Ω–µ—Ç CUDA –æ—à–∏–±–æ–∫)
- **–°–∫–æ—Ä–æ—Å—Ç—å**: +25% –±–ª–∞–≥–æ–¥–∞—Ä—è bfloat16 –∏ Tensor Cores 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è
- **–ü–∞–º—è—Ç—å**: -15% –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É precision
- **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Blackwell –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

---

## üéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

**–î–ª—è RTX 5070 Ti (Blackwell sm_120):**

1. ‚ùå **–ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Flash Attention 2** - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
2. ‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ PyTorch 2.7.0+ —Å CUDA 12.8**
3. ‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ attn_implementation="eager"**
4. ‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ torch.bfloat16** –¥–ª—è Tensor Cores 5-–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è
5. ‚úÖ **–í–∫–ª—é—á–∏—Ç–µ SDPA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏**

–≠—Ç–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±–µ—Å–ø–µ—á–∏—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ RTX 5070 Ti —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Blackwell.

---
*–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ —Å–æ–∑–¥–∞–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: PyTorch, Transformers, Flash Attention, QwenLM*