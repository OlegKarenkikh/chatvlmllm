#!/usr/bin/env python3
"""
–ó–∞–ø—É—Å–∫ dots.ocr —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π GPU –ø–∞–º—è—Ç–∏
–†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã: ValueError: Free memory on device cuda:0 (5.81/11.94 GiB)
"""

import subprocess
import time
import requests
import os

def run_command(command):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
    print(f"üîÑ {command}")
    try:
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        if result.stdout:
            print(f"‚úÖ {result.stdout.strip()}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        if e.stderr:
            print(f"‚ùå {e.stderr}")
        return False

def check_gpu_memory():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π GPU –ø–∞–º—è—Ç–∏"""
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –ø–∞–º—è—Ç–∏...")
    try:
        result = subprocess.run(
            "nvidia-smi --query-gpu=memory.total,memory.free,memory.used --format=csv,noheader,nounits",
            shell=True, capture_output=True, text=True
        )
        if result.returncode == 0:
            total, free, used = map(int, result.stdout.strip().split(', '))
            print(f"üìä GPU –ø–∞–º—è—Ç—å: {total} MB –≤—Å–µ–≥–æ, {free} MB —Å–≤–æ–±–æ–¥–Ω–æ, {used} MB –∑–∞–Ω—è—Ç–æ")
            return total, free, used
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU: {e}")
    return None, None, None

def cleanup_gpu():
    """–û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏"""
    print("üßπ –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏...")
    
    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
    print("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤...")
    run_command("docker stop $(docker ps -aq) 2>/dev/null || true")
    run_command("docker system prune -f")
    
    # –û—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–∞
    print("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–∞...")
    cleanup_script = """
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("CUDA –∫–µ—à –æ—á–∏—â–µ–Ω")
else:
    print("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
"""
    
    try:
        subprocess.run(["python", "-c", cleanup_script], check=True)
    except:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å CUDA –∫–µ—à")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–ê–ü–£–°–ö DOTS.OCR –° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ï–ô –ü–ê–ú–Ø–¢–ò")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –ø–∞–º—è—Ç–∏
    total, free, used = check_gpu_memory()
    if free and free < 6000:  # –ú–µ–Ω—å—à–µ 6GB —Å–≤–æ–±–æ–¥–Ω–æ
        print(f"‚ö†Ô∏è –ú–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–π GPU –ø–∞–º—è—Ç–∏: {free} MB")
        print("üßπ –í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É...")
        cleanup_gpu()
        time.sleep(5)
        total, free, used = check_gpu_memory()
    
    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ dots.ocr
    print("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤...")
    run_command("docker stop dots-ocr-fixed dots-ocr-simple dots-ocr-memory-opt 2>/dev/null || true")
    run_command("docker rm dots-ocr-fixed dots-ocr-simple dots-ocr-memory-opt 2>/dev/null || true")
    
    # –ü—É—Ç—å –∫ –∫–µ—à—É
    cache_path = str(os.path.expanduser("~/.cache/huggingface/hub")).replace('\\', '/')
    print(f"üìÅ –ü—É—Ç—å –∫ –∫–µ—à—É: {cache_path}")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–∞–º—è—Ç–∏
    if free:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º 40% –æ—Ç —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        gpu_util = min(0.4, (free * 0.4) / total)
        print(f"üéØ –†–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–∞—è —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è GPU: {gpu_util:.2f}")
    else:
        gpu_util = 0.35  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        print(f"üéØ –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è GPU: {gpu_util}")
    
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ dots.ocr —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–∞–º—è—Ç–∏...")
    print(f"   ‚Ä¢ GPU —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è: {gpu_util}")
    print(f"   ‚Ä¢ Max model length: 1024 (–º–∏–Ω–∏–º—É–º)")
    print(f"   ‚Ä¢ Dtype: bfloat16 (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)")
    print(f"   ‚Ä¢ Enforce eager: true (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)")
    
    # –ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –∫ –ø–∞–º—è—Ç–∏
    docker_command = f"""
    docker run -d \
        --gpus all \
        --name dots-ocr-memory-opt \
        --restart unless-stopped \
        -p 8000:8000 \
        -v {cache_path}:/root/.cache/huggingface/hub:ro \
        --shm-size=4g \
        vllm/vllm-openai:latest \
        --model rednote-hilab/dots.ocr \
        --trust-remote-code \
        --max-model-len 1024 \
        --gpu-memory-utilization {gpu_util} \
        --host 0.0.0.0 \
        --port 8000 \
        --disable-log-requests \
        --enforce-eager \
        --dtype bfloat16 \
        --max-num-seqs 1
    """.strip().replace('\n', ' ').replace('\\', '')
    
    if run_command(docker_command):
        print("‚úÖ dots.ocr –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏")
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞
        print("\n‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 5-10 –º–∏–Ω—É—Ç)...")
        max_attempts = 40
        
        for attempt in range(max_attempts):
            try:
                response = requests.get("http://localhost:8000/health", timeout=5)
                if response.status_code == 200:
                    print("‚úÖ dots.ocr –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ models endpoint
                    try:
                        models_response = requests.get("http://localhost:8000/v1/models", timeout=5)
                        if models_response.status_code == 200:
                            models_data = models_response.json()
                            print(f"üìä –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {len(models_data.get('data', []))}")
                            for model in models_data.get('data', []):
                                print(f"   ‚Ä¢ {model.get('id', 'unknown')}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}")
                    
                    break
            except requests.exceptions.ConnectionError:
                pass
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            
            print(f"‚è≥ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_attempts} (–∂–¥–µ–º –µ—â–µ 15 —Å–µ–∫...)")
            time.sleep(15)
        else:
            print("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è")
            print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: docker logs dots-ocr-memory-opt")
            print("üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print("   ‚Ä¢ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 8GB —Å–≤–æ–±–æ–¥–Ω–æ)")
            print("   ‚Ä¢ –ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –¥–ª—è –≤–∞—à–µ–π GPU")
            print("   ‚Ä¢ –ü—Ä–æ–±–ª–µ–º—ã —Å WSL/Docker")
            return
        
        print("\nüéâ –ù–ê–°–¢–†–û–ô–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("=" * 25)
        print("üì° dots.ocr –¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞: http://localhost:8000")
        print("üìã –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/docs")
        print("üîß –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:")
        print("   docker logs dots-ocr-memory-opt     # –õ–æ–≥–∏")
        print("   docker stop dots-ocr-memory-opt     # –û—Å—Ç–∞–Ω–æ–≤–∫–∞")
        print("   docker restart dots-ocr-memory-opt  # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫")
        
        print(f"\nüí° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –î–õ–Ø –û–ì–†–ê–ù–ò–ß–ï–ù–ù–û–ô –ü–ê–ú–Ø–¢–ò:")
        print(f"   ‚Ä¢ GPU —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è: {gpu_util} (—Å–Ω–∏–∂–µ–Ω–æ)")
        print("   ‚Ä¢ Max model length: 1024 (–º–∏–Ω–∏–º—É–º)")
        print("   ‚Ä¢ Dtype: bfloat16 (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)")
        print("   ‚Ä¢ Max num seqs: 1 (–æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∑–∞ —Ä–∞–∑)")
        print("   ‚Ä¢ Shared memory: 4GB (—É–º–µ–Ω—å—à–µ–Ω–æ)")
        
        print(f"\n‚ö†Ô∏è –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:")
        print("   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –∑–∞ —Ä–∞–∑")
        print("   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º 1024 —Ç–æ–∫–µ–Ω–∞ –≤ –æ—Ç–≤–µ—Ç–µ")
        print("   ‚Ä¢ –ú–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ –∏–∑-–∑–∞ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫")
        
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä")
        print("\nüîß –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ï –†–ï–®–ï–ù–ò–Ø:")
        print("1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU –≤–µ—Ä—Å–∏—é (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)")
        print("2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)")
        print("3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å transformers —Å load_in_8bit=True")
        print("4. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é OCR –º–æ–¥–µ–ª—å (GOT-OCR, PaddleOCR)")

if __name__ == "__main__":
    main()