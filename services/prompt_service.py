"""Prompt processing service for ChatVLMLLM.

Centralized prompt processing that eliminates duplication across:
- Official prompts
- Example prompts  
- User input prompts
"""

import time
import torch
import gc
import re
from typing import Dict, Any, Optional
from PIL import Image

from core.error_handler import ErrorHandler


class PromptService:
    """Unified prompt processing service."""
    
    @staticmethod
    def process_prompt(
        image: Image.Image,
        prompt: str,
        selected_model: str,
        execution_mode: str,
        max_tokens: int,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """Process a prompt with the selected model.
        
        This is the central function that handles all prompt processing,
        eliminating duplication between official prompts, examples, and user input.
        
        Args:
            image: PIL Image to process
            prompt: Text prompt
            selected_model: Model identifier
            execution_mode: "vLLM" or "Transformers"
            max_tokens: Maximum tokens to generate
            temperature: Generation temperature
            
        Returns:
            Dictionary with:
                - success: bool
                - text: str (response or error message)
                - processing_time: float
                - model_used: str
                - execution_mode: str
        """
        # Clean GPU memory before processing
        PromptService._clean_gpu_memory()
        
        start_time = time.time()
        
        try:
            if "vLLM" in execution_mode:
                result = PromptService._process_vllm(
                    image, prompt, selected_model, max_tokens, temperature
                )
            else:
                result = PromptService._process_transformers(
                    image, prompt, selected_model, max_tokens, temperature
                )
            
            processing_time = time.time() - start_time
            
            return {
                "success": True,
                "text": result,
                "processing_time": processing_time,
                "model_used": selected_model,
                "execution_mode": execution_mode
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_response = ErrorHandler.create_error_response(e, "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–º–ø—Ç–∞")
            
            return {
                "success": False,
                "text": error_response,
                "processing_time": processing_time,
                "model_used": selected_model,
                "execution_mode": execution_mode,
                "error": str(e)
            }
    
    @staticmethod
    def _process_vllm(
        image: Image.Image,
        prompt: str,
        selected_model: str,
        max_tokens: int,
        temperature: float
    ) -> str:
        """Process prompt via vLLM.
        
        Args:
            image: PIL Image
            prompt: Text prompt
            selected_model: Model name
            max_tokens: Max tokens
            temperature: Temperature
            
        Returns:
            Response text
            
        Raises:
            Exception: If processing fails
        """
        try:
            from vllm_streamlit_adapter import VLLMStreamlitAdapter
            import streamlit as st
            
            # Get or create adapter
            if "vllm_adapter" not in st.session_state:
                st.session_state.vllm_adapter = VLLMStreamlitAdapter()
            
            adapter = st.session_state.vllm_adapter
            
            # Check if model is dots.ocr for special handling
            is_dots_ocr = "dots" in selected_model.lower()
            
            if is_dots_ocr:
                # Use safe token limit for dots.ocr
                vllm_model = "rednote-hilab/dots.ocr"
                model_max_tokens = adapter.get_model_max_tokens(vllm_model)
                safe_max_tokens = min(max_tokens, model_max_tokens - 500)
                
                if safe_max_tokens < 100:
                    safe_max_tokens = model_max_tokens // 2
                
                result = adapter.process_image(image, prompt, vllm_model, safe_max_tokens)
                
                if result and result["success"]:
                    response = result["text"]
                    processing_time = result["processing_time"]
                    
                    # Adapt response based on prompt type
                    response = PromptService._adapt_dots_ocr_response(
                        response, prompt, processing_time
                    )
                else:
                    raise Exception("vLLM processing failed")
            else:
                # Regular model processing with safe token limit
                model_max_tokens = adapter.get_model_max_tokens(selected_model)
                safe_max_tokens = min(max_tokens, model_max_tokens - 500)
                
                if safe_max_tokens < 100:
                    safe_max_tokens = model_max_tokens // 2
                
                result = adapter.process_image(image, prompt, selected_model, safe_max_tokens)
                
                if result and result["success"]:
                    response = result["text"]
                    processing_time = result["processing_time"]
                    response += f"\n\n*üöÄ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–µ—Ä–µ–∑ vLLM –∑–∞ {processing_time:.2f}—Å*"
                else:
                    raise Exception("vLLM processing failed")
            
            return response
            
        except Exception as e:
            # Try fallback to Transformers if not a critical error
            if not ErrorHandler.is_cuda_error(e):
                return PromptService._process_transformers(
                    image, prompt, selected_model, max_tokens, temperature
                )
            raise
    
    @staticmethod
    def _process_transformers(
        image: Image.Image,
        prompt: str,
        selected_model: str,
        max_tokens: int,
        temperature: float
    ) -> str:
        """Process prompt via Transformers.
        
        Args:
            image: PIL Image
            prompt: Text prompt
            selected_model: Model name
            max_tokens: Max tokens
            temperature: Temperature
            
        Returns:
            Response text
            
        Raises:
            Exception: If processing fails
        """
        from models.model_loader import ModelLoader
        
        model = ModelLoader.load_model(selected_model)
        
        # Get response based on model capabilities
        if hasattr(model, 'chat'):
            response = model.chat(
                image=image,
                prompt=prompt,
                temperature=temperature,
                max_new_tokens=max_tokens
            )
        elif hasattr(model, 'process_image'):
            # For OCR models, adapt based on prompt
            if any(word in prompt.lower() for word in ['—Ç–µ–∫—Å—Ç', '–ø—Ä–æ—á–∏—Ç–∞–π', '–∏–∑–≤–ª–µ–∫–∏']):
                response = model.process_image(image)
            else:
                ocr_text = model.process_image(image)
                response = f"–≠—Ç–æ OCR –º–æ–¥–µ–ª—å. –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n\n{ocr_text}"
        else:
            response = "–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —á–∞—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–µ–∂–∏–º OCR."
        
        processing_time = time.time() - time.time()  # Will be calculated in main function
        response += f"\n\n*üîß –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ —Å –ø–æ–º–æ—â—å—é {selected_model}*"
        
        return response
    
    @staticmethod
    def _adapt_dots_ocr_response(response: str, prompt: str, processing_time: float) -> str:
        """Adapt dots.ocr response based on prompt type.
        
        Args:
            response: Model response
            prompt: Original prompt
            processing_time: Processing time
            
        Returns:
            Adapted response
        """
        # Check prompt type
        if any(word in prompt.lower() for word in ['—Ç–µ–∫—Å—Ç', '–ø—Ä–æ—á–∏—Ç–∞–π', '–∏–∑–≤–ª–µ–∫–∏', 'text', 'extract', 'read']):
            # OCR question - return as is
            pass
        elif any(word in prompt.lower() for word in ['—á–∏—Å–ª–æ', 'number']):
            # Number question - extract numbers
            numbers = re.findall(r'\d+', response)
            if numbers:
                response = f"–í –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–∞–π–¥–µ–Ω—ã —á–∏—Å–ª–∞: {', '.join(numbers)}"
            else:
                response = "–í –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —á–∏—Å–µ–ª."
        elif any(word in prompt.lower() for word in ['—Å–∫–æ–ª—å–∫–æ', 'how many']):
            # Count question
            words = len(response.split())
            response = f"–í —Ç–µ–∫—Å—Ç–µ –ø—Ä–∏–º–µ—Ä–Ω–æ {words} —Å–ª–æ–≤."
        elif any(word in prompt.lower() for word in ['–µ—Å—Ç—å –ª–∏', 'is there']):
            # Existence question
            if '—Ç–µ–∫—Å—Ç' in prompt.lower() or 'text' in prompt.lower():
                response = f"–î–∞, –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç:\n\n{response}"
            else:
                response = f"dots.ocr –º–æ–∂–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞. –ù–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n\n{response}"
        else:
            # General analytical question
            response = f"dots.ocr —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ OCR. –í–æ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å:\n\n{response}\n\nüí° –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Qwen3-VL –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –º–æ–¥–µ–ª–∏."
        
        # Add timing info
        response += f"\n\n*üöÄ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–µ—Ä–µ–∑ vLLM –∑–∞ {processing_time:.2f}—Å*"
        
        return response
    
    @staticmethod
    def _clean_gpu_memory():
        """Clean GPU memory before processing."""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
