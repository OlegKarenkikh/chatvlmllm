{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration\n",
    "\n",
    "This notebook demonstrates how to use the ChatVLMLLM models for document OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Add project root to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from PIL import Image\n",
    "from models import ModelLoader\n",
    "from utils import ImageProcessor, TextExtractor\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model\n",
    "\n",
    "First, let's load a model. Start with GOT-OCR for basic OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load GOT-OCR model\n",
    "print(\"Loading model...\")\n",
    "model = ModelLoader.load_model('got_ocr')\n",
    "print(f\"Model loaded: {model.get_model_info()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load image\n",
    "image_path = '../examples/sample_document.jpg'  # Update with your image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Display original\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Original Image')\n",
    "plt.show()\n",
    "\n",
    "# Get image info\n",
    "info = ImageProcessor.get_image_info(image)\n",
    "print(f\"Image info: {info}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess image\n",
    "processed_image = ImageProcessor.preprocess(\n",
    "    image,\n",
    "    resize=True,\n",
    "    enhance=True,\n",
    "    denoise=False\n",
    ")\n",
    "\n",
    "# Display processed\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(processed_image)\n",
    "plt.axis('off')\n",
    "plt.title('Processed Image')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract text\n",
    "print(\"Running OCR...\")\n",
    "text = model.process_image(processed_image)\n",
    "\n",
    "print(\"\\n=== Extracted Text ===")\n",
    "print(text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post-process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clean text\n",
    "cleaned_text = TextExtractor.clean_text(text)\n",
    "\n",
    "print(\"\\n=== Cleaned Text ===")\n",
    "print(cleaned_text)\n",
    "\n",
    "# Extract entities\n",
    "dates = TextExtractor.extract_dates(cleaned_text)\n",
    "emails = TextExtractor.extract_emails(cleaned_text)\n",
    "phones = TextExtractor.extract_phone_numbers(cleaned_text)\n",
    "amounts = TextExtractor.extract_amounts(cleaned_text)\n",
    "\n",
    "print(\"\\n=== Extracted Entities ===")\n",
    "print(f\"Dates: {dates}\")\n",
    "print(f\"Emails: {emails}\")\n",
    "print(f\"Phones: {phones}\")\n",
    "print(f\"Amounts: {amounts}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Field Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from utils.field_parser import FieldParser\n",
    "\n",
    "# Parse as invoice (example)\n",
    "fields = FieldParser.parse_invoice(cleaned_text)\n",
    "\n",
    "print(\"\\n=== Extracted Fields ===")\n",
    "for field, value in fields.items():\n",
    "    print(f\"{field}: {value}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Try Qwen2-VL for Intelligent Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Qwen2-VL\n",
    "qwen_model = ModelLoader.load_model('qwen_vl_2b')\n",
    "\n",
    "# Ask specific question\n",
    "prompt = \"\"\"Extract the following information from this document:\n",
    "- Document type\n",
    "- Date\n",
    "- Total amount\n",
    "- Vendor/Issuer name\n",
    "\n",
    "Provide the answer in JSON format.\"\"\"\n",
    "\n",
    "response = qwen_model.process_image(processed_image, prompt)\n",
    "\n",
    "print(\"\\n=== Qwen2-VL Response ===")\n",
    "print(response)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Chat with the model\n",
    "history = []\n",
    "\n",
    "# First question\n",
    "response1 = qwen_model.chat(processed_image, \"What type of document is this?\")\n",
    "print(f\"Q: What type of document is this?\")\n",
    "print(f\"A: {response1}\\n\")\n",
    "history.append({\"role\": \"assistant\", \"content\": response1})\n",
    "\n",
    "# Follow-up question\n",
    "response2 = qwen_model.chat(\n",
    "    processed_image,\n",
    "    \"Can you extract all the monetary amounts?\",\n",
    "    history=history\n",
    ")\n",
    "print(f\"Q: Can you extract all the monetary amounts?\")\n",
    "print(f\"A: {response2}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "# Benchmark GOT-OCR\n",
    "start = time.time()\n",
    "got_result = model.process_image(processed_image)\n",
    "got_time = time.time() - start\n",
    "\n",
    "# Benchmark Qwen2-VL\n",
    "start = time.time()\n",
    "qwen_result = qwen_model.process_image(processed_image)\n",
    "qwen_time = time.time() - start\n",
    "\n",
    "print(\"\\n=== Performance Comparison ===")\n",
    "print(f\"GOT-OCR: {got_time:.2f}s\")\n",
    "print(f\"Qwen2-VL: {qwen_time:.2f}s\")\n",
    "print(f\"Speed ratio: {qwen_time/got_time:.2f}x\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Unload models to free memory\n",
    "ModelLoader.unload_all()\n",
    "print(\"Models unloaded\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}