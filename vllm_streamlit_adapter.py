#!/usr/bin/env python3
"""
–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ vLLM API —Å Streamlit –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
–í–∫–ª—é—á–∞–µ—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
–ü–†–ò–ù–¶–ò–ü: –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∞–∫—Ç–∏–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
"""

import requests
import base64
import time
import streamlit as st
from PIL import Image
import io
from typing import Optional, Dict, Any, List
from single_container_manager import SingleContainerManager

class VLLMStreamlitAdapter:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.available_models = []
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        self.container_manager = SingleContainerManager()
        
        # –ú–∞–ø–ø–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ—Ä—Ç—ã –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        self.model_endpoints = {
            "rednote-hilab/dots.ocr": "http://localhost:8000",
            "Qwen/Qwen2-VL-2B-Instruct": "http://localhost:8001", 
            "Qwen/Qwen3-VL-2B-Instruct": "http://localhost:8004",
            "microsoft/Phi-3.5-vision-instruct": "http://localhost:8002",
            "Qwen/Qwen2-VL-7B-Instruct": "http://localhost:8003"
        }
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        self.model_priorities = {
            "rednote-hilab/dots.ocr": 1,
            "Qwen/Qwen3-VL-2B-Instruct": 2,
            "Qwen/Qwen2-VL-2B-Instruct": 3,
            "microsoft/Phi-3.5-vision-instruct": 4,
            "Qwen/Qwen2-VL-7B-Instruct": 5
        }
        
        self.check_all_connections()
    
    def check_all_connections(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∞–∫—Ç–∏–≤–Ω–æ–º—É vLLM —Å–µ—Ä–≤–µ—Ä—É"""
        self.available_models = []
        self.model_limits = {}
        self.healthy_endpoints = {}
        
        # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –æ—Ç –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        active_model_key = self.container_manager.get_active_model()
        
        if not active_model_key:
            st.warning("‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏. –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.")
            return False
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∞–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
        active_config = self.container_manager.models_config.get(active_model_key)
        if not active_config:
            st.error(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {active_model_key} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return False
        
        model_path = active_config["model_path"]
        endpoint = f"http://localhost:{active_config['port']}"
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º health
            response = requests.get(f"{endpoint}/health", timeout=5)
            if response.status_code == 200:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º models endpoint
                models_response = requests.get(f"{endpoint}/v1/models", timeout=5)
                if models_response.status_code == 200:
                    models_data = models_response.json()
                    for model in models_data.get("data", []):
                        if model["id"] == model_path:
                            self.available_models.append(model_path)
                            self.model_limits[model_path] = model.get("max_model_len", 1024)
                            self.healthy_endpoints[model_path] = endpoint
                            
                            st.success(f"‚úÖ {active_config['display_name']} –∞–∫—Ç–∏–≤–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞")
                            return True
                
                st.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ API")
                return False
            else:
                st.warning(f"‚ö†Ô∏è {active_config['display_name']} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (health check failed)")
                return False
                
        except Exception as e:
            st.warning(f"‚ö†Ô∏è {active_config['display_name']} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {str(e)[:50]}...")
            return False
    
    def get_endpoint_for_model(self, model_name: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ endpoint –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        return self.healthy_endpoints.get(model_name, self.base_url)
    
    def ensure_model_available(self, model_name: str) -> bool:
        """–û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∞–∫—Ç–∏–≤–Ω–∞ –ª–∏ —É–∂–µ –Ω—É–∂–Ω–∞—è –º–æ–¥–µ–ª—å
        if model_name in self.healthy_endpoints:
            return True
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–ª—é—á –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
        target_model_key = None
        for model_key, config in self.container_manager.models_config.items():
            if config["model_path"] == model_name:
                target_model_key = model_key
                break
        
        if not target_model_key:
            st.error(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return False
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –Ω—É–∂–Ω—É—é –º–æ–¥–µ–ª—å
        st.info(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ {model_name.split('/')[-1]}...")
        success, message = self.container_manager.start_single_container(target_model_key)
        
        if success:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö endpoints
            time.sleep(3)  # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é
            self.check_all_connections()
            return model_name in self.healthy_endpoints
        else:
            st.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}: {message}")
            return False
    
    def get_recommended_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        available_sorted = sorted(
            self.available_models,
            key=lambda x: self.model_priorities.get(x, 999)
        )
        return available_sorted
    
    def check_connection(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ vLLM —Å–µ—Ä–≤–µ—Ä—É (legacy –º–µ—Ç–æ–¥)"""
        return self.check_all_connections()
    
    def get_available_models(self) -> list:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            response = requests.get(f"{self.base_url}/v1/models", timeout=5)
            if response.status_code == 200:
                models_data = response.json()
                self.available_models = []
                self.model_limits = {}
                
                for model in models_data.get("data", []):
                    model_id = model["id"]
                    max_tokens = model.get("max_model_len", 1024)
                    
                    self.available_models.append(model_id)
                    self.model_limits[model_id] = max_tokens
                
                return self.available_models
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return []
    
    def get_model_max_tokens(self, model_id: str) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        return getattr(self, 'model_limits', {}).get(model_id, 1024)
    
    def chat_with_image(self, image: Image.Image, prompt: str, 
                       model: str = "rednote-hilab/dots.ocr") -> Optional[Dict[str, Any]]:
        """–ß–∞—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ vLLM API"""
        return self.process_image(image, prompt, model)
    
    def process_image(self, image: Image.Image, prompt: str = "Extract all text from this image", 
                     model: str = "rednote-hilab/dots.ocr", max_tokens: int = 4096) -> Optional[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ vLLM API —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º–∏"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        if not self.ensure_model_available(model):
            return {
                "success": False,
                "error": f"–ú–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞",
                "text": "",
                "processing_time": 0
            }
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π endpoint –¥–ª—è –º–æ–¥–µ–ª–∏
        endpoint = self.get_endpoint_for_model(model)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        model_max_tokens = self.get_model_max_tokens(model)
        
        # –£–õ–£–ß–®–ï–ù–ò–ï: –ë–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
        if max_tokens > model_max_tokens:
            st.warning(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—à–µ–Ω–æ {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ –º–æ–¥–µ–ª—å {model} –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º {model_max_tokens}")
            max_tokens = model_max_tokens
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        estimated_input_tokens = len(prompt.split()) * 1.3 + 200  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: –ø—Ä–æ–º–ø—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if max_tokens + estimated_input_tokens > model_max_tokens:
            adjusted_tokens = max(100, model_max_tokens - int(estimated_input_tokens))
            st.info(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã —Ç–æ–∫–µ–Ω—ã: {max_tokens} ‚Üí {adjusted_tokens} (—Ä–µ–∑–µ—Ä–≤ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤)")
            max_tokens = adjusted_tokens
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        buffer = io.BytesIO()
        image.save(buffer, format='PNG')
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        payload = {
            "model": model,
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}
                ]
            }],
            "max_tokens": max_tokens,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            "temperature": 0.1
        }
        
        try:
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É endpoint
            start_time = time.time()
            
            model_display_name = model.split('/')[-1]
            with st.spinner(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ {model_display_name} (–º–∞–∫—Å. {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤)..."):
                response = requests.post(
                    f"{endpoint}/v1/chat/completions",
                    json=payload,
                    timeout=120
                )
            
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "text": content,
                    "processing_time": processing_time,
                    "model": model,
                    "model_display_name": model_display_name,
                    "endpoint": endpoint,
                    "mode": "vLLM",
                    "tokens_used": result.get("usage", {}).get("total_tokens", 0),
                    "max_tokens_limit": model_max_tokens,
                    "actual_max_tokens": max_tokens
                }
            else:
                error_text = response.text
                st.error(f"‚ùå API –æ—à–∏–±–∫–∞: {response.status_code}")
                
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
                if "max_tokens" in error_text and "exceeds" in error_text:
                    st.error("üö® **–û–®–ò–ë–ö–ê –õ–ò–ú–ò–¢–ê –¢–û–ö–ï–ù–û–í**")
                    st.error(f"–ó–∞–ø—Ä–æ—à–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {max_tokens}")
                    st.error(f"–õ–∏–º–∏—Ç –º–æ–¥–µ–ª–∏: {model_max_tokens}")
                    st.info("üí° **–†–µ—à–µ–Ω–∏–µ:** –£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é")
                
                st.error(f"–û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {error_text}")
                return {
                    "success": False,
                    "error": f"API –æ—à–∏–±–∫–∞: {response.status_code}",
                    "text": "",
                    "processing_time": processing_time
                }
                
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ {endpoint}: {e}")
            return {
                "success": False,
                "error": str(e),
                "text": "",
                "processing_time": 0
            }
    
    def get_server_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—Å–µ—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤"""
        healthy_count = len(self.healthy_endpoints)
        total_count = len(self.model_endpoints)
        
        return {
            "status": "healthy" if healthy_count > 0 else "error",
            "healthy_endpoints": healthy_count,
            "total_endpoints": total_count,
            "available_models": self.available_models,
            "model_limits": getattr(self, 'model_limits', {}),
            "endpoints": self.healthy_endpoints
        }

def create_vllm_interface():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å vLLM"""
    st.header("üöÄ vLLM –†–µ–∂–∏–º")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
    if "vllm_adapter" not in st.session_state:
        st.session_state.vllm_adapter = VLLMStreamlitAdapter()
    
    adapter = st.session_state.vllm_adapter
    
    # –°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–µ—Ä–∞
    status = adapter.get_server_status()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if status["status"] == "healthy":
            st.success(f"‚úÖ vLLM –°–µ—Ä–≤–µ—Ä—ã ({status['healthy_endpoints']}/{status['total_endpoints']})")
        else:
            st.error("‚ùå vLLM –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    with col2:
        st.info(f"ü§ñ –ú–æ–¥–µ–ª–µ–π: {len(status['available_models'])}")
    
    with col3:
        if status.get("endpoints"):
            st.info(f"üåê –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ä—Ç–æ–≤: {len(status['endpoints'])}")
    
    if status["status"] != "healthy":
        st.error("‚ùå vLLM —Å–µ—Ä–≤–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã:")
        st.code("docker-compose -f docker-compose-vllm.yml up -d")
        return
    
    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
    if adapter.available_models:
        recommended_models = adapter.get_recommended_models()
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å—Ç–∞—Ç—É—Å –ø–∞–º—è—Ç–∏
        memory_status = adapter.memory_manager.get_memory_status()
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            selected_model = st.selectbox(
                "ü§ñ –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å",
                recommended_models,
                help="–ú–æ–¥–µ–ª–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–∞–º—è—Ç—å—é."
            )
        
        with col2:
            st.metric(
                "GPU –ø–∞–º—è—Ç—å", 
                f"{memory_status['current_memory_gb']:.1f}/{memory_status['max_memory_gb']} –ì–ë",
                f"{memory_status['memory_usage_percent']:.1f}%"
            )
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        if selected_model in adapter.healthy_endpoints:
            st.success(f"‚úÖ {selected_model.split('/')[-1]} –∞–∫—Ç–∏–≤–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
        else:
            st.warning(f"‚ö†Ô∏è {selected_model.split('/')[-1]} –±—É–¥–µ—Ç –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ")
            
            if st.button("üöÄ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å —Å–µ–π—á–∞—Å"):
                with st.spinner("–ê–∫—Ç–∏–≤–∞—Ü–∏—è –º–æ–¥–µ–ª–∏..."):
                    success = adapter.ensure_model_available(selected_model)
                    if success:
                        st.success("‚úÖ –ú–æ–¥–µ–ª—å –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞!")
                        st.rerun()
                    else:
                        st.error("‚ùå –û—à–∏–±–∫–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏")
    else:
        st.error("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        return
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–º–ø—Ç–∞
    st.subheader("üìù –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    prompt_type = st.selectbox(
        "–¢–∏–ø –∑–∞–¥–∞—á–∏",
        [
            "Extract all text from this image",
            "Describe what you see in this image",
            "Extract structured data from this document",
            "Identify and extract key information",
            "Custom prompt"
        ]
    )
    
    if prompt_type == "Custom prompt":
        custom_prompt = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π –ø—Ä–æ–º–ø—Ç",
            value="Extract all text from this image",
            help="–û–ø–∏—à–∏—Ç–µ, —á—Ç–æ –¥–æ–ª–∂–Ω–∞ —Å–¥–µ–ª–∞—Ç—å –º–æ–¥–µ–ª—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"
        )
        prompt = custom_prompt
    else:
        prompt = prompt_type
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    st.subheader("üì∑ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
        type=['png', 'jpg', 'jpeg', 'bmp', 'tiff'],
        help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: PNG, JPG, JPEG, BMP, TIFF"
    )
    
    if uploaded_file is not None:
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.open(uploaded_file)
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.image(image, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_column_width=True)
            st.info(f"üìè –†–∞–∑–º–µ—Ä: {image.size[0]}x{image.size[1]}")
        
        with col2:
            if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type="primary", use_container_width=True):
                result = adapter.process_image(image, prompt, selected_model)
                
                if result and result["success"]:
                    st.success("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                    
                    # –†–µ–∑—É–ª—å—Ç–∞—Ç
                    st.subheader("üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç OCR")
                    st.text_area(
                        "–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
                        value=result["text"],
                        height=200,
                        help="–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
                    )
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        st.metric("‚è±Ô∏è –í—Ä–µ–º—è", f"{result['processing_time']:.1f} —Å–µ–∫")
                    
                    with col2:
                        st.metric("ü§ñ –ú–æ–¥–µ–ª—å", result.get("model_display_name", result["model"].split("/")[-1]))
                    
                    with col3:
                        st.metric("üîß –†–µ–∂–∏–º", result["mode"])
                    
                    with col4:
                        st.metric("üî¢ –¢–æ–∫–µ–Ω–æ–≤", result.get("tokens_used", 0))
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    with st.expander("üìä –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"):
                        st.json({
                            "model": result["model"],
                            "processing_time": result["processing_time"],
                            "tokens_used": result.get("tokens_used", 0),
                            "mode": result["mode"],
                            "prompt": prompt
                        })
                    
                    # –ö–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if st.button("üìã –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"):
                            st.write("–¢–µ–∫—Å—Ç —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞!")
                    
                    with col2:
                        # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
                        export_data = {
                            "text": result["text"],
                            "model": result["model"],
                            "processing_time": result["processing_time"],
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        st.download_button(
                            "üíæ –°–∫–∞—á–∞—Ç—å JSON",
                            data=str(export_data),
                            file_name=f"ocr_result_{int(time.time())}.json",
                            mime="application/json"
                        )

if __name__ == "__main__":
    create_vllm_interface()