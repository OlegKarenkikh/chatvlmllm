#!/usr/bin/env python3
"""
–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ vLLM API —Å Streamlit –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
"""

import requests
import base64
import time
import streamlit as st
from PIL import Image
import io
from typing import Optional, Dict, Any

class VLLMStreamlitAdapter:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.available_models = []
        self.check_connection()
    
    def check_connection(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ vLLM —Å–µ—Ä–≤–µ—Ä—É"""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            if response.status_code == 200:
                self.get_available_models()
                return True
        except Exception as e:
            st.error(f"‚ùå –ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ vLLM —Å–µ—Ä–≤–µ—Ä—É: {e}")
        return False
    
    def get_available_models(self) -> list:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            response = requests.get(f"{self.base_url}/v1/models", timeout=5)
            if response.status_code == 200:
                models_data = response.json()
                self.available_models = []
                self.model_limits = {}
                
                for model in models_data.get("data", []):
                    model_id = model["id"]
                    max_tokens = model.get("max_model_len", 1024)
                    
                    self.available_models.append(model_id)
                    self.model_limits[model_id] = max_tokens
                
                return self.available_models
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
        return []
    
    def get_model_max_tokens(self, model_id: str) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        return getattr(self, 'model_limits', {}).get(model_id, 1024)
    
    def chat_with_image(self, image: Image.Image, prompt: str, 
                       model: str = "rednote-hilab/dots.ocr") -> Optional[Dict[str, Any]]:
        """–ß–∞—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ vLLM API"""
        return self.process_image(image, prompt, model)
    
    def process_image(self, image: Image.Image, prompt: str = "Extract all text from this image", 
                     model: str = "rednote-hilab/dots.ocr", max_tokens: int = 4096) -> Optional[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ vLLM API"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        model_max_tokens = self.get_model_max_tokens(model)
        
        # –£–õ–£–ß–®–ï–ù–ò–ï: –ë–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
        if max_tokens > model_max_tokens:
            st.warning(f"‚ö†Ô∏è –ó–∞–ø—Ä–æ—à–µ–Ω–æ {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ –º–æ–¥–µ–ª—å {model} –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º {model_max_tokens}")
            max_tokens = model_max_tokens
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        estimated_input_tokens = len(prompt.split()) * 1.3 + 200  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: –ø—Ä–æ–º–ø—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if max_tokens + estimated_input_tokens > model_max_tokens:
            adjusted_tokens = max(100, model_max_tokens - int(estimated_input_tokens))
            st.info(f"üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã —Ç–æ–∫–µ–Ω—ã: {max_tokens} ‚Üí {adjusted_tokens} (—Ä–µ–∑–µ—Ä–≤ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤)")
            max_tokens = adjusted_tokens
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        buffer = io.BytesIO()
        image.save(buffer, format='PNG')
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
        payload = {
            "model": model,
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}
                ]
            }],
            "max_tokens": max_tokens,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            "temperature": 0.1
        }
        
        try:
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            start_time = time.time()
            
            with st.spinner(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ vLLM (–º–∞–∫—Å. {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤)..."):
                response = requests.post(
                    f"{self.base_url}/v1/chat/completions",
                    json=payload,
                    timeout=120
                )
            
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "text": content,
                    "processing_time": processing_time,
                    "model": model,
                    "mode": "vLLM",
                    "tokens_used": result.get("usage", {}).get("total_tokens", 0),
                    "max_tokens_limit": model_max_tokens,
                    "actual_max_tokens": max_tokens
                }
            else:
                error_text = response.text
                st.error(f"‚ùå API –æ—à–∏–±–∫–∞: {response.status_code}")
                
                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
                if "max_tokens" in error_text and "exceeds" in error_text:
                    st.error("üö® **–û–®–ò–ë–ö–ê –õ–ò–ú–ò–¢–ê –¢–û–ö–ï–ù–û–í**")
                    st.error(f"–ó–∞–ø—Ä–æ—à–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {max_tokens}")
                    st.error(f"–õ–∏–º–∏—Ç –º–æ–¥–µ–ª–∏: {model_max_tokens}")
                    st.info("üí° **–†–µ—à–µ–Ω–∏–µ:** –£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é")
                
                st.error(f"–û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {error_text}")
                return None
                
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return None
    
    def get_server_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            if response.status_code == 200:
                return {
                    "status": "healthy",
                    "url": self.base_url,
                    "models": len(self.available_models),
                    "available_models": self.available_models,
                    "model_limits": getattr(self, 'model_limits', {})
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "url": self.base_url
            }
        
        return {"status": "unknown"}

def create_vllm_interface():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å vLLM"""
    st.header("üöÄ vLLM –†–µ–∂–∏–º")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
    if "vllm_adapter" not in st.session_state:
        st.session_state.vllm_adapter = VLLMStreamlitAdapter()
    
    adapter = st.session_state.vllm_adapter
    
    # –°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–µ—Ä–∞
    status = adapter.get_server_status()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if status["status"] == "healthy":
            st.success("‚úÖ vLLM –°–µ—Ä–≤–µ—Ä")
        else:
            st.error("‚ùå vLLM –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    with col2:
        st.info(f"üåê {status['url']}")
    
    with col3:
        if status["status"] == "healthy":
            st.info(f"ü§ñ –ú–æ–¥–µ–ª–µ–π: {status['models']}")
    
    if status["status"] != "healthy":
        st.error("‚ùå vLLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω:")
        st.code("docker-compose -f docker-compose-vllm.yml up -d dots-ocr")
        return
    
    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
    if adapter.available_models:
        selected_model = st.selectbox(
            "ü§ñ –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å",
            adapter.available_models,
            help="–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ vLLM —Å–µ—Ä–≤–µ—Ä–µ"
        )
    else:
        st.error("‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        return
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–º–ø—Ç–∞
    st.subheader("üìù –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    prompt_type = st.selectbox(
        "–¢–∏–ø –∑–∞–¥–∞—á–∏",
        [
            "Extract all text from this image",
            "Describe what you see in this image",
            "Extract structured data from this document",
            "Identify and extract key information",
            "Custom prompt"
        ]
    )
    
    if prompt_type == "Custom prompt":
        custom_prompt = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π –ø—Ä–æ–º–ø—Ç",
            value="Extract all text from this image",
            help="–û–ø–∏—à–∏—Ç–µ, —á—Ç–æ –¥–æ–ª–∂–Ω–∞ —Å–¥–µ–ª–∞—Ç—å –º–æ–¥–µ–ª—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"
        )
        prompt = custom_prompt
    else:
        prompt = prompt_type
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    st.subheader("üì∑ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ",
        type=['png', 'jpg', 'jpeg', 'bmp', 'tiff'],
        help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: PNG, JPG, JPEG, BMP, TIFF"
    )
    
    if uploaded_file is not None:
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image = Image.open(uploaded_file)
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.image(image, caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_column_width=True)
            st.info(f"üìè –†–∞–∑–º–µ—Ä: {image.size[0]}x{image.size[1]}")
        
        with col2:
            if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type="primary", use_container_width=True):
                result = adapter.process_image(image, prompt, selected_model)
                
                if result and result["success"]:
                    st.success("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                    
                    # –†–µ–∑—É–ª—å—Ç–∞—Ç
                    st.subheader("üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç OCR")
                    st.text_area(
                        "–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
                        value=result["text"],
                        height=200,
                        help="–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
                    )
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("‚è±Ô∏è –í—Ä–µ–º—è", f"{result['processing_time']:.1f} —Å–µ–∫")
                    
                    with col2:
                        st.metric("ü§ñ –ú–æ–¥–µ–ª—å", result["model"].split("/")[-1])
                    
                    with col3:
                        st.metric("üîß –†–µ–∂–∏–º", result["mode"])
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    with st.expander("üìä –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"):
                        st.json({
                            "model": result["model"],
                            "processing_time": result["processing_time"],
                            "tokens_used": result.get("tokens_used", 0),
                            "mode": result["mode"],
                            "prompt": prompt
                        })
                    
                    # –ö–Ω–æ–ø–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if st.button("üìã –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"):
                            st.write("–¢–µ–∫—Å—Ç —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞!")
                    
                    with col2:
                        # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
                        export_data = {
                            "text": result["text"],
                            "model": result["model"],
                            "processing_time": result["processing_time"],
                            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        st.download_button(
                            "üíæ –°–∫–∞—á–∞—Ç—å JSON",
                            data=str(export_data),
                            file_name=f"ocr_result_{int(time.time())}.json",
                            mime="application/json"
                        )

if __name__ == "__main__":
    create_vllm_interface()