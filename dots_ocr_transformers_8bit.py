#!/usr/bin/env python3
"""
–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: dots.ocr —á–µ—Ä–µ–∑ transformers —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
–î–ª—è GPU —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é (< 8GB —Å–≤–æ–±–æ–¥–Ω–æ)
"""

import torch
import base64
import io
from PIL import Image
from transformers import AutoModelForCausalLM, AutoProcessor
import requests
from flask import Flask, request, jsonify
import threading
import time

class DotsOCRTransformers:
    def __init__(self):
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_loaded = False
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π"""
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ dots.ocr —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π...")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π GPU –ø–∞–º—è—Ç–∏
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                free_memory = (torch.cuda.get_device_properties(0).total_memory - 
                             torch.cuda.memory_allocated(0)) / 1024**3
                print(f"üìä GPU: {gpu_memory:.1f}GB –≤—Å–µ–≥–æ, {free_memory:.1f}GB —Å–≤–æ–±–æ–¥–Ω–æ")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            print("üìù –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞...")
            self.processor = AutoProcessor.from_pretrained(
                "rednote-hilab/dots.ocr",
                trust_remote_code=True
            )
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
            print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π...")
            self.model = AutoModelForCausalLM.from_pretrained(
                "rednote-hilab/dots.ocr",
                torch_dtype=torch.bfloat16,
                device_map="auto",
                load_in_8bit=True,  # 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                max_memory={0: "6GB"}  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU
            )
            
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            self.model_loaded = True
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                print(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ GPU –ø–∞–º—è—Ç–∏: {allocated:.2f}GB")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def process_image(self, image_path_or_base64, prompt="Extract all text from this image"):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not self.model_loaded:
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if image_path_or_base64.startswith('data:image'):
                # Base64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_data = image_path_or_base64.split(',')[1]
                image_bytes = base64.b64decode(image_data)
                image = Image.open(io.BytesIO(image_bytes))
            else:
                # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
                image = Image.open(image_path_or_base64)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            print(f"üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image.size}")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image", "image": image}
                    ]
                }
            ]
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–∞
            image_inputs, video_inputs = self.processor.process_vision_info(messages)
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            
            # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs = inputs.to(self.device)
            
            print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    do_sample=False,
                    temperature=0.1,
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            print("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
            return {
                "success": True,
                "text": output_text.strip(),
                "model": "rednote-hilab/dots.ocr",
                "method": "transformers_8bit"
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return {"error": str(e)}

# Flask API —Å–µ—Ä–≤–µ—Ä
app = Flask(__name__)
ocr_model = DotsOCRTransformers()

@app.route('/health')
def health():
    return jsonify({
        "status": "healthy" if ocr_model.model_loaded else "loading",
        "model_loaded": ocr_model.model_loaded
    })

@app.route('/v1/models')
def models():
    return jsonify({
        "data": [{
            "id": "rednote-hilab/dots.ocr",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "transformers_8bit"
        }]
    })

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    try:
        data = request.json
        messages = data.get('messages', [])
        
        if not messages:
            return jsonify({"error": "No messages provided"}), 400
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
        user_message = messages[-1]
        content = user_message.get('content', [])
        
        text_prompt = "Extract all text from this image"
        image_data = None
        
        for item in content:
            if item.get('type') == 'text':
                text_prompt = item.get('text', text_prompt)
            elif item.get('type') == 'image_url':
                image_data = item.get('image_url', {}).get('url')
        
        if not image_data:
            return jsonify({"error": "No image provided"}), 400
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        result = ocr_model.process_image(image_data, text_prompt)
        
        if "error" in result:
            return jsonify({"error": result["error"]}), 500
        
        # –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ OpenAI API
        response = {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": "rednote-hilab/dots.ocr",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": result["text"]
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 100,  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                "completion_tokens": len(result["text"].split()),
                "total_tokens": 100 + len(result["text"].split())
            }
        }
        
        return jsonify(response)
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def load_model_background():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ"""
    ocr_model.load_model()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–ê–ü–£–°–ö DOTS.OCR –ß–ï–†–ï–ó TRANSFORMERS (8-BIT)")
    print("=" * 55)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}")
        print(f"üìä GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU (–º–µ–¥–ª–µ–Ω–Ω–æ)")
    
    # –ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–µ
    print("üîÑ –ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ...")
    model_thread = threading.Thread(target=load_model_background)
    model_thread.daemon = True
    model_thread.start()
    
    # –ó–∞–ø—É—Å–∫ Flask —Å–µ—Ä–≤–µ—Ä–∞
    print("üåê –ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É 8000...")
    print("üì° API –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –Ω–∞: http://localhost:8000")
    print("üìã Endpoints:")
    print("   ‚Ä¢ Health: http://localhost:8000/health")
    print("   ‚Ä¢ Models: http://localhost:8000/v1/models")
    print("   ‚Ä¢ Chat: http://localhost:8000/v1/chat/completions")
    
    app.run(host='0.0.0.0', port=8000, debug=False)

if __name__ == "__main__":
    main()