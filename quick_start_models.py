#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π vLLM
"""

import subprocess
import time
import requests
import json
import os

def run_command(command):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
    try:
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        return True, result.stdout.strip()
    except subprocess.CalledProcessError as e:
        return False, e.stderr.strip() if e.stderr else str(e)

def check_model_health(port):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
    try:
        response = requests.get(f"http://localhost:{port}/health", timeout=5)
        return response.status_code == 200
    except:
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ë–´–°–¢–†–´–ô –ó–ê–ü–£–°–ö –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–• –ú–û–î–ï–õ–ï–ô")
    print("=" * 40)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
    try:
        with open('vllm_models_config.json', 'r', encoding='utf-8') as f:
            configs = json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {e}")
        return
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞
    priority_models = [
        "rednote-hilab/dots.ocr",  # –£–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        "stepfun-ai/GOT-OCR2_0",  # OCR –º–æ–¥–µ–ª—å
        "Qwen/Qwen3-VL-2B-Instruct"  # –õ–µ–≥–∫–∞—è VLM –º–æ–¥–µ–ª—å
    ]
    
    cache_path = str(os.path.expanduser("~/.cache/huggingface/hub")).replace('\\', '/')
    
    print(f"üìÅ –ü—É—Ç—å –∫ –∫–µ—à—É: {cache_path}")
    print(f"üéØ –ó–∞–ø—É—Å–∫ {len(priority_models)} –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
    
    launched_models = []
    
    for model_name in priority_models:
        if model_name not in configs:
            print(f"‚ö†Ô∏è {model_name} - –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            continue
        
        config = configs[model_name]
        container_name = config['container_name']
        port = config['port']
        
        print(f"\nüîÑ –ó–∞–ø—É—Å–∫ {model_name}...")
        print(f"   –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä: {container_name}")
        print(f"   –ü–æ—Ä—Ç: {port}")
        print(f"   –†–∞–∑–º–µ—Ä: {config['size_gb']} –ì–ë")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞ –ª–∏ —É–∂–µ
        if check_model_health(port):
            print(f"   ‚úÖ –£–∂–µ –∑–∞–ø—É—â–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞!")
            launched_models.append(model_name)
            continue
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        run_command(f"docker stop {container_name}")
        run_command(f"docker rm {container_name}")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã –∑–∞–ø—É—Å–∫–∞
        vllm_params = config['vllm_params']
        
        docker_command = f"""
        docker run -d \
            --gpus all \
            --name {container_name} \
            --restart unless-stopped \
            -p {port}:{port} \
            -v {cache_path}:/root/.cache/huggingface/hub:ro \
            --shm-size=8g \
            vllm/vllm-openai:latest \
            --model {model_name} \
            --trust-remote-code \
            --max-model-len {vllm_params['max_model_len']} \
            --gpu-memory-utilization {vllm_params['gpu_memory_utilization']} \
            --host 0.0.0.0 \
            --port {port} \
            --disable-log-requests
        """.strip().replace('\n', ' ').replace('\\', '')
        
        if vllm_params.get('enforce_eager'):
            docker_command += " --enforce-eager"
        
        # –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        success, output = run_command(docker_command)
        
        if success:
            print(f"   ‚úÖ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω")
            
            # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (–º–∞–∫—Å–∏–º—É–º 5 –º–∏–Ω—É—Ç)
            print(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏...")
            max_attempts = 30
            
            for attempt in range(max_attempts):
                if check_model_health(port):
                    print(f"   üéâ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞! ({attempt * 10} —Å–µ–∫)")
                    launched_models.append(model_name)
                    break
                
                if attempt % 3 == 0:  # –ö–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
                    print(f"   ‚è≥ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_attempts}...")
                
                time.sleep(10)
            else:
                print(f"   ‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –≥–æ—Ç–æ–≤–∞ –∑–∞ 5 –º–∏–Ω—É—Ç")
                print(f"   üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: docker logs {container_name}")
        else:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {output}")
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print(f"\nüìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 20)
    print(f"üéØ –ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {len(priority_models)} –º–æ–¥–µ–ª–µ–π")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω–æ: {len(launched_models)} –º–æ–¥–µ–ª–µ–π")
    
    if launched_models:
        print(f"\nüü¢ –ì–û–¢–û–í–´–ï –ú–û–î–ï–õ–ò:")
        for model_name in launched_models:
            config = configs[model_name]
            print(f"   ‚Ä¢ {model_name}")
            print(f"     URL: http://localhost:{config['port']}")
            print(f"     –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {config['category']}")
        
        print(f"\nüí° –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print("   1. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:")
        print("      python test_all_vllm_models.py")
        print("   2. –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞:")
        print("      python multi_model_launcher.py --create-client")
        print("   3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞:")
        print("      python multi_model_launcher.py --status")
    else:
        print(f"\n‚ùå –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞")
        print(f"üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("   ‚Ä¢ –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU: nvidia-smi")
        print("   ‚Ä¢ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã: docker ps")
        print("   ‚Ä¢ –õ–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤: docker logs <container_name>")

if __name__ == "__main__":
    main()