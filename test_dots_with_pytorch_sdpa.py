#!/usr/bin/env python3
"""
–¢–µ—Å—Ç dots.ocr —Å PyTorch SDPA Flash Attention
"""

import os
import sys
import time
import torch
from pathlib import Path
from PIL import Image

# Add project root to path
sys.path.append(str(Path(__file__).parent))

# Set environment variable
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from models.model_loader import ModelLoader
from utils.logger import logger


def test_dots_with_pytorch_sdpa():
    """–¢–µ—Å—Ç dots.ocr —Å PyTorch SDPA."""
    
    print("‚ö° –¢–ï–°–¢ DOTS.OCR –° PYTORCH SDPA FLASH ATTENTION")
    print("=" * 60)
    
    try:
        # Load model with new SDPA implementation
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ dots.ocr —Å PyTorch SDPA...")
        
        start_time = time.time()
        model_wrapper = ModelLoader.load_model('dots_ocr')
        load_time = time.time() - start_time
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        
        # Check what attention implementation is being used
        model = model_wrapper.model
        if hasattr(model.config, 'attn_implementation'):
            attn_impl = model.config.attn_implementation
            print(f"üîß Attention implementation: {attn_impl}")
            
            if attn_impl == "sdpa":
                print("üöÄ –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø PYTORCH SDPA - –û–ü–¢–ò–ú–ê–õ–¨–ù–ê–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨!")
            elif attn_impl == "flash_attention_2":
                print("‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–Ω–µ—à–Ω–∏–π Flash Attention")
            else:
                print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è eager attention (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
        
        # Test with document
        image_path = "test_document.png"
        if not Path(image_path).exists():
            print(f"‚ùå –§–∞–π–ª {image_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        image = Image.open(image_path)
        print(f"üì∑ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image.size}")
        
        # Test OCR with timing
        print("\nüî§ –¢–µ—Å—Ç OCR —Å –Ω–æ–≤–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π...")
        
        ocr_start = time.time()
        result = model_wrapper.extract_text_only(image)
        ocr_time = time.time() - ocr_start
        
        print(f"‚úÖ OCR –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {ocr_time:.2f}s")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(str(result))} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if result and len(str(result)) > 10:
            print("‚úÖ OCR —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω–æ–≤–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π!")
        
        # Test layout analysis
        print("\nüìã –¢–µ—Å—Ç Layout –∞–Ω–∞–ª–∏–∑–∞...")
        
        layout_start = time.time()
        layout_result = model_wrapper.parse_document(image, return_json=True)
        layout_time = time.time() - layout_start
        
        print(f"‚úÖ Layout –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {layout_time:.2f}s")
        
        if isinstance(layout_result, dict):
            if 'raw_text' in layout_result:
                raw_text = layout_result['raw_text']
                if raw_text and len(raw_text) > 50:
                    print(f"üìä –ü–æ–ª—É—á–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(raw_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    # Try to parse as JSON
                    try:
                        import json
                        parsed = json.loads(raw_text)
                        if isinstance(parsed, list):
                            print(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã–π JSON: {len(parsed)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                            print("üéØ PYTORCH SDPA FLASH ATTENTION –†–ê–ë–û–¢–ê–ï–¢ –û–¢–õ–ò–ß–ù–û!")
                        else:
                            print(f"‚úÖ JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {type(parsed)}")
                    except json.JSONDecodeError:
                        print("‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ JSON, –Ω–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ")
        
        # Performance summary
        total_time = time.time() - start_time
        print(f"\nüìä –ò–¢–û–ì–ò –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {load_time:.2f}s")
        print(f"   OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞: {ocr_time:.2f}s")
        print(f"   Layout –∞–Ω–∞–ª–∏–∑: {layout_time:.2f}s")
        print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}s")
        
        # Check GPU memory usage
        if torch.cuda.is_available():
            memory_used = torch.cuda.max_memory_allocated() / 1024**3
            print(f"   –ü–∞–º—è—Ç—å GPU: {memory_used:.2f}GB")
        
        # Unload model
        ModelLoader.unload_model('dots_ocr')
        
        print(f"\nüéâ PYTORCH SDPA FLASH ATTENTION –£–°–ü–ï–®–ù–û –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù!")
        print(f"‚úÖ dots.ocr —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")


if __name__ == "__main__":
    test_dots_with_pytorch_sdpa()