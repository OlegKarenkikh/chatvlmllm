{
  "timestamp": "2026-01-24 18:53:20",
  "models": {
    "rednote-hilab/dots.ocr": {
      "config": {
        "model_name": "rednote-hilab/dots.ocr",
        "container_name": "dots-ocr-fixed",
        "port": 8000,
        "size_gb": 5.67,
        "category": "ocr",
        "vllm_params": {
          "max_model_len": 1024,
          "gpu_memory_utilization": 0.85,
          "trust_remote_code": true,
          "enforce_eager": true
        },
        "issues": [],
        "priority": 1,
        "status": "tested_working"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": "No logs available"
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "deepseek-ai/deepseek-ocr": {
      "config": {
        "model_name": "deepseek-ai/deepseek-ocr",
        "container_name": "deepseek-ocr-vllm",
        "port": 8001,
        "size_gb": 0.01,
        "category": "ocr",
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": true
        },
        "issues": [
          "Very small size - may be incomplete"
        ],
        "priority": 2,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": "No logs available"
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "stepfun-ai/GOT-OCR-2.0-hf": {
      "config": {
        "model_name": "stepfun-ai/GOT-OCR-2.0-hf",
        "container_name": "got-ocr-2-0-hf-vllm",
        "port": 8002,
        "size_gb": 1.06,
        "category": "ocr",
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.7,
          "trust_remote_code": true,
          "enforce_eager": true
        },
        "issues": [
          "May require additional dependencies"
        ],
        "priority": 2,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": ""
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "stepfun-ai/GOT-OCR2_0": {
      "status": "skipped_incompatible",
      "reason": "Known incompatible model"
    },
    "ucaslcl/GOT-OCR2_0": {
      "status": "skipped_incompatible",
      "reason": "Known incompatible model"
    },
    "datalab-to/chandra": {
      "config": {
        "model_name": "datalab-to/chandra",
        "container_name": "datalab-chandra-vllm",
        "port": 8015,
        "size_gb": 0.42,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": false
        },
        "issues": [],
        "priority": 3,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": ""
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "Qwen/Qwen2.5-VL-7B-Instruct": {
      "config": {
        "model_name": "Qwen/Qwen2.5-VL-7B-Instruct",
        "container_name": "qwen2-5-vl-7b-instruct-vllm",
        "port": 8012,
        "size_gb": 0.66,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 4096,
          "gpu_memory_utilization": 0.7,
          "trust_remote_code": true,
          "enforce_eager": false
        },
        "issues": [
          "Small size - may be incomplete"
        ],
        "priority": 3,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": ""
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "Qwen/Qwen3-VL-2B-Instruct": {
      "config": {
        "model_name": "Qwen/Qwen3-VL-2B-Instruct",
        "container_name": "qwen3-vl-2b-instruct-vllm",
        "port": 8010,
        "size_gb": 3.97,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.7,
          "trust_remote_code": true,
          "enforce_eager": false
        },
        "issues": [],
        "priority": 3,
        "status": "tested_working"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": "\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:10:20 [gpu_model_runner.py:3808] Starting to load model Qwen/Qwen3-VL-2B-Instruct...\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:10:20 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:10:33 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:10:34 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:11:45 [default_loader.py:291] Loading weights took 71.35 seconds\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:11:46 [gpu_model_runner.py:3905] Model loading took 4.24 GiB memory and 85.302389 seconds\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:11:46 [gpu_model_runner.py:4715] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:12:17 [backends.py:644] Using cache directory: /root/.cache/vllm/torch_compile_cache/dd641f0e56/rank_0_0/backbone for vLLM's torch.compile\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 08:12:17 [backends.py:704] Dynamo bytecode transform time: 5.72 s"
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "Qwen/Qwen2-VL-2B-Instruct": {
      "config": {
        "model_name": "Qwen/Qwen2-VL-2B-Instruct",
        "container_name": "qwen2-vl-2b-instruct-vllm",
        "port": 8011,
        "size_gb": 4.13,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 4096,
          "gpu_memory_utilization": 0.7,
          "trust_remote_code": true,
          "enforce_eager": false
        },
        "issues": [],
        "priority": 3,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": "No logs available"
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "Qwen/Qwen2-VL-7B-Instruct": {
      "config": {
        "model_name": "Qwen/Qwen2-VL-7B-Instruct",
        "container_name": "qwen2-vl-7b-instruct-vllm",
        "port": 8013,
        "size_gb": 7.61,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 4096,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": false
        },
        "issues": [
          "Large model - high memory usage"
        ],
        "priority": 4,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": "No logs available"
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "microsoft/Phi-3.5-vision-instruct": {
      "config": {
        "model_name": "microsoft/Phi-3.5-vision-instruct",
        "container_name": "phi-3-5-vision-instruct-vllm",
        "port": 8014,
        "size_gb": 7.73,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 4096,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": false
        },
        "issues": [
          "Large model - high memory usage"
        ],
        "priority": 4,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": "\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:19:59 [gpu_model_runner.py:3808] Starting to load model microsoft/Phi-3.5-vision-instruct...\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:20:00 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:20:00 [vllm.py:630] Asynchronous scheduling is enabled.\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:20:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'TRITON_ATTN', 'FLEX_ATTENTION')"
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "deepseek-ai/deepseek-vl-1.3b-chat": {
      "status": "skipped_broken",
      "reason": "Likely broken model (zero size)"
    },
    "h2oai/h2ovl-mississippi-800m": {
      "config": {
        "model_name": "h2oai/h2ovl-mississippi-800m",
        "container_name": "h2ovl-mississippi-800m-vllm",
        "port": 8022,
        "size_gb": 1.54,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.5,
          "trust_remote_code": true,
          "enforce_eager": true
        },
        "issues": [
          "Custom architecture - may not be supported"
        ],
        "priority": 5,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": ""
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "vikhyatk/moondream2": {
      "config": {
        "model_name": "vikhyatk/moondream2",
        "container_name": "moondream2-vllm",
        "port": 8023,
        "size_gb": 3.59,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": true
        },
        "issues": [
          "Custom architecture - may not be supported"
        ],
        "priority": 5,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": ""
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "h2oai/h2ovl-mississippi-2b": {
      "config": {
        "model_name": "h2oai/h2ovl-mississippi-2b",
        "container_name": "h2ovl-mississippi-2b-vllm",
        "port": 8021,
        "size_gb": 4.01,
        "category": "vlm",
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": true
        },
        "issues": [
          "Custom architecture - may not be supported"
        ],
        "priority": 5,
        "status": "needs_testing"
      },
      "launch_result": {
        "launch_success": true,
        "launch_time": 130.46803975105286,
        "gpu_memory_after": {
          "total_mb": 12227,
          "used_mb": 11933,
          "free_mb": 12,
          "usage_percent": 97.6
        }
      },
      "functionality_result": {
        "text_test_success": false,
        "text_error": "HTTPConnectionPool(host='localhost', port=8021): Read timed out. (read timeout=30)"
      },
      "status": "launch_ok_function_fail"
    }
  },
  "summary": {
    "total_tested": 12,
    "successful": 0,
    "failed": 12,
    "incompatible": 3,
    "removed_from_cache": 3
  }
}