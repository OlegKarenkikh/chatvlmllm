#!/usr/bin/env python3
"""
–û—Ç–ª–∞–¥–∫–∞ –≤—ã–≤–æ–¥–∞ dots.ocr
"""

import os
import sys
import time
import torch
from pathlib import Path
from PIL import Image

# Add project root to path
sys.path.append(str(Path(__file__).parent))

# Set environment variable
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from models.model_loader import ModelLoader
from utils.logger import logger


def debug_dots_output():
    """–û—Ç–ª–∞–¥–∫–∞ –≤—ã–≤–æ–¥–∞ dots.ocr."""
    
    print("üîç –û–¢–õ–ê–î–ö–ê –í–´–í–û–î–ê DOTS.OCR")
    print("=" * 40)
    
    try:
        # Load model
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        model_wrapper = ModelLoader.load_model('dots_ocr')
        
        # Get the actual model and processor
        model = model_wrapper.model
        processor = model_wrapper.processor
        
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # Test with simple image
        image_path = "test_document.png"
        if not Path(image_path).exists():
            print(f"‚ùå –§–∞–π–ª {image_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        image = Image.open(image_path)
        print(f"üì∑ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image.size}, —Ä–µ–∂–∏–º: {image.mode}")
        
        # Convert to RGB if needed
        if image.mode != 'RGB':
            image = image.convert('RGB')
            print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ RGB")
        
        # Test simple prompt
        simple_prompt = "Extract all text from this image."
        print(f"\nüìù –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç: {simple_prompt}")
        
        # Manual inference like in Modal
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": simple_prompt}
                ]
            }
        ]
        
        print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ inference...")
        
        # Preparation for inference
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        print(f"üìÑ Chat template: {text[:200]}...")
        
        # Process vision info
        from qwen_vl_utils import process_vision_info
        image_inputs, video_inputs = process_vision_info(messages)
        
        print(f"üñºÔ∏è Image inputs: {len(image_inputs) if image_inputs else 0}")
        print(f"üé• Video inputs: {len(video_inputs) if video_inputs else 0}")
        
        inputs = processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        
        inputs = inputs.to("cuda")
        print("‚úÖ Inputs –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –∏ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –Ω–∞ GPU")
        
        # Generate with detailed settings
        print("üöÄ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs, 
                max_new_tokens=1000,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                do_sample=False,
                temperature=1.0,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        # Decode output
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        (output_text,) = processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        print(f"\nüì§ RAW OUTPUT:")
        print(f"–î–ª–∏–Ω–∞: {len(output_text)}")
        print(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ: '{output_text}'")
        print(f"Repr: {repr(output_text)}")
        
        if output_text.strip():
            print("‚úÖ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç!")
            
            # Try to parse as JSON
            try:
                import json
                parsed = json.loads(output_text)
                print(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã–π JSON: {type(parsed)}")
                if isinstance(parsed, list):
                    print(f"üìä –≠–ª–µ–º–µ–Ω—Ç–æ–≤: {len(parsed)}")
                elif isinstance(parsed, dict):
                    print(f"üìä –ö–ª—é—á–∏: {list(parsed.keys())}")
            except json.JSONDecodeError as e:
                print(f"‚ùå –ù–µ JSON: {e}")
                print("üí° –í–æ–∑–º–æ–∂–Ω–æ, –Ω—É–∂–µ–Ω –¥—Ä—É–≥–æ–π –ø—Ä–æ–º–ø—Ç")
        else:
            print("‚ùå –ü—É—Å—Ç–æ–π –≤—ã–≤–æ–¥!")
            print("üí° –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º")
        
        # Unload model
        ModelLoader.unload_model('dots_ocr')
        print("\n‚úÖ –û—Ç–ª–∞–¥–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")


if __name__ == "__main__":
    debug_dots_output()