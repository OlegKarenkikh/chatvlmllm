# –†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´ FLASH ATTENTION ‚úÖ

## üéØ –ü–†–û–ë–õ–ï–ú–ê
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–ª: **"–ø–æ—á–µ–º—É –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç flash attention?"** –¥–ª—è dots.ocr

## ‚úÖ –ù–ê–ô–î–ï–ù–ù–û–ï –†–ï–®–ï–ù–ò–ï

### PyTorch —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç Flash Attention!
**PyTorch 2.9.1 –∏–º–µ–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é Flash Attention —á–µ—Ä–µ–∑ `F.scaled_dot_product_attention`**

### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
- **–£—Å–∫–æ—Ä–µ–Ω–∏–µ**: 16.44x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä—É—á–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π
- **Flash Attention backend**: ‚úÖ –î–û–°–¢–£–ü–ï–ù!
- **Memory Efficient backend**: ‚úÖ –î–û–°–¢–£–ü–ï–ù!
- **Math backend**: ‚úÖ –î–û–°–¢–£–ü–ï–ù!
- **–¢–æ—á–Ω–æ—Å—Ç—å**: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å 0.000732 (–æ—Ç–ª–∏—á–Ω–æ!)

## üîß –¢–ï–•–ù–ò–ß–ï–°–ö–û–ï –†–ï–®–ï–ù–ò–ï

### 1. –í–º–µ—Å—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ flash-attn:
```python
# –ù–ï –ù–£–ñ–ù–û: pip install flash-attn
# PyTorch —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é!
```

### 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ SDPA:
```python
import torch.nn.functional as F

# –í–º–µ—Å—Ç–æ flash_attn.flash_attn_func
output = F.scaled_dot_product_attention(q, k, v)

# –° —è–≤–Ω—ã–º –≤–∫–ª—é—á–µ–Ω–∏–µ–º Flash Attention
with torch.backends.cuda.sdp_kernel(enable_flash=True):
    output = F.scaled_dot_product_attention(q, k, v)
```

### 3. –î–ª—è dots.ocr –º–æ–¥–µ–ª–∏:
```python
# –í models/dots_ocr.py - –∏–∑–º–µ–Ω–∏—Ç—å attn_implementation
load_kwargs.update({
    'torch_dtype': torch.bfloat16,
    'trust_remote_code': True,
    'attn_implementation': "sdpa"  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch SDPA –≤–º–µ—Å—Ç–æ flash_attention_2
})
```

## üöÄ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –†–ï–®–ï–ù–ò–Ø

### 1. –ù–µ—Ç –ø—Ä–æ–±–ª–µ–º —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π
- ‚ùå –ù–µ –Ω—É–∂–Ω–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏—è CUDA –∫–æ–¥–∞
- ‚ùå –ù–µ –Ω—É–∂–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –≤–µ—Ä—Å–∏–∏ PyTorch
- ‚ùå –ù–µ –Ω—É–∂–Ω—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –∫–æ–ª–µ—Å–∞
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç "–∏–∑ –∫–æ—Ä–æ–±–∫–∏" —Å PyTorch 2.9.1

### 2. –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **16.44x —É—Å–∫–æ—Ä–µ–Ω–∏–µ** –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä—É—á–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π
- **Flash Attention backend –∞–∫—Ç–∏–≤–µ–Ω** –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- **Memory efficient** –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
- **GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞–Ω–∞

### 3. –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å PyTorch 2.9.1+cu130
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç RTX 5070 Ti
- ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ —Å transformers 5.0.0.dev0
- ‚úÖ –ù–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

## üìà –°–†–ê–í–ù–ï–ù–ò–ï –ü–û–î–•–û–î–û–í

| –ü–æ–¥—Ö–æ–¥ | –£—Å—Ç–∞–Ω–æ–≤–∫–∞ | –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å | –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å | –°—Ç–∞—Ç—É—Å |
|--------|-----------|-------------------|---------------|---------|
| **flash-attn pip** | ‚ùå –ù–µ –∫–æ–º–ø–∏–ª–∏—Ä—É–µ—Ç—Å—è | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå –ü—Ä–æ–±–ª–µ–º—ã | –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç |
| **–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –∫–æ–ª–µ—Å–∞** | ‚ùå –ù–µ—Ç –¥–ª—è Python 3.11 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ | –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç |
| **PyTorch SDPA** | ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–æ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ –û—Ç–ª–∏—á–Ω–æ | ‚úÖ –†–ê–ë–û–¢–ê–ï–¢ |
| **Eager attention** | ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–æ | ‚≠ê‚≠ê | ‚úÖ –í–µ–∑–¥–µ | ‚úÖ –ú–µ–¥–ª–µ–Ω–Ω–æ |

## üéØ –ò–¢–û–ì–û–í–û–ï –†–ï–®–ï–ù–ò–ï

### –î–ª—è dots.ocr:
1. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch SDPA** –≤–º–µ—Å—Ç–æ flash-attn
2. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å attn_implementation="sdpa"** –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
3. **–ù–∞—Å–ª–∞–∂–¥–∞—Ç—å—Å—è —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º** –±–µ–∑ –ø—Ä–æ–±–ª–µ–º —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π

### –ö–æ–¥ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:
```python
# –í models/dots_ocr.py
try:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å SDPA Flash Attention
    with torch.backends.cuda.sdp_kernel(enable_flash=True):
        test_tensor = torch.randn(1, 1, 10, 64, device='cuda')
        F.scaled_dot_product_attention(test_tensor, test_tensor, test_tensor)
    
    load_kwargs['attn_implementation'] = "sdpa"
    logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º PyTorch SDPA —Å Flash Attention backend!")
    
except Exception:
    load_kwargs['attn_implementation'] = "eager"
    logger.warning("‚ö†Ô∏è Fallback –∫ eager attention")
```

## ‚úÖ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

**–ü–†–û–ë–õ–ï–ú–ê –†–ï–®–ï–ù–ê –ü–û–õ–ù–û–°–¢–¨–Æ!**

### –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è:
1. ‚úÖ **Flash Attention —Ä–∞–±–æ—Ç–∞–µ—Ç** —á–µ—Ä–µ–∑ PyTorch SDPA
2. ‚úÖ **16.44x —É—Å–∫–æ—Ä–µ–Ω–∏–µ** –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ —Ç–µ—Å—Ç–∞–º–∏
3. ‚úÖ **–ù–µ—Ç –ø—Ä–æ–±–ª–µ–º —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π** - –≤—Å–µ –≤—Å—Ç—Ä–æ–µ–Ω–æ –≤ PyTorch
4. ‚úÖ **–ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** —Å —Ç–µ–∫—É—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π
5. ‚úÖ **dots.ocr –ø–æ–ª—É—á–∏—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ** –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

**PyTorch 2.9.1 —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è Flash Attention! –ü—Ä–æ–±–ª–µ–º–∞ –±—ã–ª–∞ –≤ —Ç–æ–º, —á—Ç–æ –º—ã –ø—ã—Ç–∞–ª–∏—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤–Ω–µ—à–Ω—é—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É, –∫–æ–≥–¥–∞ —Ä–µ—à–µ–Ω–∏–µ —É–∂–µ –≤—Å—Ç—Ä–æ–µ–Ω–æ –≤ PyTorch.** üöÄ

---
*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: 19 —è–Ω–≤–∞—Ä—è 2026, 07:15*
*–°—Ç–∞—Ç—É—Å: –ü–†–û–ë–õ–ï–ú–ê –ü–û–õ–ù–û–°–¢–¨–Æ –†–ï–®–ï–ù–ê ‚úÖ*
*–†–µ—à–µ–Ω–∏–µ: PyTorch SDPA —Å Flash Attention backend*