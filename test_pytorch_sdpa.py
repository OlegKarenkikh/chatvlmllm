#!/usr/bin/env python3
"""
–¢–µ—Å—Ç PyTorch Scaled Dot Product Attention –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Flash Attention
"""

import torch
import torch.nn.functional as F
import time


def test_pytorch_sdpa():
    """–¢–µ—Å—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ SDPA –≤ PyTorch."""
    
    print("üî¨ –¢–ï–°–¢ PYTORCH SCALED DOT PRODUCT ATTENTION")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return
    
    device = torch.device("cuda")
    print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # Test parameters
    batch_size = 2
    seq_len = 512
    num_heads = 8
    head_dim = 64
    
    print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ—Å—Ç–∞:")
    print(f"   Batch size: {batch_size}")
    print(f"   Sequence length: {seq_len}")
    print(f"   Number of heads: {num_heads}")
    print(f"   Head dimension: {head_dim}")
    
    # Create test tensors
    q = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)
    k = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)
    v = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)
    
    print(f"‚úÖ –¢–µ–Ω–∑–æ—Ä—ã —Å–æ–∑–¥–∞–Ω—ã: {q.shape}")
    
    # Test 1: Standard attention
    print("\nüî§ –¢–µ—Å—Ç 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ")
    start_time = time.time()
    
    with torch.no_grad():
        # Manual attention calculation
        scale = 1.0 / (head_dim ** 0.5)
        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * scale
        attn_weights = F.softmax(attn_weights, dim=-1)
        output_manual = torch.matmul(attn_weights, v)
    
    manual_time = time.time() - start_time
    print(f"‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: {manual_time:.4f}s")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {output_manual.shape}")
    
    # Test 2: PyTorch SDPA
    print("\n‚ö° –¢–µ—Å—Ç 2: PyTorch SDPA (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ)")
    start_time = time.time()
    
    with torch.no_grad():
        # PyTorch optimized attention
        output_sdpa = F.scaled_dot_product_attention(q, k, v)
    
    sdpa_time = time.time() - start_time
    print(f"‚úÖ PyTorch SDPA: {sdpa_time:.4f}s")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {output_sdpa.shape}")
    
    # Compare results
    diff = torch.abs(output_manual - output_sdpa).max().item()
    print(f"\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å: {diff:.6f}")
    print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {manual_time/sdpa_time:.2f}x")
    
    if diff < 1e-3:
        print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç!")
    else:
        print("‚ö†Ô∏è –ï—Å—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")
    
    # Test 3: Check available backends
    print(f"\nüîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –±—ç–∫–µ–Ω–¥—ã SDPA:")
    
    try:
        # Check what backends are available
        with torch.backends.cuda.sdp_kernel():
            print("   ‚úÖ CUDA kernel –¥–æ—Å—Ç—É–ø–µ–Ω")
    except:
        print("   ‚ùå CUDA kernel –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    try:
        # Test with different settings
        with torch.backends.cuda.sdp_kernel(enable_flash=True):
            output_flash = F.scaled_dot_product_attention(q, k, v)
            print("   ‚úÖ Flash Attention backend –¥–æ—Å—Ç—É–ø–µ–Ω!")
    except:
        print("   ‚ö†Ô∏è Flash Attention backend –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback")
    
    try:
        with torch.backends.cuda.sdp_kernel(enable_math=True):
            output_math = F.scaled_dot_product_attention(q, k, v)
            print("   ‚úÖ Math backend –¥–æ—Å—Ç—É–ø–µ–Ω")
    except:
        print("   ‚ùå Math backend –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    try:
        with torch.backends.cuda.sdp_kernel(enable_mem_efficient=True):
            output_mem = F.scaled_dot_product_attention(q, k, v)
            print("   ‚úÖ Memory Efficient backend –¥–æ—Å—Ç—É–ø–µ–Ω")
    except:
        print("   ‚ùå Memory Efficient backend –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    print(f"\nüí° –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:")
    print(f"   PyTorch SDPA —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –∑–∞–º–µ–Ω–∞ Flash Attention!")
    print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {manual_time/sdpa_time:.2f}x –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä—É—á–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π")
    print(f"   –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–º–µ—Å—Ç–æ flash-attn –¥–ª—è dots.ocr")


if __name__ == "__main__":
    test_pytorch_sdpa()