#!/usr/bin/env python3
"""
–¢–µ—Å—Ç dots.ocr —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

import torch
from transformers import AutoModelForCausalLM, AutoProcessor
from PIL import Image
import time

def test_official_dots_approach():
    """–¢–µ—Å—Ç —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º dots.ocr"""
    print("üß™ –¢–ï–°–¢ –û–§–ò–¶–ò–ê–õ–¨–ù–û–ì–û –ü–û–î–•–û–î–ê DOTS.OCR")
    print("=" * 50)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
    print(f"üñ•Ô∏è GPU: {torch.cuda.get_device_name(0)}")
    print(f"üêç PyTorch: {torch.__version__}")
    print(f"‚ö° CUDA: {torch.version.cuda}")
    print()
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±)
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º dots.ocr (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±)...")
        start_time = time.time()
        
        model = AutoModelForCausalLM.from_pretrained(
            "rednote-hilab/dots.ocr",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        processor = AutoProcessor.from_pretrained(
            "rednote-hilab/dots.ocr",
            trust_remote_code=True
        )
        
        load_time = time.time() - start_time
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        print()
        
        # –¢–µ—Å—Ç —Å –ø—Ä–æ—Å—Ç—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –ø—Ä–æ—Å—Ç—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º...")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º
        image = Image.new('RGB', (400, 100), color='white')
        from PIL import ImageDraw, ImageFont
        draw = ImageDraw.Draw(image)
        
        try:
            font = ImageFont.truetype("C:/Windows/Fonts/arial.ttf", 24)
        except:
            font = ImageFont.load_default()
        
        draw.text((50, 30), "HELLO WORLD", fill='black', font=font)
        image.save('test_simple_hello.png')
        
        # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –æ–±—Ä–∞–±–æ—Ç–∫–∏
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": "Extract all text from this image"}
                ]
            }
        ]
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ chat template
        text = processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        print(f"üìù Chat template: {text[:200]}...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        inputs = processor(
            text=[text],
            images=[image],
            padding=True,
            return_tensors="pt"
        ).to("cuda")
        
        print(f"üîß Input shape: {inputs.input_ids.shape}")
        print(f"üñºÔ∏è Image tensor shape: {inputs.pixel_values.shape if hasattr(inputs, 'pixel_values') else 'No pixel_values'}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
        print("üöÄ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç...")
        start_gen = time.time()
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        gen_time = time.time() - start_gen
        print(f"‚è±Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–Ω—è–ª–∞: {gen_time:.3f}s")
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        print(f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç: '{output_text}'")
        print(f"üìè –î–ª–∏–Ω–∞: {len(output_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if output_text.strip():
            print("‚úÖ –£–°–ü–ï–•! dots.ocr —Ä–∞–±–æ—Ç–∞–µ—Ç")
            return True
        else:
            print("‚ùå –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_different_prompts():
    """–¢–µ—Å—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏"""
    print("\nüß™ –¢–ï–°–¢ –†–ê–ó–ù–´–• –ü–†–û–ú–ü–¢–û–í")
    print("=" * 30)
    
    try:
        model = AutoModelForCausalLM.from_pretrained(
            "rednote-hilab/dots.ocr",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        processor = AutoProcessor.from_pretrained(
            "rednote-hilab/dots.ocr",
            trust_remote_code=True
        )
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = Image.open('test_simple_hello.png')
        
        # –†–∞–∑–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
        prompts = [
            "OCR",
            "Read text",
            "What text is in this image?",
            "Extract all text",
            "Transcribe the text in the image"
        ]
        
        for prompt in prompts:
            print(f"\nüîç –ü—Ä–æ–º–ø—Ç: '{prompt}'")
            
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }]
            
            text = processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = processor(
                text=[text],
                images=[image],
                padding=True,
                return_tensors="pt"
            ).to("cuda")
            
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: '{output_text.strip()}'")
            
            if "HELLO" in output_text.upper():
                print("‚úÖ –¢–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω!")
            else:
                print("‚ùå –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ –ø—Ä–æ–º–ø—Ç–æ–≤: {e}")

if __name__ == "__main__":
    success = test_official_dots_approach()
    
    if success:
        test_different_prompts()
        print("\nüéâ DOTS.OCR –†–ê–ë–û–¢–ê–ï–¢ –° –û–§–ò–¶–ò–ê–õ–¨–ù–´–ú –ü–û–î–•–û–î–û–ú!")
    else:
        print("\n‚ùå DOTS.OCR –ù–ï –†–ê–ë–û–¢–ê–ï–¢ –î–ê–ñ–ï –° –û–§–ò–¶–ò–ê–õ–¨–ù–´–ú –ü–û–î–•–û–î–û–ú")
        print("üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("   - –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π PyTorch/transformers")
        print("   - –ü—Ä–æ–±–ª–µ–º—ã —Å CUDA/flash-attention")
        print("   - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∏–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä")