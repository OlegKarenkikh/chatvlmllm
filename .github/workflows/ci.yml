name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libmagic1
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov flake8 black isort
        pip install torch --index-url https://download.pytorch.org/whl/cpu
        pip install pillow pyyaml numpy opencv-python-headless python-magic
    
    - name: Lint with flake8
      run: |
        # Остановка при синтаксических ошибках
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # Предупреждения (не останавливают сборку)
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics
    
    - name: Check formatting with black
      run: |
        black --check --diff . || true
    
    - name: Check imports with isort
      run: |
        isort --check-only --diff . || true
    
    - name: Run tests
      run: |
        pytest tests/ -v --tb=short
    
    - name: Run tests with coverage
      run: |
        pytest tests/ --cov=. --cov-report=xml --cov-report=term-missing
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

  docker:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Build Docker image
      run: |
        docker build -t chatvlmllm:latest .
    
    - name: Test Docker image
      run: |
        docker run --rm chatvlmllm:latest python -c "from models import ModelLoader; print('OK')"
