# ChatVLMLLM - Document OCR & Vision-Language Models

A comprehensive toolkit for document OCR, visual understanding, and multimodal AI applications using state-of-the-art vision-language models.

## âœ¨ Features

### Supported Models

- ğŸ” **GOT-OCR 2.0** - Specialized OCR for complex layouts
- ğŸ¤– **Qwen2-VL** (2B, 7B) - Advanced vision-language understanding
- â­ **Qwen3-VL** (2B, 4B, 8B) - Latest VLM with 32 languages OCR, visual agent, 256K context
- ğŸ“š **dots.ocr** - SOTA multilingual document parser (100+ languages)

### Key Capabilities

- ğŸŒ **Multilingual OCR** - 32+ languages with high accuracy
- ğŸ¤– **Visual Agent** - GUI interaction and automation (Qwen3-VL)
- ğŸ“Š **Document Analysis** - Layout detection, table extraction, structure parsing
- ğŸ§  **Visual Reasoning** - Complex reasoning about images and diagrams
- ğŸ¥ **Video Understanding** - 256K context for long videos (Qwen3-VL)
- ğŸ“¦ **Flexible Quantization** - FP16, INT8, INT4 support
- âš¡ **Flash Attention 2** - Faster inference with lower memory
- ğŸš€ **REST API** - Production-ready FastAPI endpoints
- ğŸ³ **Docker Support** - GPU-enabled containerization

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/OlegKarenkikh/chatvlmllm.git
cd chatvlmllm

# Install dependencies
pip install -r requirements.txt

# Install latest transformers for Qwen3-VL
pip install git+https://github.com/huggingface/transformers
```

### Check GPU Compatibility

```bash
python scripts/check_gpu.py
```

### Basic Usage

#### Python API

```python
from models import ModelLoader
from PIL import Image

# Load Qwen3-VL 2B
model = ModelLoader.load_model('qwen3_vl_2b')

# Process image
image = Image.open('document.jpg')
result = model.extract_text(image)

print(result)
```

#### Streamlit App

```bash
streamlit run app.py
```

Access: http://localhost:8501

#### REST API

```bash
# Start API server
uvicorn api:app --host 0.0.0.0 --port 8000 --reload

# Use API
curl -X POST "http://localhost:8000/ocr?model=qwen3_vl_2b" \
  -F "file=@document.jpg"
```

API Docs: http://localhost:8000/docs

#### Docker

```bash
# Build and run
docker-compose up -d

# Access services
# Streamlit: http://localhost:8501
# API: http://localhost:8000/docs
```

## ğŸ“Š GPU Requirements

| GPU | VRAM | Best Model | Status |
|-----|------|-----------|--------|
| RTX 5090 | 32GB | Qwen3-VL 8B@FP16 | âœ… Perfect |
| RTX 5080 | 16GB | Qwen3-VL 8B@INT8 | âœ… Excellent |
| RTX 5070 | 12GB | Qwen3-VL 4B@FP16 | âœ… Good |
| RTX 5060 Ti | 16GB | Qwen3-VL 8B@INT8 | âœ… Best Value |
| RTX 5060 Ti | 8GB | Qwen3-VL 4B@INT4 | âš ï¸ Limited |

See [GPU Requirements Guide](docs/gpu_requirements.md) for detailed compatibility.

## ğŸ“– Documentation

- [GPU Requirements](docs/gpu_requirements.md) - Comprehensive GPU compatibility guide
- [Qwen3-VL Guide](docs/qwen3_vl_guide.md) - Qwen3-VL usage and optimization
- [API Guide](docs/api_guide.md) - REST API documentation
- [Model Cache Guide](docs/model_cache_guide.md) - Managing model downloads

## ğŸ”§ API Usage

### Python Client

```python
import requests

# OCR
with open('document.jpg', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/ocr',
        files={'file': f},
        params={'model': 'qwen3_vl_2b'}
    )
    print(response.json()['text'])

# Chat
with open('image.jpg', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/chat',
        files={'file': f},
        data={'prompt': 'What\'s in this image?'}
    )
    print(response.json()['response'])
```

See [examples/api_usage.py](examples/api_usage.py) for more examples.

### cURL

```bash
# Health check
curl http://localhost:8000/health

# OCR
curl -X POST "http://localhost:8000/ocr" \
  -F "file=@document.jpg" \
  -F "model=qwen3_vl_2b"

# Chat
curl -X POST "http://localhost:8000/chat" \
  -F "file=@image.jpg" \
  -F "prompt=Describe this image"
```

See [examples/api_curl.sh](examples/api_curl.sh) for more examples.

## ğŸ³ Docker Deployment

### Using docker-compose

```bash
# Start all services
docker-compose up -d

# Check logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Services

- **API**: http://localhost:8000
  - Swagger UI: http://localhost:8000/docs
  - ReDoc: http://localhost:8000/redoc
- **Streamlit**: http://localhost:8501

### Requirements

- Docker 20.10+
- NVIDIA Docker runtime
- 16GB+ VRAM recommended

## ğŸ› ï¸ Configuration

### config.yaml

```yaml
models:
  qwen3_vl_8b:
    model_path: "Qwen/Qwen3-VL-8B-Instruct"
    precision: "int8"  # fp16, bf16, int8, int4
    use_flash_attention: true
    device_map: "auto"
```

### INT4 Quantization (66% VRAM Reduction)

```yaml
models:
  qwen3_vl_8b:
    precision: "int4"  # 17.6GB -> 6GB
```

## âœ¨ What's New

### v1.0.0 (2026-01-15)

#### ğŸ‰ Major Features

- âœ… **Qwen3-VL Support** - All three models (2B, 4B, 8B)
- âœ… **REST API** - Production-ready FastAPI
- âœ… **Docker** - Full containerization with GPU support
- âœ… **Streamlit App** - Updated with all models
- âœ… **Documentation** - Complete API and usage guides

#### ğŸ”¥ Qwen3-VL Highlights

- ğŸŒ **32 languages OCR** (vs 19 in Qwen2-VL)
- ğŸ¤– **Visual agent** capabilities
- ğŸ“š **256K context** (expandable to 1M)
- ğŸ¯ **3D grounding** for spatial reasoning
- ğŸ§  **Thinking mode** for complex tasks
- ğŸ“¦ **INT4 support** - 66% less VRAM

## ğŸ’» Usage Examples

### Document OCR

```python
# Extract text from document
text = model.extract_text(image, language="Russian")
```

### Document Analysis

```python
# Analyze document structure
analysis = model.analyze_document(image, focus="layout")
```

### Visual Reasoning

```python
# Complex reasoning
reasoning = model.visual_reasoning(
    image, 
    question="Explain the workflow in this diagram"
)
```

### Visual Agent (Qwen3-VL)

```python
# GUI interaction
actions = model.chat(
    image=screenshot,
    prompt="Find and click the Submit button"
)
```

## ğŸ’¡ Tips & Best Practices

### For 8GB VRAM

```python
# Use INT4 quantization
model = ModelLoader.load_model(
    'qwen3_vl_8b',
    precision='int4'  # 6GB instead of 17.6GB
)
```

### For 12GB VRAM

```python
# Run multiple models
qwen4b = ModelLoader.load_model('qwen3_vl_4b')  # 8.9GB
qwen2b = ModelLoader.load_model('qwen3_vl_2b')  # 4.4GB
# Total: ~11GB with INT8
```

### For 16GB+ VRAM

```python
# Optimal quality
model = ModelLoader.load_model(
    'qwen3_vl_8b',
    precision='int8',  # 10GB
    use_flash_attention=True
)
```

## ğŸ”§ Development

### Project Structure

```
chatvlmllm/
â”œâ”€â”€ api.py                  # FastAPI REST API
â”œâ”€â”€ app.py                  # Streamlit application
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ got_ocr.py          # GOT-OCR integration
â”‚   â”œâ”€â”€ qwen_vl.py          # Qwen2-VL integration
â”‚   â”œâ”€â”€ qwen3_vl.py         # Qwen3-VL integration
â”‚   â”œâ”€â”€ dots_ocr.py         # dots.ocr integration
â”‚   â””â”€â”€ model_loader.py     # Model factory
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ model_cache.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ check_gpu.py        # GPU compatibility checker
â”‚   â””â”€â”€ check_models.py     # Model cache checker
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ gpu_requirements.md
â”‚   â”œâ”€â”€ qwen3_vl_guide.md
â”‚   â””â”€â”€ api_guide.md
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ api_usage.py
â”‚   â””â”€â”€ api_curl.sh
â”œâ”€â”€ Dockerfile              # Docker image
â”œâ”€â”€ docker-compose.yml      # Docker services
â””â”€â”€ config.yaml             # Configuration
```

### Testing

```bash
pytest tests/
```

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or PR.

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“ License

MIT License

## ğŸ”— Links

- **GitHub**: https://github.com/OlegKarenkikh/chatvlmllm
- **Qwen3-VL**: https://github.com/QwenLM/Qwen3-VL
- **GOT-OCR**: https://github.com/Ucas-HaoranWei/GOT-OCR2.0
- **dots.ocr**: https://github.com/rednote-hilab/dots.ocr

## â­ Acknowledgments

- Qwen Team for Qwen3-VL
- Stepfun AI for GOT-OCR 2.0
- RedNote for dots.ocr

---

**Star â­ this repo if you find it useful!**

## ğŸ“Š Status

![GitHub stars](https://img.shields.io/github/stars/OlegKarenkikh/chatvlmllm?style=social)
![GitHub forks](https://img.shields.io/github/forks/OlegKarenkikh/chatvlmllm?style=social)
![License](https://img.shields.io/github/license/OlegKarenkikh/chatvlmllm)

**Production Ready** | **7 Models** | **REST API** | **Docker Support**