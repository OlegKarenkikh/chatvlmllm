#!/usr/bin/env python3
"""
–ö–ª–∏–µ–Ω—Ç –¥–ª—è dots.ocr —á–µ—Ä–µ–∑ vLLM Docker —Å–µ—Ä–≤–µ—Ä
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å chatvlmllm –ø—Ä–æ–µ–∫—Ç–æ–º
"""

import requests
import base64
import json
import time
from typing import Dict, List, Any, Optional
from PIL import Image
import io
import logging

logger = logging.getLogger(__name__)

class VLLMDotsOCRClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è dots.ocr —á–µ—Ä–µ–∑ vLLM Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({"Content-Type": "application/json"})
        
    def health_check(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ vLLM —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            response = self.session.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except Exception as e:
            logger.warning(f"Health check failed: {e}")
            return False
    
    def get_models(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            response = self.session.get(f"{self.base_url}/v1/models", timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                return {"error": f"HTTP {response.status_code}"}
        except Exception as e:
            return {"error": str(e)}
    
    def encode_image_to_base64(self, image_path: str) -> str:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            raise ValueError(f"Cannot encode image {image_path}: {e}")
    
    def encode_pil_image_to_base64(self, image: Image.Image) -> str:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64"""
        try:
            buffer = io.BytesIO()
            image.save(buffer, format='PNG')
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        except Exception as e:
            raise ValueError(f"Cannot encode PIL image: {e}")
    
    def process_image(self, image_path: str, prompt: str = "Extract all text from this image") -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ vLLM dots.ocr"""
        try:
            start_time = time.time()
            
            # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_base64 = self.encode_image_to_base64(image_path)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
            payload = {
                "model": "dots.ocr",
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 2048,
                "temperature": 0.0
            }
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            response = self.session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload,
                timeout=120  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π timeout –¥–ª—è OCR
            )
            
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "content": content,
                    "model": "dots.ocr-vllm",
                    "processing_time": f"{processing_time:.3f}s",
                    "usage": result.get("usage", {}),
                    "raw_response": result
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "processing_time": f"{processing_time:.3f}s"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "processing_time": "N/A"
            }
    
    def process_pil_image(self, image: Image.Image, prompt: str = "Extract all text from this image") -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            start_time = time.time()
            
            # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_base64 = self.encode_pil_image_to_base64(image)
            
            payload = {
                "model": "dots.ocr",
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 2048,
                "temperature": 0.0
            }
            
            response = self.session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload,
                timeout=120
            )
            
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "content": content,
                    "model": "dots.ocr-vllm",
                    "processing_time": f"{processing_time:.3f}s",
                    "usage": result.get("usage", {})
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "processing_time": f"{processing_time:.3f}s"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "processing_time": "N/A"
            }
    
    def chat_completion(self, messages: List[Dict], max_tokens: int = 2048) -> Dict[str, Any]:
        """
        OpenAI —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è chatvlmllm –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –û—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI API
        """
        try:
            start_time = time.time()
            
            payload = {
                "model": "dots.ocr",
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": 0.0
            }
            
            response = self.session.post(
                f"{self.base_url}/v1/chat/completions",
                json=payload,
                timeout=120
            )
            
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                if "usage" not in result:
                    result["usage"] = {}
                result["usage"]["processing_time"] = f"{processing_time:.3f}s"
                result["usage"]["model"] = "dots.ocr-vllm"
                
                return result
            else:
                return {
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": f"vLLM Error: {response.text}"
                        },
                        "finish_reason": "error"
                    }],
                    "usage": {
                        "processing_time": f"{processing_time:.3f}s",
                        "model": "dots.ocr-vllm",
                        "error": True
                    }
                }
                
        except Exception as e:
            return {
                "error": str(e),
                "choices": [{
                    "message": {
                        "role": "assistant",
                        "content": f"Connection error: {str(e)}"
                    },
                    "finish_reason": "error"
                }],
                "usage": {
                    "processing_time": "N/A",
                    "model": "dots.ocr-vllm",
                    "error": True
                }
            }
    
    def get_server_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–µ—Ä–≤–µ—Ä–µ"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è
            health = self.health_check()
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            models = self.get_models()
            
            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
            try:
                metrics_response = self.session.get(f"{self.base_url}/metrics", timeout=5)
                metrics_available = metrics_response.status_code == 200
            except:
                metrics_available = False
            
            return {
                "healthy": health,
                "models": models,
                "metrics_available": metrics_available,
                "base_url": self.base_url
            }
            
        except Exception as e:
            return {
                "healthy": False,
                "error": str(e),
                "base_url": self.base_url
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ chatvlmllm
_vllm_client_instance = None

def get_vllm_dots_ocr_client(base_url: str = "http://localhost:8000") -> VLLMDotsOCRClient:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ vLLM –∫–ª–∏–µ–Ω—Ç–∞"""
    global _vllm_client_instance
    
    if _vllm_client_instance is None or _vllm_client_instance.base_url != base_url:
        _vllm_client_instance = VLLMDotsOCRClient(base_url)
    
    return _vllm_client_instance

def test_vllm_client():
    """–¢–µ—Å—Ç vLLM –∫–ª–∏–µ–Ω—Ç–∞"""
    print("üß™ –¢–ï–°–¢ VLLM DOTS.OCR –ö–õ–ò–ï–ù–¢–ê")
    print("=" * 40)
    
    client = get_vllm_dots_ocr_client()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è...")
    if not client.health_check():
        print("‚ùå vLLM —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω:")
        print("   docker ps | grep dots-ocr-server")
        return False
    
    print("‚úÖ vLLM —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–µ—Ä–≤–µ—Ä–µ
    print("\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–µ—Ä–µ:")
    server_info = client.get_server_info()
    print(f"   URL: {server_info['base_url']}")
    print(f"   –ó–¥–æ—Ä–æ–≤—å–µ: {server_info['healthy']}")
    print(f"   –ú–µ—Ç—Ä–∏–∫–∏: {server_info.get('metrics_available', False)}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    print("\nüñºÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
    try:
        from PIL import Image, ImageDraw, ImageFont
        
        img = Image.new('RGB', (600, 200), color='white')
        draw = ImageDraw.Draw(img)
        
        try:
            font = ImageFont.truetype("arial.ttf", 32)
        except:
            font = ImageFont.load_default()
        
        draw.text((50, 80), "VLLM DOTS.OCR TEST", fill='black', font=font)
        img.save('vllm_test_image.png')
        
        print("‚úÖ –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return False
    
    # –¢–µ—Å—Ç OCR
    print("\nüîç –¢–µ—Å—Ç OCR —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...")
    result = client.process_image('vllm_test_image.png', "Extract all text from this image")
    
    if result["success"]:
        print(f"‚úÖ OCR —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['content']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['processing_time']}")
        
        if "VLLM" in result['content'].upper() or "TEST" in result['content'].upper():
            print("üéâ –¢–µ–∫—Å—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
            return True
        else:
            print("‚ö†Ô∏è –¢–µ–∫—Å—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é")
            return True
    else:
        print(f"‚ùå OCR –æ—à–∏–±–∫–∞: {result['error']}")
        return False

if __name__ == "__main__":
    success = test_vllm_client()
    
    if success:
        print("\nüéâ VLLM DOTS.OCR –ö–õ–ò–ï–ù–¢ –†–ê–ë–û–¢–ê–ï–¢!")
        print("üìã –ì–æ—Ç–æ–≤ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ chatvlmllm")
    else:
        print("\n‚ùå –ü–†–û–ë–õ–ï–ú–´ –° VLLM –ö–õ–ò–ï–ù–¢–û–ú")
        print("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞")