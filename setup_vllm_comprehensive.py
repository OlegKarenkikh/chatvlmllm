#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ vLLM —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—Å–µ—Ö –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: dots.ocr –∏ –¥—Ä—É–≥–∏–µ OCR –º–æ–¥–µ–ª–∏
"""

import subprocess
import time
import requests
import sys
import os
import json
import yaml
from pathlib import Path

class VLLMSetup:
    def __init__(self):
        self.cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        self.models_config = {}
        self.running_containers = []
        
    def run_command(self, command, shell=True, check=True):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        print(f"üîÑ –í—ã–ø–æ–ª–Ω—è–µ–º: {command}")
        try:
            result = subprocess.run(command, shell=shell, check=check, 
                                  capture_output=True, text=True)
            if result.stdout:
                print(f"‚úÖ {result.stdout.strip()}")
            return result
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            if e.stderr:
                print(f"‚ùå Stderr: {e.stderr}")
            return None

    def check_prerequisites(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"""
        print("üîç –ü–†–û–í–ï–†–ö–ê –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–´–• –¢–†–ï–ë–û–í–ê–ù–ò–ô")
        print("=" * 50)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Docker
        result = self.run_command("docker --version", check=False)
        if result is None:
            print("‚ùå Docker –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ NVIDIA –¥—Ä–∞–π–≤–µ—Ä–∞
        result = self.run_command("nvidia-smi", check=False)
        if result is None:
            print("‚ùå NVIDIA –¥—Ä–∞–π–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –≤ Docker
        result = self.run_command("docker run --rm --gpus all nvidia/cuda:12.8-base-ubuntu22.04 nvidia-smi", check=False)
        if result is None:
            print("‚ùå GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ Docker")
            return False
        
        print("‚úÖ –í—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã")
        return True

    def analyze_cached_models(self):
        """–ê–Ω–∞–ª–∏–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        print("\nüîç –ê–ù–ê–õ–ò–ó –ö–ï–®–ò–†–û–í–ê–ù–ù–´–• –ú–û–î–ï–õ–ï–ô")
        print("=" * 40)
        
        if not self.cache_dir.exists():
            print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞ HuggingFace –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return {}
        
        model_dirs = [d for d in self.cache_dir.iterdir() if d.is_dir() and d.name.startswith('models--')]
        
        models = {
            'ocr': [],
            'vlm': [],
            'other': []
        }
        
        for model_dir in model_dirs:
            model_name = model_dir.name.replace('models--', '').replace('--', '/')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
            snapshots_dir = model_dir / "snapshots"
            if not snapshots_dir.exists():
                continue
                
            snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
            if not snapshot_dirs:
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
            if any(keyword in model_name.lower() for keyword in ['ocr', 'got', 'dots']):
                models['ocr'].append(model_name)
            elif any(keyword in model_name.lower() for keyword in ['vision', 'vlm', 'qwen', 'phi']):
                models['vlm'].append(model_name)
            else:
                models['other'].append(model_name)
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π:")
        print(f"   OCR: {len(models['ocr'])}")
        print(f"   VLM: {len(models['vlm'])}")
        print(f"   –î—Ä—É–≥–∏–µ: {len(models['other'])}")
        
        return models

    def create_vllm_config(self, models):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è vLLM"""
        print("\nüìù –°–û–ó–î–ê–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò VLLM")
        print("=" * 35)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞
        priority_models = {
            'dots_ocr': 'rednote-hilab/dots.ocr',
            'got_ocr': 'stepfun-ai/GOT-OCR2_0',
            'deepseek_ocr': 'deepseek-ai/deepseek-ocr',
            'qwen3_vl': 'Qwen/Qwen3-VL-2B-Instruct',
            'phi3_vision': 'microsoft/Phi-3.5-vision-instruct'
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        available_models = {}
        all_models = models['ocr'] + models['vlm'] + models['other']
        
        for key, model_name in priority_models.items():
            if model_name in all_models:
                available_models[key] = model_name
                print(f"‚úÖ {key}: {model_name}")
            else:
                print(f"‚ùå {key}: {model_name} (–Ω–µ –Ω–∞–π–¥–µ–Ω–∞)")
        
        self.models_config = available_models
        return available_models

    def start_dots_ocr_container(self):
        """–ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ dots.ocr (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)"""
        if 'dots_ocr' not in self.models_config:
            print("‚ùå dots.ocr –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ")
            return False
            
        print("\nüöÄ –ó–ê–ü–£–°–ö DOTS.OCR –ö–û–ù–¢–ï–ô–ù–ï–†–ê")
        print("=" * 35)
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        self.run_command("docker stop dots-ocr-vllm", check=False)
        self.run_command("docker rm dots-ocr-vllm", check=False)
        
        # –ü—É—Ç—å –∫ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        cache_path = str(self.cache_dir).replace('\\', '/')
        
        # –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ —Å –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫–µ—à–∞
        docker_command = f"""
        docker run -d \
            --gpus all \
            --name dots-ocr-vllm \
            --restart unless-stopped \
            -p 8000:8000 \
            -v {cache_path}:/root/.cache/huggingface/hub:ro \
            -e VLLM_GPU_MEMORY_UTILIZATION=0.8 \
            -e VLLM_MAX_MODEL_LEN=4096 \
            -e CUDA_VISIBLE_DEVICES=0 \
            --shm-size=8g \
            vllm/vllm-openai:latest \
            --model rednote-hilab/dots.ocr \
            --trust-remote-code \
            --max-model-len 4096 \
            --gpu-memory-utilization 0.8
        """.strip().replace('\n', ' ').replace('\\', '')
        
        result = self.run_command(docker_command)
        
        if result:
            print("‚úÖ dots.ocr –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É 8000")
            self.running_containers.append(('dots-ocr-vllm', 8000, 'rednote-hilab/dots.ocr'))
            return True
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å dots.ocr –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä")
            return False

    def start_additional_models(self):
        """–ó–∞–ø—É—Å–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä—Ç–∞—Ö"""
        print("\nüöÄ –ó–ê–ü–£–°–ö –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ú–û–î–ï–õ–ï–ô")
        print("=" * 40)
        
        port = 8001
        cache_path = str(self.cache_dir).replace('\\', '/')
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        additional_models = [
            ('got_ocr', 'stepfun-ai/GOT-OCR2_0', 'got-ocr-vllm'),
            ('qwen3_vl', 'Qwen/Qwen3-VL-2B-Instruct', 'qwen3-vl-vllm'),
            ('phi3_vision', 'microsoft/Phi-3.5-vision-instruct', 'phi3-vision-vllm')
        ]
        
        for model_key, model_name, container_name in additional_models:
            if model_key not in self.models_config:
                print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {model_name} (–Ω–µ –Ω–∞–π–¥–µ–Ω–∞)")
                continue
            
            print(f"\nüîÑ –ó–∞–ø—É—Å–∫ {model_name} –Ω–∞ –ø–æ—Ä—Ç—É {port}")
            
            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
            self.run_command(f"docker stop {container_name}", check=False)
            self.run_command(f"docker rm {container_name}", check=False)
            
            # –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
            docker_command = f"""
            docker run -d \
                --gpus all \
                --name {container_name} \
                --restart unless-stopped \
                -p {port}:{port} \
                -v {cache_path}:/root/.cache/huggingface/hub:ro \
                -e VLLM_GPU_MEMORY_UTILIZATION=0.6 \
                -e VLLM_MAX_MODEL_LEN=2048 \
                --shm-size=4g \
                vllm/vllm-openai:latest \
                --model {model_name} \
                --trust-remote-code \
                --max-model-len 2048 \
                --gpu-memory-utilization 0.6 \
                --port {port}
            """.strip().replace('\n', ' ').replace('\\', '')
            
            result = self.run_command(docker_command)
            
            if result:
                print(f"‚úÖ {model_name} –∑–∞–ø—É—â–µ–Ω–∞ –Ω–∞ –ø–æ—Ä—Ç—É {port}")
                self.running_containers.append((container_name, port, model_name))
                port += 1
            else:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å {model_name}")

    def wait_for_servers(self):
        """–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤"""
        print("\n‚è≥ –û–ñ–ò–î–ê–ù–ò–ï –ó–ê–ü–£–°–ö–ê –°–ï–†–í–ï–†–û–í")
        print("=" * 35)
        
        max_attempts = 20
        
        for container_name, port, model_name in self.running_containers:
            print(f"\nüîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ {model_name} –Ω–∞ –ø–æ—Ä—Ç—É {port}")
            
            for attempt in range(max_attempts):
                try:
                    response = requests.get(f"http://localhost:{port}/health", timeout=5)
                    if response.status_code == 200:
                        print(f"‚úÖ {model_name} –≥–æ—Ç–æ–≤–∞!")
                        break
                except:
                    pass
                
                print(f"‚è≥ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_attempts}...")
                time.sleep(15)
            else:
                print(f"‚ùå {model_name} –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª–∞—Å—å")
                print(f"üìã –õ–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ {container_name}:")
                self.run_command(f"docker logs --tail 20 {container_name}")

    def create_unified_client(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("\nüìù –°–û–ó–î–ê–ù–ò–ï –£–ù–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–û–ì–û –ö–õ–ò–ï–ù–¢–ê")
        print("=" * 45)
        
        client_code = f'''#!/usr/bin/env python3
"""
–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö vLLM —Å–µ—Ä–≤–µ—Ä–æ–≤
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {time.strftime("%Y-%m-%d %H:%M:%S")}
"""

import requests
import base64
import json
from typing import Dict, Any, Optional

class UnifiedVLLMClient:
    def __init__(self):
        self.servers = {servers_config}
        
    def health_check_all(self) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤"""
        status = {{}}
        for name, config in self.servers.items():
            try:
                response = requests.get(f"http://localhost:{{config['port']}}/health", timeout=5)
                status[name] = response.status_code == 200
            except:
                status[name] = False
        return status
    
    def process_image(self, image_path: str, model: str = "dots_ocr", 
                     prompt: str = "Extract all text from this image") -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
        
        if model not in self.servers:
            return {{"success": False, "error": f"–ú–æ–¥–µ–ª—å {{model}} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"}}
        
        server_config = self.servers[model]
        port = server_config['port']
        model_name = server_config['model']
        
        try:
            # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            with open(image_path, "rb") as f:
                image_base64 = base64.b64encode(f.read()).decode('utf-8')
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ MIME —Ç–∏–ø–∞
            ext = image_path.lower().split('.')[-1]
            mime_type = "image/jpeg" if ext in ['jpg', 'jpeg'] else f"image/{{ext}}"
            
            # –ó–∞–ø—Ä–æ—Å –∫ API
            payload = {{
                "model": model_name,
                "messages": [{{
                    "role": "user",
                    "content": [
                        {{"type": "text", "text": prompt}},
                        {{"type": "image_url", "image_url": {{"url": f"data:{{mime_type}};base64,{{image_base64}}"}}}}
                    ]
                }}],
                "max_tokens": 2048,
                "temperature": 0.1
            }}
            
            response = requests.post(
                f"http://localhost:{{port}}/v1/chat/completions", 
                json=payload, 
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return {{
                    "success": True,
                    "content": result["choices"][0]["message"]["content"],
                    "model": model_name,
                    "server": model
                }}
            else:
                return {{"success": False, "error": f"HTTP {{response.status_code}}: {{response.text}}"}}
                
        except Exception as e:
            return {{"success": False, "error": str(e)}}
    
    def list_available_models(self) -> Dict[str, Dict]:
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        available = {{}}
        status = self.health_check_all()
        
        for name, config in self.servers.items():
            available[name] = {{
                "model": config['model'],
                "port": config['port'],
                "status": "online" if status.get(name, False) else "offline",
                "description": config.get('description', '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è')
            }}
        
        return available

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    client = UnifiedVLLMClient()
    
    print("üîç –ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ù–û–°–¢–ò –°–ï–†–í–ï–†–û–í")
    print("=" * 40)
    
    models = client.list_available_models()
    for name, info in models.items():
        status_icon = "‚úÖ" if info['status'] == 'online' else "‚ùå"
        print(f"{{status_icon}} {{name}}: {{info['model']}} (–ø–æ—Ä—Ç {{info['port']}})")
    
    # –¢–µ—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
    test_images = ['vllm_test_image.png', 'test_image.png', 'simple_test.png']
    test_image = None
    
    for img in test_images:
        if os.path.exists(img):
            test_image = img
            break
    
    if test_image:
        print(f"\\nüß™ –¢–ï–°–¢ –° –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï–ú: {{test_image}}")
        print("=" * 50)
        
        # –¢–µ—Å—Ç —Å dots.ocr (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        if 'dots_ocr' in models and models['dots_ocr']['status'] == 'online':
            print("üîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º dots.ocr...")
            result = client.process_image(test_image, 'dots_ocr')
            if result['success']:
                print(f"‚úÖ dots.ocr —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {{result['content'][:200]}}...")
            else:
                print(f"‚ùå dots.ocr –æ—à–∏–±–∫–∞: {{result['error']}}")
    else:
        print("\\n‚ö†Ô∏è –¢–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        print("üí° –°–æ–∑–¥–∞–π—Ç–µ test_image.png –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
'''
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–µ—Ä–≤–µ—Ä–æ–≤
        servers_config = {}
        for container_name, port, model_name in self.running_containers:
            key = container_name.replace('-vllm', '').replace('-', '_')
            servers_config[key] = {
                'port': port,
                'model': model_name,
                'description': f'vLLM —Å–µ—Ä–≤–µ—Ä –¥–ª—è {model_name}'
            }
        
        # –ü–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ –∫–æ–¥
        client_code = client_code.replace('{servers_config}', json.dumps(servers_config, indent=12))
        
        with open('unified_vllm_client.py', 'w', encoding='utf-8') as f:
            f.write(client_code)
        
        print("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: unified_vllm_client.py")

    def create_management_script(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º–∏"""
        print("\nüìù –°–û–ó–î–ê–ù–ò–ï –°–ö–†–ò–ü–¢–ê –£–ü–†–ê–í–õ–ï–ù–ò–Ø")
        print("=" * 35)
        
        management_code = f'''#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º–∏
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {time.strftime("%Y-%m-%d %H:%M:%S")}
"""

import subprocess
import sys

CONTAINERS = {[f'"{name}"' for name, _, _ in self.running_containers]}

def run_command(command):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
    try:
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {{e}}")
        return None

def status():
    """–°—Ç–∞—Ç—É—Å –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
    print("üìä –°–¢–ê–¢–£–° –ö–û–ù–¢–ï–ô–ù–ï–†–û–í")
    print("=" * 30)
    
    for container in CONTAINERS:
        result = run_command(f"docker ps -f name={{container}} --format 'table {{{{.Names}}}}\\t{{{{.Status}}}}\\t{{{{.Ports}}}}'")
        if result and container in result:
            print(f"‚úÖ {{container}}: –ó–∞–ø—É—â–µ–Ω")
        else:
            print(f"‚ùå {{container}}: –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

def start_all():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
    print("üöÄ –ó–ê–ü–£–°–ö –í–°–ï–• –ö–û–ù–¢–ï–ô–ù–ï–†–û–í")
    print("=" * 30)
    
    for container in CONTAINERS:
        print(f"üîÑ –ó–∞–ø—É—Å–∫ {{container}}...")
        result = run_command(f"docker start {{container}}")
        if result:
            print(f"‚úÖ {{container}} –∑–∞–ø—É—â–µ–Ω")
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å {{container}}")

def stop_all():
    """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
    print("üõë –û–°–¢–ê–ù–û–í–ö–ê –í–°–ï–• –ö–û–ù–¢–ï–ô–ù–ï–†–û–í")
    print("=" * 30)
    
    for container in CONTAINERS:
        print(f"üîÑ –û—Å—Ç–∞–Ω–æ–≤–∫–∞ {{container}}...")
        result = run_command(f"docker stop {{container}}")
        if result:
            print(f"‚úÖ {{container}} –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å {{container}}")

def restart_all():
    """–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
    print("üîÑ –ü–ï–†–ï–ó–ê–ü–£–°–ö –í–°–ï–• –ö–û–ù–¢–ï–ô–ù–ï–†–û–í")
    print("=" * 30)
    
    stop_all()
    print()
    start_all()

def logs(container_name=None):
    """–ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤"""
    if container_name:
        if container_name in CONTAINERS:
            print(f"üìã –õ–û–ì–ò {{container_name}}")
            print("=" * 30)
            run_command(f"docker logs --tail 50 {{container_name}}")
        else:
            print(f"‚ùå –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä {{container_name}} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    else:
        print("üìã –õ–û–ì–ò –í–°–ï–• –ö–û–ù–¢–ï–ô–ù–ï–†–û–í")
        print("=" * 30)
        for container in CONTAINERS:
            print(f"\\n--- {{container}} ---")
            run_command(f"docker logs --tail 10 {{container}}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    if len(sys.argv) < 2:
        print("üîß –£–ü–†–ê–í–õ–ï–ù–ò–ï VLLM –ö–û–ù–¢–ï–ô–ù–ï–†–ê–ú–ò")
        print("=" * 35)
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python manage_vllm.py status     - –°—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤")
        print("  python manage_vllm.py start      - –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö")
        print("  python manage_vllm.py stop       - –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö")
        print("  python manage_vllm.py restart    - –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö")
        print("  python manage_vllm.py logs       - –õ–æ–≥–∏ –≤—Å–µ—Ö")
        print("  python manage_vllm.py logs <name> - –õ–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞")
        return
    
    command = sys.argv[1].lower()
    
    if command == "status":
        status()
    elif command == "start":
        start_all()
    elif command == "stop":
        stop_all()
    elif command == "restart":
        restart_all()
    elif command == "logs":
        container = sys.argv[2] if len(sys.argv) > 2 else None
        logs(container)
    else:
        print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: {{command}}")

if __name__ == "__main__":
    main()
'''
        
        with open('manage_vllm.py', 'w', encoding='utf-8') as f:
            f.write(management_code)
        
        print("‚úÖ –°–∫—Ä–∏–ø—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω: manage_vllm.py")

    def main(self):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        print("üöÄ –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê VLLM –° –ö–ï–®–ò–†–û–í–ê–ù–ù–´–ú–ò –ú–û–î–ï–õ–Ø–ú–ò")
        print("=" * 70)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        if not self.check_prerequisites():
            print("\n‚ùå –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –∏–∑-–∑–∞ –Ω–µ–≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π")
            sys.exit(1)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        models = self.analyze_cached_models()
        if not models['ocr'] and not models['vlm']:
            print("\n‚ùå –ü–æ–¥—Ö–æ–¥—è—â–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –∫–µ—à–µ")
            sys.exit(1)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        available_models = self.create_vllm_config(models)
        if not available_models:
            print("\n‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–ø—É—Å–∫–∞")
            sys.exit(1)
        
        # –ó–∞–ø—É—Å–∫ dots.ocr (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        dots_ocr_started = self.start_dots_ocr_container()
        
        # –ó–∞–ø—É—Å–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.start_additional_models()
        
        if not self.running_containers:
            print("\n‚ùå –ù–∏ –æ–¥–∏–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –±—ã–ª –∑–∞–ø—É—â–µ–Ω")
            sys.exit(1)
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–æ–≤
        self.wait_for_servers()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤
        self.create_unified_client()
        self.create_management_script()
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print("\nüéâ –ù–ê–°–¢–†–û–ô–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("=" * 30)
        print("üì° –ó–∞–ø—É—â–µ–Ω–Ω—ã–µ —Å–µ—Ä–≤–µ—Ä—ã:")
        for container_name, port, model_name in self.running_containers:
            print(f"   ‚Ä¢ {model_name} ‚Üí http://localhost:{port}")
        
        print("\nüìã –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        print("   ‚Ä¢ unified_vllm_client.py - –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç")
        print("   ‚Ä¢ manage_vllm.py - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º–∏")
        
        print("\nüí° –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
        print("   python unified_vllm_client.py  # –¢–µ—Å—Ç –≤—Å–µ—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤")
        print("   python manage_vllm.py status   # –°—Ç–∞—Ç—É—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤")
        print("   python manage_vllm.py logs     # –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤")
        
        if dots_ocr_started:
            print("\nüéØ dots.ocr –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –Ω–∞ http://localhost:8000")
        
        print("\n‚úÖ –í—Å–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã –∫ vLLM!")

if __name__ == "__main__":
    setup = VLLMSetup()
    setup.main()