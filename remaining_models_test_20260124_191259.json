{
  "timestamp": "2026-01-24 18:57:01",
  "models": {
    "Qwen/Qwen2-VL-2B-Instruct": {
      "config": {
        "container_name": "qwen2-vl-2b-instruct-vllm",
        "port": 8011,
        "vllm_params": {
          "max_model_len": 4096,
          "gpu_memory_utilization": 0.7,
          "trust_remote_code": true,
          "enforce_eager": false
        }
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": "\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:58:46 [default_loader.py:291] Loading weights took 38.62 seconds\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:58:46 [gpu_model_runner.py:3905] Model loading took 4.15 GiB memory and 53.407673 seconds\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:58:46 [gpu_model_runner.py:4715] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:59:18 [backends.py:644] Using cache directory: /root/.cache/vllm/torch_compile_cache/925502db92/rank_0_0/backbone for vLLM's torch.compile\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:59:18 [backends.py:704] Dynamo bytecode transform time: 3.92 s\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:59:26 [backends.py:261] Cache the graph of compile range (1, 2048) for later use\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:59:30 [backends.py:278] Compiling a graph for compile range (1, 2048) takes 7.22 s\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:59:30 [monitor.py:34] torch.compile takes 11.14 s in total\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:59:30 [gpu_worker.py:358] Available KV cache memory: 1.68 GiB\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:59:31 [kv_cache_utils.py:1305] GPU KV cache size: 62,752 tokens\n\u001b[0;36m(EngineCore_DP0 pid=80)\u001b[0;0m INFO 01-24 07:59:31 [kv_cache_utils.py:1310] Maximum concurrency for 4,096 tokens per request: 15.32x"
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "Qwen/Qwen2-VL-7B-Instruct": {
      "config": {
        "container_name": "qwen2-vl-7b-instruct-vllm",
        "port": 8013,
        "vllm_params": {
          "max_model_len": 4096,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": false
        }
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": ""
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "microsoft/Phi-3.5-vision-instruct": {
      "config": {
        "container_name": "phi-3-5-vision-instruct-vllm",
        "port": 8014,
        "vllm_params": {
          "max_model_len": 4096,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": false
        }
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": "\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:04:07 [gpu_model_runner.py:3808] Starting to load model microsoft/Phi-3.5-vision-instruct...\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:04:07 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:04:07 [vllm.py:630] Asynchronous scheduling is enabled.\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:04:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'TRITON_ATTN', 'FLEX_ATTENTION')\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:06:04 [default_loader.py:291] Loading weights took 97.37 seconds\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:06:05 [gpu_model_runner.py:3905] Model loading took 7.72 GiB memory and 117.101780 seconds\n\u001b[0;36m(EngineCore_DP0 pid=82)\u001b[0;0m INFO 01-24 08:06:05 [gpu_model_runner.py:4715] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 2 image items of the maximum feature size."
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "stepfun-ai/GOT-OCR-2.0-hf": {
      "config": {
        "container_name": "got-ocr-2-0-hf-vllm",
        "port": 8002,
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.7,
          "trust_remote_code": true,
          "enforce_eager": true
        }
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": ""
      },
      "functionality_result": null,
      "status": "launch_failed"
    },
    "vikhyatk/moondream2": {
      "config": {
        "container_name": "moondream2-vllm",
        "port": 8023,
        "vllm_params": {
          "max_model_len": 2048,
          "gpu_memory_utilization": 0.6,
          "trust_remote_code": true,
          "enforce_eager": true
        }
      },
      "launch_result": {
        "launch_success": false,
        "error": "Timeout after 180 seconds",
        "error_type": "timeout",
        "logs": ""
      },
      "functionality_result": null,
      "status": "launch_failed"
    }
  },
  "summary": {
    "total_tested": 5,
    "successful": 0,
    "failed": 5
  }
}