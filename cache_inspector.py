#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Å–ø–µ–∫—Ü–∏—è –∫–µ—à–∞ HuggingFace –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ vLLM
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Any

class CacheInspector:
    def __init__(self):
        self.cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        self.required_files = {
            "essential": [
                "config.json",
                "tokenizer.json",
                "tokenizer_config.json"
            ],
            "model_files": [
                "model.safetensors",
                "pytorch_model.bin",
                "model.safetensors.index.json",
                "pytorch_model.bin.index.json"
            ],
            "optional": [
                "generation_config.json",
                "preprocessor_config.json",
                "processor_config.json",
                "special_tokens_map.json",
                "vocab.json",
                "merges.txt",
                "added_tokens.json"
            ]
        }
    
    def get_model_cache_path(self, model_name: str) -> Path:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –∫–µ—à—É –º–æ–¥–µ–ª–∏"""
        cache_name = f"models--{model_name.replace('/', '--')}"
        return self.cache_dir / cache_name
    
    def get_latest_snapshot(self, model_path: Path) -> Path:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–Ω–∞–ø—à–æ—Ç–∞ –º–æ–¥–µ–ª–∏"""
        snapshots_dir = model_path / "snapshots"
        if not snapshots_dir.exists():
            return None
        
        snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
        if not snapshot_dirs:
            return None
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π —Å–Ω–∞–ø—à–æ—Ç
        return max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
    
    def check_model_files(self, model_name: str) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏"""
        model_path = self.get_model_cache_path(model_name)
        
        result = {
            "model_name": model_name,
            "cache_exists": model_path.exists(),
            "cache_path": str(model_path),
            "total_size_mb": 0,
            "files_found": {},
            "missing_files": [],
            "issues": [],
            "readiness_score": 0,
            "vllm_ready": False
        }
        
        if not model_path.exists():
            result["issues"].append("Model cache directory not found")
            return result
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–Ω–∞–ø—à–æ—Ç–∞
        latest_snapshot = self.get_latest_snapshot(model_path)
        if not latest_snapshot:
            result["issues"].append("No snapshots found")
            return result
        
        result["snapshot_path"] = str(latest_snapshot)
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        total_size = 0
        for root, dirs, files in os.walk(model_path):
            for file in files:
                file_path = os.path.join(root, file)
                if os.path.exists(file_path):
                    total_size += os.path.getsize(file_path)
        
        result["total_size_mb"] = round(total_size / (1024**2), 2)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        essential_found = 0
        for file_name in self.required_files["essential"]:
            file_path = latest_snapshot / file_name
            if file_path.exists():
                result["files_found"][file_name] = {
                    "exists": True,
                    "size_kb": round(file_path.stat().st_size / 1024, 2)
                }
                essential_found += 1
            else:
                result["missing_files"].append(file_name)
                result["files_found"][file_name] = {"exists": False}
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏ (–Ω—É–∂–µ–Ω —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω)
        model_file_found = False
        for file_name in self.required_files["model_files"]:
            file_path = latest_snapshot / file_name
            if file_path.exists():
                result["files_found"][file_name] = {
                    "exists": True,
                    "size_mb": round(file_path.stat().st_size / (1024**2), 2)
                }
                model_file_found = True
            else:
                result["files_found"][file_name] = {"exists": False}
        
        if not model_file_found:
            result["issues"].append("No model weight files found")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        for file_name in self.required_files["optional"]:
            file_path = latest_snapshot / file_name
            if file_path.exists():
                result["files_found"][file_name] = {
                    "exists": True,
                    "size_kb": round(file_path.stat().st_size / 1024, 2)
                }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è vision –º–æ–¥–µ–ª–µ–π
        vision_files = [
            "preprocessor_config.json",
            "processor_config.json"
        ]
        
        vision_file_found = any(
            (latest_snapshot / f).exists() for f in vision_files
        )
        
        if vision_file_found:
            result["model_type"] = "vision"
        else:
            result["model_type"] = "text"
        
        # –†–∞—Å—á–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        readiness_score = 0
        
        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã (60% –æ—Ç –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏)
        readiness_score += (essential_found / len(self.required_files["essential"])) * 60
        
        # –§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ (40% –æ—Ç –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏)
        if model_file_found:
            readiness_score += 40
        
        result["readiness_score"] = round(readiness_score, 1)
        result["vllm_ready"] = readiness_score >= 90 and result["total_size_mb"] > 0.1
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        if result["total_size_mb"] < 0.1:
            result["issues"].append("Model size too small - likely incomplete download")
        
        if essential_found < len(self.required_files["essential"]):
            result["issues"].append(f"Missing {len(self.required_files['essential']) - essential_found} essential files")
        
        return result
    
    def inspect_all_models(self) -> Dict[str, Any]:
        """–ò–Ω—Å–ø–µ–∫—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–µ—à–µ"""
        print("üîç –î–ï–¢–ê–õ–¨–ù–ê–Ø –ò–ù–°–ü–ï–ö–¶–ò–Ø –ö–ï–®–ê HUGGINGFACE")
        print("=" * 50)
        
        if not self.cache_dir.exists():
            print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞ HuggingFace –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return {}
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–µ—à–µ
        model_dirs = [d for d in self.cache_dir.iterdir() 
                     if d.is_dir() and d.name.startswith('models--')]
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(model_dirs)} –º–æ–¥–µ–ª–µ–π –≤ –∫–µ—à–µ")
        print(f"üìÇ –ü—É—Ç—å –∫ –∫–µ—à—É: {self.cache_dir}")
        
        results = {}
        total_size_gb = 0
        ready_models = 0
        incomplete_models = 0
        
        for model_dir in model_dirs:
            model_name = model_dir.name.replace('models--', '').replace('--', '/')
            print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞: {model_name}")
            
            result = self.check_model_files(model_name)
            results[model_name] = result
            
            total_size_gb += result["total_size_mb"] / 1024
            
            if result["vllm_ready"]:
                ready_models += 1
                print(f"   ‚úÖ –ì–æ—Ç–æ–≤–∞ –∫ vLLM ({result['readiness_score']}%, {result['total_size_mb']} –ú–ë)")
            else:
                incomplete_models += 1
                print(f"   ‚ùå –ù–µ –≥–æ—Ç–æ–≤–∞ ({result['readiness_score']}%, {result['total_size_mb']} –ú–ë)")
                if result["issues"]:
                    for issue in result["issues"]:
                        print(f"      ‚ö†Ô∏è {issue}")
        
        # –°–≤–æ–¥–∫–∞
        print(f"\nüìä –°–í–û–î–ö–ê –ò–ù–°–ü–ï–ö–¶–ò–ò")
        print("=" * 25)
        print(f"–í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(results)}")
        print(f"–ì–æ—Ç–æ–≤—ã –∫ vLLM: {ready_models}")
        print(f"–ù–µ –≥–æ—Ç–æ–≤—ã: {incomplete_models}")
        print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –∫–µ—à–∞: {total_size_gb:.2f} –ì–ë")
        
        return results
    
    def create_detailed_report(self, results: Dict[str, Any]):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report = {
            "inspection_timestamp": __import__('time').strftime("%Y-%m-%d %H:%M:%S"),
            "cache_path": str(self.cache_dir),
            "summary": {
                "total_models": len(results),
                "ready_models": sum(1 for r in results.values() if r["vllm_ready"]),
                "incomplete_models": sum(1 for r in results.values() if not r["vllm_ready"]),
                "total_size_gb": round(sum(r["total_size_mb"] for r in results.values()) / 1024, 2)
            },
            "models": results
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON –æ—Ç—á–µ—Ç–∞
        with open('cache_inspection_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        with open('cache_inspection_report.txt', 'w', encoding='utf-8') as f:
            f.write("–û–¢–ß–ï–¢ –û –ò–ù–°–ü–ï–ö–¶–ò–ò –ö–ï–®–ê HUGGINGFACE\n")
            f.write("=" * 40 + "\n")
            f.write(f"–î–∞—Ç–∞: {report['inspection_timestamp']}\n")
            f.write(f"–ü—É—Ç—å –∫ –∫–µ—à—É: {report['cache_path']}\n\n")
            
            f.write("–°–í–û–î–ö–ê:\n")
            f.write("-" * 10 + "\n")
            f.write(f"–í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {report['summary']['total_models']}\n")
            f.write(f"–ì–æ—Ç–æ–≤—ã –∫ vLLM: {report['summary']['ready_models']}\n")
            f.write(f"–ù–µ –≥–æ—Ç–æ–≤—ã: {report['summary']['incomplete_models']}\n")
            f.write(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {report['summary']['total_size_gb']} –ì–ë\n\n")
            
            # –ì–æ—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
            ready_models = [name for name, result in results.items() if result["vllm_ready"]]
            if ready_models:
                f.write("‚úÖ –ì–û–¢–û–í–´–ï –ö vLLM –ú–û–î–ï–õ–ò:\n")
                f.write("-" * 30 + "\n")
                for model_name in ready_models:
                    result = results[model_name]
                    f.write(f"‚Ä¢ {model_name}\n")
                    f.write(f"  –†–∞–∑–º–µ—Ä: {result['total_size_mb']} –ú–ë\n")
                    f.write(f"  –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {result['readiness_score']}%\n")
                    f.write(f"  –¢–∏–ø: {result.get('model_type', 'unknown')}\n\n")
            
            # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–æ–¥–µ–ª–∏
            incomplete_models = [name for name, result in results.items() if not result["vllm_ready"]]
            if incomplete_models:
                f.write("‚ùå –ü–†–û–ë–õ–ï–ú–ù–´–ï –ú–û–î–ï–õ–ò:\n")
                f.write("-" * 25 + "\n")
                for model_name in incomplete_models:
                    result = results[model_name]
                    f.write(f"‚Ä¢ {model_name}\n")
                    f.write(f"  –†–∞–∑–º–µ—Ä: {result['total_size_mb']} –ú–ë\n")
                    f.write(f"  –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {result['readiness_score']}%\n")
                    if result["issues"]:
                        f.write("  –ü—Ä–æ–±–ª–µ–º—ã:\n")
                        for issue in result["issues"]:
                            f.write(f"    - {issue}\n")
                    f.write("\n")
        
        print(f"\nüíæ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   üìÑ cache_inspection_report.json")
        print(f"   üìÑ cache_inspection_report.txt")
    
    def get_vllm_ready_models(self, results: Dict[str, Any]) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π, –≥–æ—Ç–æ–≤—ã—Ö –∫ vLLM"""
        return [name for name, result in results.items() if result["vllm_ready"]]
    
    def get_incomplete_models(self, results: Dict[str, Any]) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–µ–ø–æ–ª–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return [name for name, result in results.items() if not result["vllm_ready"]]
    
    def suggest_cleanup(self, results: Dict[str, Any]):
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –æ—á–∏—Å—Ç–∫–µ –∫–µ—à–∞"""
        print(f"\nüßπ –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ü–û –û–ß–ò–°–¢–ö–ï")
        print("=" * 30)
        
        # –ú–æ–¥–µ–ª–∏ —Å –Ω—É–ª–µ–≤—ã–º —Ä–∞–∑–º–µ—Ä–æ–º
        zero_size_models = [name for name, result in results.items() 
                           if result["total_size_mb"] < 0.1]
        
        if zero_size_models:
            print(f"üóëÔ∏è –ú–æ–¥–µ–ª–∏ —Å –Ω—É–ª–µ–≤—ã–º —Ä–∞–∑–º–µ—Ä–æ–º (–º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å):")
            for model_name in zero_size_models:
                print(f"   ‚Ä¢ {model_name}")
        
        # –ú–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–æ–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å—é
        low_readiness_models = [name for name, result in results.items() 
                               if result["readiness_score"] < 50 and result["total_size_mb"] > 0.1]
        
        if low_readiness_models:
            print(f"\n‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–æ–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å—é (—Ç—Ä–µ–±—É—é—Ç –¥–æ–≤–∞–≥—Ä—É–∑–∫–∏):")
            for model_name in low_readiness_models:
                result = results[model_name]
                print(f"   ‚Ä¢ {model_name} ({result['readiness_score']}%)")
        
        # –ü–æ–¥—Å—á–µ—Ç –º–µ—Å—Ç–∞ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è
        cleanup_size = sum(results[name]["total_size_mb"] for name in zero_size_models)
        if cleanup_size > 0:
            print(f"\nüíæ –ú–æ–∂–Ω–æ –æ—Å–≤–æ–±–æ–¥–∏—Ç—å: {cleanup_size:.2f} –ú–ë")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    inspector = CacheInspector()
    
    # –ó–∞–ø—É—Å–∫ –∏–Ω—Å–ø–µ–∫—Ü–∏–∏
    results = inspector.inspect_all_models()
    
    if results:
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
        inspector.create_detailed_report(results)
        
        # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –æ—á–∏—Å—Ç–∫–µ
        inspector.suggest_cleanup(results)
        
        # –°–ø–∏—Å–æ–∫ –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        ready_models = inspector.get_vllm_ready_models(results)
        if ready_models:
            print(f"\nüéØ –ú–û–î–ï–õ–ò –ì–û–¢–û–í–´–ï –ö –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Æ vLLM:")
            for model_name in ready_models:
                result = results[model_name]
                print(f"   ‚úÖ {model_name} ({result['total_size_mb']} –ú–ë)")
        
        print(f"\n‚úÖ –ò–Ω—Å–ø–µ–∫—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    else:
        print(f"\n‚ùå –ò–Ω—Å–ø–µ–∫—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")

if __name__ == "__main__":
    main()