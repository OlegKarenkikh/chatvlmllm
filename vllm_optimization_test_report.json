{
  "timestamp": "2026-01-24T23:29:39.751677",
  "optimization_applied": true,
  "test_results": [
    {
      "max_tokens": 512,
      "success": true,
      "processing_time": 43.81964087486267,
      "tokens_used": 124,
      "response_length": 32
    },
    {
      "max_tokens": 1024,
      "success": true,
      "processing_time": 0.11384344100952148,
      "tokens_used": 124,
      "response_length": 32
    },
    {
      "max_tokens": 2048,
      "success": true,
      "processing_time": 0.10713744163513184,
      "tokens_used": 124,
      "response_length": 32
    },
    {
      "max_tokens": 4096,
      "success": true,
      "processing_time": 0.1098940372467041,
      "tokens_used": 124,
      "response_length": 32
    },
    {
      "max_tokens": 8192,
      "success": false,
      "error": 400,
      "error_text": "{\"error\":{\"message\":\"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 8192 tokens and your request has 17 input tokens (8192 > 8192 - 17). (parameter="
    }
  ],
  "summary": {
    "successful_tests": 4,
    "failed_tests": 1,
    "success_rate": 80.0
  }
}