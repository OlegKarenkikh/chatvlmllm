#!/usr/bin/env python3
"""
–°–µ–ª–µ–∫—Ç–æ—Ä —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π:
1. Transformers —Ä–µ–∂–∏–º (8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è, –Ω–∏–∑–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏)
2. vLLM —Ä–µ–∂–∏–º (–≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏)

–° –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫–µ—à–µ–π –º–æ–¥–µ–ª–µ–π
"""

import os
import subprocess
import json
import time
import requests
from pathlib import Path

class ModelModeSelector:
    def __init__(self):
        self.cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        self.models_config = self.load_models_config()
        self.current_mode = None
        self.running_containers = []
        
    def load_models_config(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π"""
        config = {
            "transformers_models": {
                "rednote-hilab/dots.ocr": {
                    "name": "DotsOCR",
                    "size_gb": 5.67,
                    "memory_8bit_gb": 3.5,
                    "category": "ocr",
                    "priority": 1
                },
                "stepfun-ai/GOT-OCR-2.0-hf": {
                    "name": "GOT-OCR 2.0",
                    "size_gb": 1.06,
                    "memory_8bit_gb": 0.8,
                    "category": "ocr",
                    "priority": 2
                },
                "Qwen/Qwen2-VL-2B-Instruct": {
                    "name": "Qwen2-VL 2B",
                    "size_gb": 4.13,
                    "memory_8bit_gb": 2.5,
                    "category": "vlm",
                    "priority": 3
                },
                "microsoft/Phi-3.5-vision-instruct": {
                    "name": "Phi-3.5 Vision",
                    "size_gb": 7.73,
                    "memory_8bit_gb": 4.5,
                    "category": "vlm",
                    "priority": 4
                }
            },
            "vllm_models": {
                "rednote-hilab/dots.ocr": {
                    "name": "DotsOCR",
                    "container_name": "dots-ocr-vllm",
                    "port": 8000,
                    "size_gb": 5.67,
                    "memory_required_gb": 8.0,
                    "category": "ocr",
                    "vllm_params": {
                        "max_model_len": 2048,
                        "gpu_memory_utilization": 0.7,
                        "trust_remote_code": True,
                        "enforce_eager": True,
                        "dtype": "bfloat16"
                    },
                    "priority": 1
                },
                "Qwen/Qwen2-VL-2B-Instruct": {
                    "name": "Qwen2-VL 2B",
                    "container_name": "qwen2-vl-2b-vllm",
                    "port": 8001,
                    "size_gb": 4.13,
                    "memory_required_gb": 6.0,
                    "category": "vlm",
                    "vllm_params": {
                        "max_model_len": 4096,
                        "gpu_memory_utilization": 0.6,
                        "trust_remote_code": True,
                        "enforce_eager": False,
                        "dtype": "bfloat16"
                    },
                    "priority": 2
                },
                "stepfun-ai/GOT-OCR-2.0-hf": {
                    "name": "GOT-OCR 2.0",
                    "container_name": "got-ocr-vllm",
                    "port": 8002,
                    "size_gb": 1.06,
                    "memory_required_gb": 3.0,
                    "category": "ocr",
                    "vllm_params": {
                        "max_model_len": 2048,
                        "gpu_memory_utilization": 0.5,
                        "trust_remote_code": True,
                        "enforce_eager": True,
                        "dtype": "bfloat16"
                    },
                    "priority": 3
                }
            }
        }
        return config
    
    def get_gpu_info(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU"""
        try:
            result = subprocess.run([
                "nvidia-smi", 
                "--query-gpu=memory.total,memory.free,memory.used",
                "--format=csv,noheader,nounits"
            ], capture_output=True, text=True, check=True)
            
            total, free, used = map(int, result.stdout.strip().split(', '))
            return {
                'total_mb': total,
                'free_mb': free,
                'used_mb': used,
                'total_gb': total / 1024,
                'free_gb': free / 1024,
                'used_gb': used / 1024
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
            return None
    
    def check_cache_dir(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫–µ—à–∞"""
        print(f"üìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {self.cache_dir}")
        
        if not self.cache_dir.exists():
            print("üìÅ –°–æ–∑–¥–∞–Ω–∏–µ –∫–µ—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
        if not os.access(self.cache_dir, os.R_OK | os.W_OK):
            print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∞–≤ –¥–ª—è –∫–µ—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
        cached_models = []
        for item in self.cache_dir.iterdir():
            if item.is_dir() and item.name.startswith('models--'):
                model_name = item.name.replace('models--', '').replace('--', '/')
                cached_models.append(model_name)
        
        if cached_models:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(cached_models)} –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:")
            for model in cached_models:
                print(f"   ‚Ä¢ {model}")
        else:
            print("üì• –ö–µ—à –ø—É—Å—Ç, –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏")
        
        return True
    
    def recommend_mode(self):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        gpu_info = self.get_gpu_info()
        if not gpu_info:
            return "transformers", "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å GPU –ø–∞–º—è—Ç—å"
        
        free_gb = gpu_info['free_gb']
        total_gb = gpu_info['total_gb']
        
        print(f"üìä GPU –ø–∞–º—è—Ç—å: {free_gb:.1f} GB —Å–≤–æ–±–æ–¥–Ω–æ –∏–∑ {total_gb:.1f} GB")
        
        if free_gb >= 8:
            return "vllm", f"–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è vLLM —Ä–µ–∂–∏–º–∞ ({free_gb:.1f} GB —Å–≤–æ–±–æ–¥–Ω–æ)"
        elif free_gb >= 4:
            return "transformers", f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Transformers —Ä–µ–∂–∏–º ({free_gb:.1f} GB —Å–≤–æ–±–æ–¥–Ω–æ)"
        else:
            return "transformers", f"–¢–æ–ª—å–∫–æ Transformers —Ä–µ–∂–∏–º –≤–æ–∑–º–æ–∂–µ–Ω ({free_gb:.1f} GB —Å–≤–æ–±–æ–¥–Ω–æ)"
    
    def start_transformers_mode(self, model_name=None):
        """–ó–∞–ø—É—Å–∫ Transformers —Ä–µ–∂–∏–º–∞"""
        if not model_name:
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            model_name = "rednote-hilab/dots.ocr"
        
        print(f"üöÄ –ó–∞–ø—É—Å–∫ Transformers —Ä–µ–∂–∏–º–∞ —Å –º–æ–¥–µ–ª—å—é: {model_name}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –∑–∞–ø—É—Å–∫–∞
        script_content = f'''#!/usr/bin/env python3
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from dots_ocr_transformers_8bit import DotsOCRTransformers, app
import threading

def load_model():
    ocr_model = DotsOCRTransformers()
    ocr_model.model_name = "{model_name}"
    ocr_model.load_model()
    app.ocr_model = ocr_model

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ Transformers —Ä–µ–∂–∏–º–∞")
    print(f"üì¶ –ú–æ–¥–µ–ª—å: {model_name}")
    print(f"üìÅ –ö–µ—à: {os.path.expanduser('~/.cache/huggingface/hub')}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–µ
    model_thread = threading.Thread(target=load_model)
    model_thread.daemon = True
    model_thread.start()
    
    # –ó–∞–ø—É—Å–∫ Flask —Å–µ—Ä–≤–µ—Ä–∞
    app.run(host='0.0.0.0', port=8000, debug=False)
'''
        
        with open("run_transformers_mode.py", "w", encoding="utf-8") as f:
            f.write(script_content)
        
        print("‚úÖ Transformers —Ä–µ–∂–∏–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        print("üåê –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É 8000...")
        print("üì° API: http://localhost:8000")
        
        self.current_mode = "transformers"
        return True
    
    def start_vllm_mode(self, models=None):
        """–ó–∞–ø—É—Å–∫ vLLM —Ä–µ–∂–∏–º–∞ —Å –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫–µ—à–µ–π"""
        if not models:
            models = ["rednote-hilab/dots.ocr"]
        
        print(f"üöÄ –ó–∞–ø—É—Å–∫ vLLM —Ä–µ–∂–∏–º–∞ —Å –º–æ–¥–µ–ª—è–º–∏: {', '.join(models)}")
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        self.stop_all_containers()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—É—Ç–µ–π –¥–ª—è –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        cache_path = str(self.cache_dir).replace('\\', '/')
        
        started_containers = []
        
        for model_name in models:
            if model_name not in self.models_config["vllm_models"]:
                print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ vLLM —Ä–µ–∂–∏–º–µ")
                continue
            
            model_config = self.models_config["vllm_models"][model_name]
            container_name = model_config["container_name"]
            port = model_config["port"]
            vllm_params = model_config["vllm_params"]
            
            print(f"üîÑ –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –¥–ª—è {model_config['name']}...")
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã Docker
            docker_cmd = [
                "docker", "run", "-d",
                "--gpus", "all",
                "--name", container_name,
                "--restart", "unless-stopped",
                "-p", f"{port}:8000",
                # –ö–†–ò–¢–ò–ß–ù–û: –ú–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–µ—à–∞ —Å –ø—Ä–∞–≤–∞–º–∏ –Ω–∞ —á—Ç–µ–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å
                "-v", f"{cache_path}:/root/.cache/huggingface/hub",
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                "-v", f"{cache_path}:/home/vllm/.cache/huggingface/hub",
                "--shm-size=8g",
                # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∫–µ—à–∞
                "-e", f"HF_HOME=/root/.cache/huggingface",
                "-e", f"TRANSFORMERS_CACHE=/root/.cache/huggingface/hub",
                "-e", f"HF_HUB_CACHE=/root/.cache/huggingface/hub",
                # –û–±—Ä–∞–∑ vLLM
                "vllm/vllm-openai:latest",
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã vLLM
                "--model", model_name,
                "--host", "0.0.0.0",
                "--port", "8000",
                "--trust-remote-code",
                "--max-model-len", str(vllm_params["max_model_len"]),
                "--gpu-memory-utilization", str(vllm_params["gpu_memory_utilization"]),
                "--dtype", vllm_params["dtype"],
                "--disable-log-requests"
            ]
            
            if vllm_params["enforce_eager"]:
                docker_cmd.append("--enforce-eager")
            
            try:
                result = subprocess.run(docker_cmd, check=True, capture_output=True, text=True)
                print(f"‚úÖ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä {container_name} –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {port}")
                started_containers.append({
                    "name": container_name,
                    "model": model_name,
                    "port": port,
                    "config": model_config
                })
            except subprocess.CalledProcessError as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ {container_name}: {e}")
                print(f"‚ùå Stderr: {e.stderr}")
        
        if started_containers:
            self.running_containers = started_containers
            self.current_mode = "vllm"
            
            print(f"\nüéâ vLLM —Ä–µ–∂–∏–º –∑–∞–ø—É—â–µ–Ω —Å {len(started_containers)} –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º–∏:")
            for container in started_containers:
                print(f"   ‚Ä¢ {container['config']['name']}: http://localhost:{container['port']}")
            
            print(f"\nüìÅ –ö–µ—à –º–æ–¥–µ–ª–µ–π –ø—Ä–∏–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω: {cache_path}")
            print("üíæ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –º–æ–≥—É—Ç —á–∏—Ç–∞—Ç—å –∏ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –≤ –∫–µ—à")
            
            return True
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞")
            return False
    
    def stop_all_containers(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
        print("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞
        container_names = []
        for model_config in self.models_config["vllm_models"].values():
            container_names.append(model_config["container_name"])
        
        for container_name in container_names:
            try:
                subprocess.run(["docker", "stop", container_name], 
                             check=False, capture_output=True)
                subprocess.run(["docker", "rm", container_name], 
                             check=False, capture_output=True)
            except:
                pass
        
        self.running_containers = []
    
    def wait_for_containers(self, timeout=300):
        """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤"""
        if not self.running_containers:
            return False
        
        print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤...")
        
        ready_containers = []
        start_time = time.time()
        
        while len(ready_containers) < len(self.running_containers) and (time.time() - start_time) < timeout:
            for container in self.running_containers:
                if container["name"] in [c["name"] for c in ready_containers]:
                    continue
                
                try:
                    response = requests.get(f"http://localhost:{container['port']}/health", timeout=5)
                    if response.status_code == 200:
                        print(f"‚úÖ {container['config']['name']} –≥–æ—Ç–æ–≤")
                        ready_containers.append(container)
                except:
                    pass
            
            if len(ready_containers) < len(self.running_containers):
                time.sleep(10)
        
        if len(ready_containers) == len(self.running_containers):
            print("üéâ –í—Å–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –≥–æ—Ç–æ–≤—ã!")
            return True
        else:
            print(f"‚ö†Ô∏è –ì–æ—Ç–æ–≤–æ {len(ready_containers)} –∏–∑ {len(self.running_containers)} –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤")
            return False
    
    def show_status(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å"""
        print("\nüìä –¢–ï–ö–£–©–ò–ô –°–¢–ê–¢–£–°")
        print("=" * 30)
        
        # GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        gpu_info = self.get_gpu_info()
        if gpu_info:
            print(f"üéÆ GPU –ø–∞–º—è—Ç—å: {gpu_info['used_gb']:.1f}/{gpu_info['total_gb']:.1f} GB")
            print(f"   –°–≤–æ–±–æ–¥–Ω–æ: {gpu_info['free_gb']:.1f} GB")
        
        # –ö–µ—à –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print(f"üìÅ –ö–µ—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.cache_dir}")
        if self.cache_dir.exists():
            cache_size = sum(f.stat().st_size for f in self.cache_dir.rglob('*') if f.is_file())
            print(f"üíæ –†–∞–∑–º–µ—Ä –∫–µ—à–∞: {cache_size / (1024**3):.2f} GB")
        
        # –¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º
        print(f"üîß –†–µ–∂–∏–º: {self.current_mode or '–ù–µ –∑–∞–ø—É—â–µ–Ω'}")
        
        # –ó–∞–ø—É—â–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
        if self.running_containers:
            print(f"üê≥ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã ({len(self.running_containers)}):")
            for container in self.running_containers:
                print(f"   ‚Ä¢ {container['config']['name']}: http://localhost:{container['port']}")
        else:
            print("üê≥ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã: –ù–µ –∑–∞–ø—É—â–µ–Ω—ã")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    selector = ModelModeSelector()
    
    print("üéØ –°–ï–õ–ï–ö–¢–û–† –†–ï–ñ–ò–ú–ê –†–ê–ë–û–¢–´ –ú–û–î–ï–õ–ï–ô")
    print("=" * 40)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞
    if not selector.check_cache_dir():
        print("‚ùå –ü—Ä–æ–±–ª–µ–º—ã —Å –∫–µ—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π")
        return
    
    while True:
        print("\nüîß –í–´–ë–ï–†–ò–¢–ï –î–ï–ô–°–¢–í–ò–ï:")
        print("1. üìä –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
        print("2. ü§ñ –ó–∞–ø—É—Å—Ç–∏—Ç—å Transformers —Ä–µ–∂–∏–º")
        print("3. üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å vLLM —Ä–µ–∂–∏–º")
        print("4. üõë –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã")
        print("5. ‚è≥ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤")
        print("6. üìÅ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–µ—à–µ–º")
        print("0. ‚ùå –í—ã—Ö–æ–¥")
        
        choice = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä: ").strip()
        
        if choice == "1":
            selector.show_status()
            mode, reason = selector.recommend_mode()
            print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {mode.upper()} —Ä–µ–∂–∏–º")
            print(f"   –ü—Ä–∏—á–∏–Ω–∞: {reason}")
            
        elif choice == "2":
            print("\nü§ñ TRANSFORMERS –†–ï–ñ–ò–ú")
            print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
            for i, (model_name, config) in enumerate(selector.models_config["transformers_models"].items(), 1):
                print(f"{i}. {config['name']} ({config['size_gb']} GB, 8-bit: {config['memory_8bit_gb']} GB)")
            
            model_choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (Enter –¥–ª—è dots.ocr): ").strip()
            if model_choice.isdigit():
                model_names = list(selector.models_config["transformers_models"].keys())
                if 1 <= int(model_choice) <= len(model_names):
                    selected_model = model_names[int(model_choice) - 1]
                else:
                    selected_model = "rednote-hilab/dots.ocr"
            else:
                selected_model = "rednote-hilab/dots.ocr"
            
            selector.start_transformers_mode(selected_model)
            
        elif choice == "3":
            print("\nüöÄ vLLM –†–ï–ñ–ò–ú")
            print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
            for i, (model_name, config) in enumerate(selector.models_config["vllm_models"].items(), 1):
                print(f"{i}. {config['name']} ({config['size_gb']} GB, —Ç—Ä–µ–±—É–µ—Ç {config['memory_required_gb']} GB)")
            
            models_input = input("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, Enter –¥–ª—è dots.ocr): ").strip()
            if models_input:
                model_indices = [int(x.strip()) for x in models_input.split(',') if x.strip().isdigit()]
                model_names = list(selector.models_config["vllm_models"].keys())
                selected_models = [model_names[i-1] for i in model_indices if 1 <= i <= len(model_names)]
            else:
                selected_models = ["rednote-hilab/dots.ocr"]
            
            if selector.start_vllm_mode(selected_models):
                selector.wait_for_containers()
            
        elif choice == "4":
            selector.stop_all_containers()
            selector.current_mode = None
            print("‚úÖ –í—Å–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
            
        elif choice == "5":
            if selector.running_containers:
                selector.wait_for_containers()
            else:
                print("‚ùå –ù–µ—Ç –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤")
                
        elif choice == "6":
            print(f"\nüìÅ –£–ü–†–ê–í–õ–ï–ù–ò–ï –ö–ï–®–ï–ú")
            print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {selector.cache_dir}")
            print("1. –ü–æ–∫–∞–∑–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–µ—à–∞")
            print("2. –û—á–∏—Å—Ç–∏—Ç—å –∫–µ—à")
            
            cache_choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ: ").strip()
            if cache_choice == "1":
                if selector.cache_dir.exists():
                    for item in selector.cache_dir.iterdir():
                        if item.is_dir():
                            size = sum(f.stat().st_size for f in item.rglob('*') if f.is_file())
                            print(f"   üì¶ {item.name}: {size / (1024**3):.2f} GB")
                else:
                    print("üìÅ –ö–µ—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            elif cache_choice == "2":
                confirm = input("‚ö†Ô∏è –£–¥–∞–ª–∏—Ç—å –≤–µ—Å—å –∫–µ—à? (yes/no): ").strip().lower()
                if confirm == "yes":
                    import shutil
                    if selector.cache_dir.exists():
                        shutil.rmtree(selector.cache_dir)
                        selector.cache_dir.mkdir(parents=True, exist_ok=True)
                        print("‚úÖ –ö–µ—à –æ—á–∏—â–µ–Ω")
                    else:
                        print("üìÅ –ö–µ—à —É–∂–µ –ø—É—Å—Ç")
                        
        elif choice == "0":
            selector.stop_all_containers()
            break
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")

if __name__ == "__main__":
    main()