{
  "model_loader_patches": {
    "force_eager_attention": true,
    "disable_flash_attention": true,
    "disable_quantization": true,
    "enable_cuda_recovery": true,
    "max_retries": 3,
    "fallback_to_cpu": true
  },
  "error_handling": {
    "cuda_device_assert": {
      "action": "clear_cache_and_retry",
      "max_retries": 2,
      "fallback": "cpu_mode"
    },
    "flash_attention_error": {
      "action": "disable_flash_attention",
      "fallback_attention": "eager"
    },
    "quantization_error": {
      "action": "disable_quantization",
      "fallback_precision": "fp16"
    },
    "transformers_version_error": {
      "action": "use_compatible_model",
      "fallback_model": "qwen3_vl_2b"
    }
  }
}