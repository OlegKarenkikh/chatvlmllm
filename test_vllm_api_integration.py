#!/usr/bin/env python3
"""
–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å vLLM API –º–æ–¥–µ–ª—è–º–∏
"""

import requests
import json
import time
import base64
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import io

class VLLMAPITester:
    def __init__(self):
        self.vllm_endpoints = {
            "dots.ocr": "http://localhost:8000",
            "Qwen3-VL-2B": "http://localhost:8010", 
            "Qwen2-VL-2B": "http://localhost:8011"
        }
        self.test_image_path = "test_vllm_integration.png"
        self.results = {}
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        self.create_test_image()
    
    def create_test_image(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è vLLM"""
        img = Image.new('RGB', (800, 600), color='white')
        draw = ImageDraw.Draw(img)
        
        try:
            font = ImageFont.truetype("arial.ttf", 20)
            font_title = ImageFont.truetype("arialbd.ttf", 24)
        except:
            font = font_title = ImageFont.load_default()
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        draw.text((50, 30), "vLLM INTEGRATION TEST", fill='black', font=font_title)
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content = """
–¢–ï–°–¢–û–í–´–ô –î–û–ö–£–ú–ï–ù–¢ –î–õ–Ø vLLM

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å vLLM API.

–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
‚Ä¢ –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: 24.01.2026
‚Ä¢ –ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞: VLM-2026-001
‚Ä¢ –°—Ç–∞—Ç—É—Å: –ê–ö–¢–ò–í–ù–´–ô

–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç:
‚Ä¢ –†—É—Å—Å–∫–∏–π: –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!
‚Ä¢ English: Hello, world!
‚Ä¢ Fran√ßais: Bonjour le monde!

–ß–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:
‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: 42
‚Ä¢ –¶–µ–Ω–∞: 1,234.56 ‚ÇΩ
‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç: 95.7%

–ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
üìß Email: test@vllm-integration.com
üìû –¢–µ–ª–µ—Ñ–æ–Ω: +7 (999) 123-45-67
üåê –°–∞–π—Ç: https://vllm-test.example.com
        """
        
        draw.multiline_text((50, 80), content.strip(), fill='black', font=font, spacing=5)
        
        # –†–∞–º–∫–∞
        draw.rectangle([(30, 20), (770, 580)], outline='black', width=2)
        
        img.save(self.test_image_path)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {self.test_image_path}")
    
    def encode_image_base64(self, image_path: str) -> str:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    
    def test_vllm_health(self, model_name: str, endpoint: str) -> dict:
        """–¢–µ—Å—Ç health endpoint vLLM –º–æ–¥–µ–ª–∏"""
        try:
            response = requests.get(f"{endpoint}/health", timeout=10)
            
            if response.status_code == 200:
                return {
                    "status": "healthy",
                    "response_time": response.elapsed.total_seconds(),
                    "details": response.json() if response.content else {}
                }
            else:
                return {
                    "status": "unhealthy",
                    "http_code": response.status_code,
                    "error": response.text[:200]
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def test_vllm_models_endpoint(self, model_name: str, endpoint: str) -> dict:
        """–¢–µ—Å—Ç /v1/models endpoint"""
        try:
            response = requests.get(f"{endpoint}/v1/models", timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                models = data.get('data', [])
                
                return {
                    "status": "success",
                    "models_count": len(models),
                    "models": [m.get('id', 'unknown') for m in models],
                    "response_time": response.elapsed.total_seconds()
                }
            else:
                return {
                    "status": "error",
                    "http_code": response.status_code,
                    "error": response.text[:200]
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def test_vllm_text_completion(self, model_name: str, endpoint: str) -> dict:
        """–¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ completion"""
        try:
            payload = {
                "model": model_name,
                "messages": [
                    {"role": "user", "content": "Hello! How are you today?"}
                ],
                "max_tokens": 100,
                "temperature": 0.7
            }
            
            start_time = time.time()
            response = requests.post(
                f"{endpoint}/v1/chat/completions",
                json=payload,
                timeout=60
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                
                if 'choices' in data and len(data['choices']) > 0:
                    message = data['choices'][0].get('message', {})
                    content = message.get('content', '')
                    
                    return {
                        "status": "success",
                        "response": content,
                        "response_length": len(content),
                        "processing_time": processing_time,
                        "usage": data.get('usage', {})
                    }
                else:
                    return {
                        "status": "error",
                        "error": "No choices in response",
                        "response": data
                    }
            else:
                return {
                    "status": "error",
                    "http_code": response.status_code,
                    "error": response.text[:300]
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def test_vllm_vision_completion(self, model_name: str, endpoint: str) -> dict:
        """–¢–µ—Å—Ç vision completion —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"""
        try:
            # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_base64 = self.encode_image_base64(self.test_image_path)
            
            payload = {
                "model": model_name,
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Please extract all text from this image and describe what you see."
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.1
            }
            
            start_time = time.time()
            response = requests.post(
                f"{endpoint}/v1/chat/completions",
                json=payload,
                timeout=120  # –ë–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è vision –∑–∞–¥–∞—á
            )
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                
                if 'choices' in data and len(data['choices']) > 0:
                    message = data['choices'][0].get('message', {})
                    content = message.get('content', '')
                    
                    # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ OCR
                    expected_keywords = [
                        'vLLM', 'INTEGRATION', 'TEST', '–¢–ï–°–¢–û–í–´–ô', '–î–û–ö–£–ú–ï–ù–¢',
                        '24.01.2026', 'VLM-2026-001', '–ê–ö–¢–ò–í–ù–´–ô',
                        'test@vllm-integration.com', '+7 (999) 123-45-67'
                    ]
                    
                    found_keywords = sum(1 for keyword in expected_keywords 
                                       if keyword.lower() in content.lower())
                    accuracy = found_keywords / len(expected_keywords)
                    
                    return {
                        "status": "success",
                        "response": content,
                        "response_length": len(content),
                        "processing_time": processing_time,
                        "ocr_accuracy": accuracy,
                        "found_keywords": found_keywords,
                        "total_keywords": len(expected_keywords),
                        "usage": data.get('usage', {})
                    }
                else:
                    return {
                        "status": "error",
                        "error": "No choices in response",
                        "response": data
                    }
            else:
                return {
                    "status": "error",
                    "http_code": response.status_code,
                    "error": response.text[:300]
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def test_single_model(self, model_name: str, endpoint: str) -> dict:
        """–ü–æ–ª–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
        print(f"üåê Endpoint: {endpoint}")
        print("-" * 50)
        
        model_results = {
            "model_name": model_name,
            "endpoint": endpoint,
            "tests": {}
        }
        
        # 1. Health check
        print("   1Ô∏è‚É£ Health check...")
        health_result = self.test_vllm_health(model_name, endpoint)
        model_results["tests"]["health"] = health_result
        
        if health_result["status"] == "healthy":
            print(f"      ‚úÖ –ú–æ–¥–µ–ª—å –∑–¥–æ—Ä–æ–≤–∞ ({health_result['response_time']:.2f}—Å)")
        else:
            print(f"      ‚ùå –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {health_result.get('error', 'Unknown error')}")
            return model_results
        
        # 2. Models endpoint
        print("   2Ô∏è‚É£ Models endpoint...")
        models_result = self.test_vllm_models_endpoint(model_name, endpoint)
        model_results["tests"]["models"] = models_result
        
        if models_result["status"] == "success":
            print(f"      ‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {models_result['models_count']}")
        else:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ models endpoint: {models_result.get('error', 'Unknown')}")
        
        # 3. Text completion
        print("   3Ô∏è‚É£ Text completion...")
        text_result = self.test_vllm_text_completion(model_name, endpoint)
        model_results["tests"]["text_completion"] = text_result
        
        if text_result["status"] == "success":
            response_preview = text_result["response"][:100] + "..." if len(text_result["response"]) > 100 else text_result["response"]
            print(f"      ‚úÖ –¢–µ–∫—Å—Ç: {response_preview}")
            print(f"      ‚è±Ô∏è –í—Ä–µ–º—è: {text_result['processing_time']:.2f}—Å")
        else:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ text completion: {text_result.get('error', 'Unknown')}")
        
        # 4. Vision completion (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
        print("   4Ô∏è‚É£ Vision completion...")
        vision_result = self.test_vllm_vision_completion(model_name, endpoint)
        model_results["tests"]["vision_completion"] = vision_result
        
        if vision_result["status"] == "success":
            print(f"      ‚úÖ Vision OCR —Ç–æ—á–Ω–æ—Å—Ç—å: {vision_result['ocr_accuracy']:.1%}")
            print(f"      ‚è±Ô∏è –í—Ä–µ–º—è: {vision_result['processing_time']:.2f}—Å")
            print(f"      üìù –ù–∞–π–¥–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {vision_result['found_keywords']}/{vision_result['total_keywords']}")
        else:
            print(f"      ‚ùå –û—à–∏–±–∫–∞ vision completion: {vision_result.get('error', 'Unknown')}")
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        successful_tests = sum(1 for test in model_results["tests"].values() 
                             if test.get("status") in ["success", "healthy"])
        total_tests = len(model_results["tests"])
        success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        model_results["success_rate"] = success_rate
        model_results["successful_tests"] = successful_tests
        model_results["total_tests"] = total_tests
        
        print(f"   üìä –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%} ({successful_tests}/{total_tests})")
        
        return model_results
    
    def run_all_tests(self):
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï vLLM API –ò–ù–¢–ï–ì–†–ê–¶–ò–ò")
        print("=" * 50)
        
        all_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_image": self.test_image_path,
            "models": {},
            "summary": {
                "total_models": len(self.vllm_endpoints),
                "available_models": 0,
                "working_models": 0,
                "avg_success_rate": 0
            }
        }
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model_name, endpoint in self.vllm_endpoints.items():
            model_result = self.test_single_model(model_name, endpoint)
            all_results["models"][model_name] = model_result
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            if model_result["tests"]["health"]["status"] == "healthy":
                all_results["summary"]["available_models"] += 1
                
                if model_result["success_rate"] >= 0.75:  # 75% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
                    all_results["summary"]["working_models"] += 1
        
        # –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        success_rates = [result["success_rate"] for result in all_results["models"].values()]
        if success_rates:
            all_results["summary"]["avg_success_rate"] = sum(success_rates) / len(success_rates)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = f"vllm_api_test_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        self.generate_final_report(all_results, results_file)
        
        return all_results
    
    def generate_final_report(self, results: dict, results_file: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print(f"\nüèÜ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ vLLM API –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
        print("=" * 50)
        
        summary = results["summary"]
        
        print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {summary['total_models']}")
        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã—Ö: {summary['available_models']}")
        print(f"   –†–∞–±–æ—Ç–∞—é—â–∏—Ö: {summary['working_models']}")
        print(f"   –°—Ä–µ–¥–Ω—è—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {summary['avg_success_rate']:.1%}")
        
        print(f"\nüìã –î–µ—Ç–∞–ª–∏ –ø–æ –º–æ–¥–µ–ª—è–º:")
        for model_name, result in results["models"].items():
            status_icon = "‚úÖ" if result["success_rate"] >= 0.75 else "‚ö†Ô∏è" if result["success_rate"] >= 0.5 else "‚ùå"
            print(f"   {status_icon} {model_name}: {result['success_rate']:.1%} ({result['successful_tests']}/{result['total_tests']})")
            
            # –î–µ—Ç–∞–ª–∏ –ø–æ —Ç–µ—Å—Ç–∞–º
            for test_name, test_result in result["tests"].items():
                test_status = test_result.get("status", "unknown")
                test_icon = "‚úÖ" if test_status in ["success", "healthy"] else "‚ùå"
                print(f"      {test_icon} {test_name}: {test_status}")
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        
        if summary["working_models"] == summary["total_models"]:
            print("   üéâ –í—Å–µ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –æ—Ç–ª–∏—á–Ω–æ! –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É.")
        elif summary["working_models"] >= summary["total_models"] * 0.5:
            print("   ‚úÖ –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç. –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é.")
            print("   üîß –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.")
        else:
            print("   ‚ö†Ô∏è –ú–Ω–æ–≥–æ –ø—Ä–æ–±–ª–µ–º —Å –º–æ–¥–µ–ª—è–º–∏. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞.")
            print("   üõ†Ô∏è –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–∞–ø—É—Å–∫ vLLM –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.")
        
        if summary["available_models"] == 0:
            print("   üö® –ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞!")
            print("   üîß –ó–∞–ø—É—Å—Ç–∏—Ç–µ –º–æ–¥–µ–ª–∏: python launch_working_models.py")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    tester = VLLMAPITester()
    
    print("üîç –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é vLLM API...")
    print(f"üìÅ –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {tester.test_image_path}")
    print(f"üéØ –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {list(tester.vllm_endpoints.keys())}")
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
    results = tester.run_all_tests()
    
    # –û—á–∏—Å—Ç–∫–∞
    if Path(tester.test_image_path).exists():
        Path(tester.test_image_path).unlink()
        print(f"üßπ –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É–¥–∞–ª–µ–Ω–æ")
    
    print(f"\nüéØ vLLM API –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    
    # –í–æ–∑–≤—Ä–∞—Ç –∫–æ–¥–∞ –≤—ã—Ö–æ–¥–∞
    summary = results["summary"]
    if summary["working_models"] >= summary["total_models"] * 0.5:
        return 0  # –£—Å–ø–µ—Ö
    else:
        return 1  # –ü—Ä–æ–±–ª–µ–º—ã

if __name__ == "__main__":
    import sys
    sys.exit(main())