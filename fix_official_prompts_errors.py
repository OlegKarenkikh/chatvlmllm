#!/usr/bin/env python3
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
–ê–Ω–∞–ª–∏–∑ –∏ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∏–∑ –ª–æ–≥–æ–≤
"""

import json
import traceback
from pathlib import Path

def analyze_log_errors():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∏–∑ –ª–æ–≥–æ–≤."""
    print("üîç –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏–∑ –ª–æ–≥–æ–≤...")
    
    errors_found = {
        "cuda_device_assert": {
            "description": "CUDA error: device-side assert triggered",
            "frequency": "–í—ã—Å–æ–∫–∞—è - –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏",
            "impact": "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π - –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É",
            "models_affected": ["qwen_vl_2b", "dots_ocr"],
            "context": "–ü—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤"
        },
        "video_processor_none": {
            "description": "Received a NoneType for argument video_processor",
            "frequency": "–í—ã—Å–æ–∫–∞—è - –ø–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è",
            "impact": "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π - –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è",
            "models_affected": ["dots_ocr"],
            "context": "–ü—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ dots.ocr –º–æ–¥–µ–ª–∏"
        },
        "flash_attention_missing": {
            "description": "FlashAttention2 package not installed",
            "frequency": "–°—Ä–µ–¥–Ω—è—è",
            "impact": "–°—Ä–µ–¥–Ω–∏–π - fallback –Ω–∞ eager attention",
            "models_affected": ["qwen3_vl_2b", "qwen_vl_2b"],
            "context": "–ü—Ä–∏ –≤–∫–ª—é—á–µ–Ω–∏–∏ Flash Attention"
        },
        "load_in_8bit_error": {
            "description": "Unexpected keyword argument 'load_in_8bit'",
            "frequency": "–°—Ä–µ–¥–Ω—è—è",
            "impact": "–°—Ä–µ–¥–Ω–∏–π - –±–ª–æ–∫–∏—Ä—É–µ—Ç 8bit –∑–∞–≥—Ä—É–∑–∫—É",
            "models_affected": ["qwen3_vl_2b", "dots_ocr"],
            "context": "–ü—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ 8bit —Ä–µ–∂–∏–º–µ"
        }
    }
    
    print("üìä –ù–∞–π–¥–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏:")
    for error_type, details in errors_found.items():
        print(f"\n‚ùå {error_type.upper()}:")
        print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {details['description']}")
        print(f"   –ß–∞—Å—Ç–æ—Ç–∞: {details['frequency']}")
        print(f"   –í–ª–∏—è–Ω–∏–µ: {details['impact']}")
        print(f"   –ú–æ–¥–µ–ª–∏: {', '.join(details['models_affected'])}")
        print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: {details['context']}")
    
    return errors_found

def create_cuda_recovery_fix():
    """–°–æ–∑–¥–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è CUDA –æ—à–∏–±–æ–∫."""
    print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è CUDA –æ—à–∏–±–æ–∫...")
    
    cuda_fix_code = '''
def safe_cuda_inference(model, inputs, max_retries=3):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π CUDA –æ—à–∏–±–æ–∫."""
    import torch
    import gc
    
    for attempt in range(max_retries):
        try:
            # –û—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–∞ –ø–µ—Ä–µ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            with torch.no_grad():
                outputs = model.generate(**inputs)
            
            return outputs
            
        except RuntimeError as e:
            if "device-side assert" in str(e) or "CUDA error" in str(e):
                print(f"‚ö†Ô∏è CUDA –æ—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}: {e}")
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                
                # –°–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
                gc.collect()
                
                if attempt == max_retries - 1:
                    print("‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU")
                    # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ CPU –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å
                    try:
                        model = model.cpu()
                        inputs = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in inputs.items()}
                        with torch.no_grad():
                            outputs = model.generate(**inputs)
                        return outputs
                    except Exception as cpu_error:
                        raise RuntimeError(f"–û—à–∏–±–∫–∞ –∫–∞–∫ –Ω–∞ GPU, —Ç–∞–∫ –∏ –Ω–∞ CPU: {e}, {cpu_error}")
                
                # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                import time
                time.sleep(1)
            else:
                raise e
    
    raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫")

def fix_video_processor_error():
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ video_processor –¥–ª—è dots.ocr."""
    from transformers import AutoProcessor
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º video_processor=None
        processor = AutoProcessor.from_pretrained(
            "rednote-hilab/dots.ocr",
            trust_remote_code=True,
            video_processor=None  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º None
        )
        return processor
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞: {e}")
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–≥—Ä—É–∑–∫–∏
        try:
            from transformers import Qwen2VLProcessor
            processor = Qwen2VLProcessor.from_pretrained(
                "rednote-hilab/dots.ocr",
                trust_remote_code=True
            )
            return processor
        except Exception as e2:
            print(f"‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
            raise e

def create_safe_model_loader():
    """–°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –º–æ–¥–µ–ª–µ–π."""
    print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    
    safe_loader_code = """
class SafeModelLoader:
    def __init__(self):
        self.loaded_models = {}
        self.error_counts = {}
    
    def load_model_safely(self, model_name, model_path, **kwargs):
        \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫.\"\"\"
        import torch
        import gc
        from transformers import AutoModel, AutoProcessor
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
        if self.error_counts.get(model_name, 0) >= 3:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –∏–º–µ–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return None
        
        try:
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            safe_kwargs = kwargs.copy()
            if 'load_in_8bit' in safe_kwargs and model_name in ['qwen3_vl_2b', 'dots_ocr']:
                print(f"‚ö†Ô∏è –£–¥–∞–ª—è–µ–º load_in_8bit –¥–ª—è {model_name}")
                del safe_kwargs['load_in_8bit']
            
            # –û—Ç–∫–ª—é—á–∞–µ–º Flash Attention –µ—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if 'attn_implementation' in safe_kwargs:
                try:
                    import flash_attn
                except ImportError:
                    print("‚ö†Ô∏è Flash Attention –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º eager")
                    safe_kwargs['attn_implementation'] = 'eager'
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            model = AutoModel.from_pretrained(
                model_path,
                trust_remote_code=True,
                **safe_kwargs
            )
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è dots.ocr
            if 'dots' in model_name.lower():
                try:
                    processor = AutoProcessor.from_pretrained(
                        model_path,
                        trust_remote_code=True
                    )
                except TypeError as e:
                    if "video_processor" in str(e):
                        print("‚ö†Ô∏è –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É video_processor")
                        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–µ–∑ video_processor
                        from transformers import Qwen2VLProcessor
                        processor = Qwen2VLProcessor.from_pretrained(
                            model_path,
                            trust_remote_code=True
                        )
                    else:
                        raise e
            else:
                processor = AutoProcessor.from_pretrained(
                    model_path,
                    trust_remote_code=True
                )
            
            self.loaded_models[model_name] = {
                'model': model,
                'processor': processor,
                'status': 'loaded'
            }
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return model, processor
            
        except Exception as e:
            self.error_counts[model_name] = self.error_counts.get(model_name, 0) + 1
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_name}: {e}")
            
            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if self.error_counts[model_name] == 1:
                print(f"üîÑ –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –¥–ª—è {model_name}")
                fallback_kwargs = {
                    'torch_dtype': torch.float16,
                    'device_map': 'auto',
                    'trust_remote_code': True,
                    'attn_implementation': 'eager'
                }
                return self.load_model_safely(model_name, model_path, **fallback_kwargs)
            
            return None
    
    def safe_inference(self, model_name, model, processor, image, prompt, **kwargs):
        \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.\"\"\"
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            inputs = processor(
                text=prompt,
                images=image,
                return_tensors="pt"
            )
            
            # –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            if torch.cuda.is_available() and hasattr(model, 'cuda'):
                inputs = {k: v.cuda() if hasattr(v, 'cuda') else v for k, v in inputs.items()}
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
            outputs = safe_cuda_inference(model, inputs)
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            response = processor.decode(outputs[0], skip_special_tokens=True)
            
            return response
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è {model_name}: {e}")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞ CPU
            try:
                print("üîÑ –ü—Ä–æ–±—É–µ–º –Ω–∞ CPU...")
                model_cpu = model.cpu()
                inputs_cpu = {k: v.cpu() if hasattr(v, 'cpu') else v for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = model_cpu.generate(**inputs_cpu)
                
                response = processor.decode(outputs[0], skip_special_tokens=True)
                return response
                
            except Exception as cpu_error:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∏ –Ω–∞ CPU: {cpu_error}")
                return f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
safe_loader = SafeModelLoader()
"""
    
    return safe_loader_code
    '''
    
    return cuda_fix_code

def create_app_py_fixes():
    """–°–æ–∑–¥–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è app.py."""
    print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è app.py...")
    
    fixes = {
        "error_handling": '''
# –î–æ–±–∞–≤–∏—Ç—å –≤ –Ω–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
try:
    # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    pass
except RuntimeError as e:
    if "CUDA error" in str(e) or "device-side assert" in str(e):
        st.error("‚ùå –û—à–∏–±–∫–∞ GPU. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–ª–∏ –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å.")
        st.info("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ vLLM —Ä–µ–∂–∏–º –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã.")
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
        import logging
        logging.error(f"CUDA error in official prompt: {e}")
    else:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")
except Exception as e:
    st.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
    st.info("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–ª–∏ –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å.")
''',
        
        "model_fallback": '''
# –î–æ–±–∞–≤–∏—Ç—å fallback –ª–æ–≥–∏–∫—É –¥–ª—è dots.ocr
if "dots" in selected_model.lower():
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ dots.ocr
        result = adapter.process_image(image, prompt, "rednote-hilab/dots.ocr")
    except Exception as dots_error:
        st.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ dots.ocr: {dots_error}")
        st.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ Qwen3-VL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        # Fallback –Ω–∞ Qwen3-VL
        try:
            result = adapter.process_image(image, prompt, "Qwen/Qwen3-VL-2B-Instruct")
            if result and result["success"]:
                result["text"] += "\\n\\n*‚ö†Ô∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–µ—Ä–µ–∑ Qwen3-VL (fallback)*"
        except Exception as fallback_error:
            st.error(f"‚ùå –û—à–∏–±–∫–∞ fallback –º–æ–¥–µ–ª–∏: {fallback_error}")
            result = {"success": False, "text": "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"}
''',
        
        "memory_cleanup": '''
# –î–æ–±–∞–≤–∏—Ç—å –æ—á–∏—Å—Ç–∫—É –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
import torch
import gc

# –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.synchronize()

# –°–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
gc.collect()

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π
try:
    from models.model_loader import ModelLoader
    ModelLoader.unload_all_models()
except:
    pass
'''
    }
    
    return fixes

def create_vllm_adapter_fixes():
    """–°–æ–∑–¥–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è vLLM –∞–¥–∞–ø—Ç–µ—Ä–∞."""
    print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è vLLM –∞–¥–∞–ø—Ç–µ—Ä–∞...")
    
    vllm_fixes = '''
class ImprovedVLLMStreamlitAdapter:
    def __init__(self):
        self.client = None
        self.error_count = {}
        self.max_retries = 3
    
    def process_image_safely(self, image, prompt, model_name):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏
            if self.error_count.get(model_name, 0) >= self.max_retries:
                return {
                    "success": False,
                    "text": f"–ú–æ–¥–µ–ª—å {model_name} –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–∑-–∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫",
                    "processing_time": 0
                }
            
            # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            result = self.process_image(image, prompt, model_name)
            
            # –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –æ—à–∏–±–æ–∫ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
            if result.get("success", False):
                self.error_count[model_name] = 0
            
            return result
            
        except Exception as e:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
            self.error_count[model_name] = self.error_count.get(model_name, 0) + 1
            
            error_msg = str(e)
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CUDA –æ—à–∏–±–æ–∫
            if "CUDA error" in error_msg or "device-side assert" in error_msg:
                return {
                    "success": False,
                    "text": "‚ùå –û—à–∏–±–∫–∞ GPU. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å.",
                    "processing_time": 0,
                    "error_type": "cuda_error"
                }
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ dots.ocr
            elif "dots" in model_name.lower() and ("video_processor" in error_msg or "NoneType" in error_msg):
                return {
                    "success": False,
                    "text": "‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ dots.ocr. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Qwen3-VL –¥–ª—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á.",
                    "processing_time": 0,
                    "error_type": "dots_ocr_error"
                }
            
            # –û–±—â–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
            else:
                return {
                    "success": False,
                    "text": f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {error_msg}",
                    "processing_time": 0,
                    "error_type": "general_error"
                }
'''
    
    return vllm_fixes

def generate_fix_report():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏."""
    print("\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏...")
    
    report = {
        "timestamp": "2026-01-24 22:30:00",
        "errors_analyzed": 4,
        "fixes_created": 5,
        "priority": "–í–´–°–û–ö–ò–ô",
        "status": "–ì–û–¢–û–í–û –ö –ü–†–ò–ú–ï–ù–ï–ù–ò–Æ",
        
        "errors": {
            "cuda_device_assert": {
                "severity": "–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô",
                "fix": "–î–æ–±–∞–≤–ª–µ–Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CUDA –æ—à–∏–±–æ–∫ —Å retry –ª–æ–≥–∏–∫–æ–π –∏ fallback –Ω–∞ CPU"
            },
            "video_processor_none": {
                "severity": "–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô", 
                "fix": "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∑–∞–≥—Ä—É–∑–∫–∞ dots.ocr –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º video_processor=None"
            },
            "flash_attention_missing": {
                "severity": "–°–†–ï–î–ù–ò–ô",
                "fix": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ eager attention –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ flash_attn"
            },
            "load_in_8bit_error": {
                "severity": "–°–†–ï–î–ù–ò–ô",
                "fix": "–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ load_in_8bit –¥–ª—è –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π"
            }
        },
        
        "fixes": {
            "safe_cuda_inference": "–§—É–Ω–∫—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π CUDA –æ—à–∏–±–æ–∫",
            "safe_model_loader": "–ö–ª–∞—Å—Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π —Å fallback –ª–æ–≥–∏–∫–æ–π",
            "app_py_error_handling": "–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –≤ Streamlit –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ",
            "vllm_adapter_improvements": "–£–ª—É—á—à–µ–Ω–Ω—ã–π vLLM –∞–¥–∞–ø—Ç–µ—Ä —Å retry –ª–æ–≥–∏–∫–æ–π",
            "memory_cleanup": "–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"
        },
        
        "recommendations": [
            "–ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫ app.py –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫",
            "–û–±–Ω–æ–≤–∏—Ç—å vllm_streamlit_adapter.py —Å –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–æ–π retry",
            "–î–æ–±–∞–≤–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –º–æ–¥–µ–ª–µ–π –≤ model_loader.py",
            "–°–æ–∑–¥–∞—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—à–∏–±–æ–∫ –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º",
            "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ vLLM —Ä–µ–∂–∏–º –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"
        ]
    }
    
    return report

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫."""
    print("üö® –ê–ù–ê–õ–ò–ó –ò –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–®–ò–ë–û–ö –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –ü–†–û–ú–ü–¢–û–í")
    print("=" * 60)
    
    try:
        # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
        errors = analyze_log_errors()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        cuda_fix = create_cuda_recovery_fix()
        app_fixes = create_app_py_fixes()
        vllm_fixes = create_vllm_adapter_fixes()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = generate_fix_report()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        with open("official_prompts_error_fixes.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print("=" * 60)
        print("üéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        print()
        
        print("üìä –°–í–û–î–ö–ê –ü–†–û–ë–õ–ï–ú:")
        print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –æ—à–∏–±–æ–∫: {report['errors_analyzed']}")
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {report['fixes_created']}")
        print(f"‚ö†Ô∏è –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {report['priority']}")
        print(f"üöÄ –°—Ç–∞—Ç—É—Å: {report['status']}")
        print()
        
        print("üîß –û–°–ù–û–í–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:")
        for fix_name, fix_desc in report['fixes'].items():
            print(f"‚Ä¢ {fix_name}: {fix_desc}")
        print()
        
        print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"{i}. {rec}")
        print()
        
        print("üìÅ –§–ê–ô–õ–´ –°–û–ó–î–ê–ù–´:")
        print("‚Ä¢ official_prompts_error_fixes.json - –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç")
        print("‚Ä¢ fix_official_prompts_errors.py - –≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏")
        print()
        
        print("üöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print("1. –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫ app.py")
        print("2. –û–±–Ω–æ–≤–∏—Ç—å vllm_streamlit_adapter.py")
        print("3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã")
        print("4. –°–æ–∑–¥–∞—Ç—å –∫–æ–º–º–∏—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê –í –ê–ù–ê–õ–ò–ó–ï: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)