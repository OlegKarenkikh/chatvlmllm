#!/usr/bin/env python3
"""GPU setup and verification script for ChatVLMLLM."""

import sys
import subprocess
from pathlib import Path

try:
    import torch
except ImportError:
    print("‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121")
    sys.exit(1)


def check_cuda():
    """Check CUDA availability and version."""
    print("=" * 60)
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞!")
        print("\n–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("1. –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã")
        print("2. PyTorch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ CUDA")
        print("3. GPU –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç CUDA")
        print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã: https://www.nvidia.com/Download/index.aspx")
        print("- –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å CUDA: pip install torch --index-url https://download.pytorch.org/whl/cu121")
        return False
    
    print(f"‚úì CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.version.cuda}")
    print(f"‚úì cuDNN –≤–µ—Ä—Å–∏—è: {torch.backends.cudnn.version()}")
    print(f"‚úì –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
    
    return True


def check_gpu_info():
    """Display detailed GPU information."""
    print("\n" + "=" * 60)
    print("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU")
    print("=" * 60)
    
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"\nGPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"  VRAM: {props.total_memory / 1024**3:.2f} GB")
        print(f"  Compute Capability: {props.major}.{props.minor}")
        print(f"  Multiprocessors: {props.multi_processor_count}")
        print(f"  CUDA Cores: ~{props.multi_processor_count * 128}")
        
        # Memory usage
        allocated = torch.cuda.memory_allocated(i) / 1024**3
        reserved = torch.cuda.memory_reserved(i) / 1024**3
        print(f"  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {allocated:.2f} GB")
        print(f"  –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {reserved:.2f} GB")
        print(f"  –°–≤–æ–±–æ–¥–Ω–æ: {(props.total_memory / 1024**3) - reserved:.2f} GB")


def check_flash_attention():
    """Check Flash Attention installation."""
    print("\n" + "=" * 60)
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ Flash Attention")
    print("=" * 60)
    
    try:
        import flash_attn
        print(f"‚úì Flash Attention —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        print(f"  –í–µ—Ä—Å–∏—è: {flash_attn.__version__}")
        
        # Test Flash Attention 2
        try:
            from flash_attn import flash_attn_func
            print("‚úì Flash Attention 2 –¥–æ—Å—Ç—É–ø–Ω–∞")
            return True
        except ImportError:
            print("‚ö†Ô∏è Flash Attention 2 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return False
            
    except ImportError:
        print("‚ùå Flash Attention –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        print("\n–£—Å—Ç–∞–Ω–æ–≤–∫–∞:")
        print("  pip install flash-attn --no-build-isolation")
        print("\n–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¢—Ä–µ–±—É–µ—Ç—Å—è:")
        print("  - CUDA 11.6+")
        print("  - PyTorch 1.12+")
        print("  - –ö–æ–º–ø–∏–ª—è—Ç–æ—Ä C++ (MSVC –Ω–∞ Windows, GCC –Ω–∞ Linux)")
        return False


def check_bitsandbytes():
    """Check BitsAndBytes for quantization."""
    print("\n" + "=" * 60)
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ BitsAndBytes (–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è)")
    print("=" * 60)
    
    try:
        import bitsandbytes as bnb
        print(f"‚úì BitsAndBytes —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        print(f"  –í–µ—Ä—Å–∏—è: {bnb.__version__}")
        
        # Check CUDA support
        if hasattr(bnb, 'cextension'):
            print("‚úì CUDA —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã")
            return True
        else:
            print("‚ö†Ô∏è CUDA —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
            return False
            
    except ImportError:
        print("‚ùå BitsAndBytes –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        print("\n–£—Å—Ç–∞–Ω–æ–≤–∫–∞:")
        print("  pip install bitsandbytes")
        print("\n–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
        print("  - INT8 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—ç–∫–æ–Ω–æ–º–∏—è 50% VRAM)")
        print("  - INT4 –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (—ç–∫–æ–Ω–æ–º–∏—è 75% VRAM)")
        return False


def benchmark_gpu():
    """Run simple GPU benchmark."""
    print("\n" + "=" * 60)
    print("–ë–µ–Ω—á–º–∞—Ä–∫ GPU")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("–ü—Ä–æ–ø—É—â–µ–Ω–æ: CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return
    
    print("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    
    # Matrix multiplication benchmark
    size = 4096
    device = torch.device("cuda:0")
    
    # Warm-up
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    _ = torch.matmul(a, b)
    torch.cuda.synchronize()
    
    # Benchmark
    import time
    start = time.time()
    for _ in range(10):
        c = torch.matmul(a, b)
        torch.cuda.synchronize()
    elapsed = time.time() - start
    
    tflops = (2 * size**3 * 10) / elapsed / 1e12
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã (–º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ {size}x{size}):")
    print(f"  –í—Ä–µ–º—è: {elapsed:.3f} —Å–µ–∫—É–Ω–¥")
    print(f"  –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {tflops:.2f} TFLOPS")
    
    # Memory bandwidth test
    size_mb = 1024  # 1GB
    data = torch.randn(size_mb * 1024 * 256, device=device)
    torch.cuda.synchronize()
    
    start = time.time()
    for _ in range(10):
        _ = data * 2
        torch.cuda.synchronize()
    elapsed = time.time() - start
    
    bandwidth = (size_mb * 10) / elapsed
    print(f"\n–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏:")
    print(f"  {bandwidth:.2f} GB/s")


def recommend_settings():
    """Recommend optimal settings based on GPU."""
    print("\n" + "=" * 60)
    print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("\n–†–µ–∂–∏–º CPU:")
        print("  - precision: fp32")
        print("  - use_flash_attention: false")
        print("  - batch_size: 1")
        print("  - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –º–æ–¥–µ–ª—å: qwen_vl_2b")
        return
    
    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
    gpu_name = torch.cuda.get_device_name(0)
    
    print(f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ GPU: {gpu_name}")
    print(f"–î–æ—Å—Ç—É–ø–Ω–æ VRAM: {vram:.2f} GB")
    print("\n–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ config.yaml:\n")
    
    if vram < 6:
        print("‚ö†Ô∏è –ú–∞–ª–æ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏ (<6GB)")
        print("models:")
        print("  qwen_vl_2b:")
        print("    precision: int4")
        print("    use_flash_attention: false")
        print("    device_map: auto")
        print("\n–û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ VRAM: ~1.2GB")
    elif vram < 8:
        print("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è 6-8GB VRAM:")
        print("models:")
        print("  qwen_vl_2b:")
        print("    precision: int8")
        print("    use_flash_attention: true")
        print("    device_map: auto")
        print("\n–û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ VRAM: ~2.4GB")
    elif vram < 12:
        print("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è 8-12GB VRAM:")
        print("models:")
        print("  qwen_vl_2b:")
        print("    precision: fp16")
        print("    use_flash_attention: true")
        print("  qwen3_vl_2b:")
        print("    precision: int8")
        print("    use_flash_attention: true")
        print("\n–û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ VRAM: ~7.1GB (–æ–±–µ –º–æ–¥–µ–ª–∏)")
    else:
        print("–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è 12GB+ VRAM:")
        print("models:")
        print("  qwen_vl_2b:")
        print("    precision: fp16")
        print("    use_flash_attention: true")
        print("  qwen3_vl_2b:")
        print("    precision: fp16")
        print("    use_flash_attention: true")
        print("\n–û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ VRAM: ~9.1GB (–æ–±–µ –º–æ–¥–µ–ª–∏)")


def main():
    """Main setup and verification routine."""
    print("\n" + "#" * 60)
    print("# ChatVLMLLM - GPU Setup & Verification")
    print("#" * 60 + "\n")
    
    # Check CUDA
    cuda_ok = check_cuda()
    
    if cuda_ok:
        # GPU info
        check_gpu_info()
        
        # Check optional components
        flash_ok = check_flash_attention()
        bnb_ok = check_bitsandbytes()
        
        # Benchmark
        try:
            benchmark_gpu()
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
    
    # Recommendations
    recommend_settings()
    
    # Summary
    print("\n" + "=" * 60)
    print("–ò—Ç–æ–≥–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å")
    print("=" * 60)
    
    status = []
    status.append(("CUDA", "‚úì" if cuda_ok else "‚ùå"))
    
    if cuda_ok:
        try:
            import flash_attn
            status.append(("Flash Attention", "‚úì"))
        except ImportError:
            status.append(("Flash Attention", "‚ö†Ô∏è"))
        
        try:
            import bitsandbytes
            status.append(("BitsAndBytes", "‚úì"))
        except ImportError:
            status.append(("BitsAndBytes", "‚ö†Ô∏è"))
    
    for name, state in status:
        print(f"{name:20s}: {state}")
    
    print("\n" + "#" * 60)
    if cuda_ok:
        print("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ —Å GPU! üöÄ")
    else:
        print("–°–∏—Å—Ç–µ–º–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU (–º–µ–¥–ª–µ–Ω–Ω–æ) ‚ö†Ô∏è")
    print("#" * 60 + "\n")


if __name__ == "__main__":
    main()
