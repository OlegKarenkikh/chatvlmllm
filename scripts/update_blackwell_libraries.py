#!/usr/bin/env python3
"""
–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï –ë–ò–ë–õ–ò–û–¢–ï–ö –î–õ–Ø RTX 5070 TI (BLACKWELL)

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
- PyTorch: https://pytorch.org/get-started/locally/
- Flash Attention: https://github.com/Dao-AILab/flash-attention
- Qwen: https://github.com/QwenLM/Qwen3-VL
"""

import subprocess
import sys
import os
import torch
from pathlib import Path

def run_command(command, description=""):
    """–í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–∞–Ω–¥—É —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    print(f"\nüîß {description}")
    print(f"–ö–æ–º–∞–Ω–¥–∞: {command}")
    
    try:
        result = subprocess.run(command, shell=True, check=True, 
                              capture_output=True, text=True)
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {description}")
        if result.stdout:
            print(f"–í—ã–≤–æ–¥: {result.stdout.strip()}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {description}")
        print(f"–ö–æ–¥ –æ—à–∏–±–∫–∏: {e.returncode}")
        if e.stderr:
            print(f"–û—à–∏–±–∫–∞: {e.stderr.strip()}")
        return False

def check_gpu_compatibility():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å GPU."""
    print("üîç –ü–†–û–í–ï–†–ö–ê –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò GPU")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    compute_capability = torch.cuda.get_device_capability(0)
    
    print(f"GPU: {gpu_name}")
    print(f"Compute Capability: {compute_capability}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º RTX 5070 Ti
    is_blackwell = "5070" in gpu_name or "5080" in gpu_name or "5090" in gpu_name
    is_sm120 = compute_capability >= (12, 0)
    
    if is_blackwell and is_sm120:
        print("‚úÖ RTX 5070 Ti (Blackwell) –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞")
        print("‚ö†Ô∏è Flash Attention 2 –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ Blackwell")
        print("‚úÖ –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å eager attention + bfloat16")
        return True
    elif compute_capability >= (8, 0):
        print("‚úÖ Ampere/Ada GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞")
        print("‚úÖ Flash Attention 2 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        return True
    else:
        print("‚ö†Ô∏è –°—Ç–∞—Ä–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GPU")
        print("‚ö†Ô∏è Flash Attention –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è")
        return True

def check_current_versions():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫."""
    print("\nüìã –¢–ï–ö–£–©–ò–ï –í–ï–†–°–ò–ò –ë–ò–ë–õ–ò–û–¢–ï–ö")
    print("=" * 50)
    
    try:
        import torch
        print(f"PyTorch: {torch.__version__}")
        print(f"CUDA: {torch.version.cuda}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É Blackwell
        arch_list = torch.cuda.get_arch_list()
        blackwell_support = 'sm_120' in arch_list
        print(f"Blackwell Support (sm_120): {'‚úÖ' if blackwell_support else '‚ùå'}")
        
        if torch.cuda.is_available():
            print(f"bfloat16 Support: {'‚úÖ' if torch.cuda.is_bf16_supported() else '‚ùå'}")
        
    except ImportError:
        print("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    try:
        import transformers
        print(f"Transformers: {transformers.__version__}")
    except ImportError:
        print("Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    try:
        import flash_attn
        print(f"Flash Attention: {flash_attn.__version__}")
        print("‚ö†Ô∏è Flash Attention —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ Blackwell")
    except ImportError:
        print("Flash Attention –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell)")

def install_pytorch_blackwell():
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Blackwell."""
    print("\nüöÄ –£–°–¢–ê–ù–û–í–ö–ê PYTORCH –° –ü–û–î–î–ï–†–ñ–ö–û–ô BLACKWELL")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –≤–µ—Ä—Å–∏—é
    try:
        import torch
        current_version = torch.__version__
        arch_list = torch.cuda.get_arch_list()
        
        if 'sm_120' in arch_list and current_version.startswith('2.7'):
            print(f"‚úÖ PyTorch {current_version} —É–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Blackwell")
            return True
    except ImportError:
        pass
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch 2.7.0 —Å CUDA 12.8
    commands = [
        "pip uninstall -y torch torchvision torchaudio",
        "pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128"
    ]
    
    for cmd in commands:
        if not run_command(cmd, f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {cmd}"):
            return False
    
    return True

def install_transformers_optimized():
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ transformers."""
    print("\nüìö –£–°–¢–ê–ù–û–í–ö–ê –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–• TRANSFORMERS")
    print("=" * 50)
    
    commands = [
        "pip install --upgrade transformers>=4.50.0",
        "pip install --upgrade accelerate>=1.2.0",
        "pip install --upgrade qwen-vl-utils",
        "pip install --upgrade optimum",
        "pip install --upgrade bitsandbytes"
    ]
    
    for cmd in commands:
        run_command(cmd, f"–£—Å—Ç–∞–Ω–æ–≤–∫–∞: {cmd.split()[-1]}")
    
    return True

def remove_flash_attention():
    """–£–¥–∞–ª—è–µ–º Flash Attention –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–Ω–µ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å Blackwell)."""
    print("\nüóëÔ∏è –£–î–ê–õ–ï–ù–ò–ï FLASH ATTENTION (–ù–ï –°–û–í–ú–ï–°–¢–ò–ú –° BLACKWELL)")
    print("=" * 50)
    
    try:
        import flash_attn
        print("‚ö†Ô∏è Flash Attention –æ–±–Ω–∞—Ä—É–∂–µ–Ω - —É–¥–∞–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Blackwell")
        run_command("pip uninstall -y flash-attn", "–£–¥–∞–ª–µ–Ω–∏–µ Flash Attention")
    except ImportError:
        print("‚úÖ Flash Attention –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (—Ö–æ—Ä–æ—à–æ –¥–ª—è Blackwell)")
    
    return True

def update_model_configs():
    """–û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è Blackwell."""
    print("\n‚öôÔ∏è –û–ë–ù–û–í–õ–ï–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)
    
    config_updates = {
        "precision": "bf16",  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell Tensor Cores
        "attn_implementation": "eager",  # –°—Ç–∞–±–∏–ª—å–Ω–æ –Ω–∞ sm_120
        "use_flash_attention": False,  # –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ Blackwell
        "enable_blackwell_optimizations": True
    }
    
    print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
    for key, value in config_updates.items():
        print(f"  {key}: {value}")
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config_content = """# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø RTX 5070 TI (BLACKWELL)

models:
  qwen_vl_2b:
    name: "Qwen2-VL 2B (Blackwell Optimized)"
    model_path: "Qwen/Qwen2-VL-2B-Instruct"
    precision: "bf16"  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Blackwell Tensor Cores
    attn_implementation: "eager"  # –°—Ç–∞–±–∏–ª—å–Ω–æ –Ω–∞ sm_120
    use_flash_attention: false  # –ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–∞ Blackwell
    device_map: "auto"
    trust_remote_code: true
    
  qwen3_vl_2b:
    name: "Qwen3-VL 2B (Blackwell Optimized)"
    model_path: "Qwen/Qwen3-VL-2B-Instruct"
    precision: "bf16"
    attn_implementation: "eager"
    use_flash_attention: false
    device_map: "auto"
    trust_remote_code: true
    
  dots_ocr:
    name: "dots.ocr (Blackwell Compatible)"
    model_path: "rednote-hilab/dots.ocr"
    precision: "bf16"
    attn_implementation: "eager"
    use_flash_attention: false
    device_map: "auto"
    trust_remote_code: true

performance:
  blackwell_optimizations:
    enable_tf32: true
    enable_cudnn_benchmark: true
    use_bfloat16: true
    enable_sdpa: true
    
gpu_requirements:
  rtx_5070_ti:
    compute_capability: "sm_120"
    cuda_version: "12.8+"
    pytorch_version: "2.7.0+"
    flash_attention_support: false
    recommended_precision: "bf16"
    tensor_cores: "5th_gen"
"""
    
    try:
        with open("config_blackwell_optimized.yaml", "w", encoding="utf-8") as f:
            f.write(config_content)
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ config_blackwell_optimized.yaml")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    
    return True

def verify_installation():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫–∏."""
    print("\n‚úÖ –ü–†–û–í–ï–†–ö–ê –£–°–¢–ê–ù–û–í–ö–ò")
    print("=" * 50)
    
    try:
        import torch
        print(f"PyTorch: {torch.__version__}")
        print(f"CUDA: {torch.version.cuda}")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            compute_cap = torch.cuda.get_device_capability(0)
            arch_list = torch.cuda.get_arch_list()
            
            print(f"GPU: {gpu_name}")
            print(f"Compute Capability: {compute_cap}")
            print(f"Blackwell Support: {'‚úÖ' if 'sm_120' in arch_list else '‚ùå'}")
            print(f"bfloat16 Support: {'‚úÖ' if torch.cuda.is_bf16_supported() else '‚ùå'}")
            
            # –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏
            try:
                x = torch.randn(10, 10, device='cuda', dtype=torch.bfloat16)
                y = torch.matmul(x, x.T)
                print("‚úÖ –¢–µ—Å—Ç CUDA + bfloat16: –£—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                print(f"‚ùå –¢–µ—Å—Ç CUDA + bfloat16: {e}")
        
        import transformers
        print(f"Transformers: {transformers.__version__}")
        
        try:
            import flash_attn
            print(f"‚ö†Ô∏è Flash Attention: {flash_attn.__version__} (–º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ Blackwell)")
        except ImportError:
            print("‚úÖ Flash Attention –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Blackwell)")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
        return False

def create_test_script():
    """–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π."""
    test_script = '''#!/usr/bin/env python3
"""
–¢–ï–°–¢ BLACKWELL –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ô
"""

import torch
import time
from transformers import AutoModelForImageTextToText, AutoProcessor

def test_blackwell_optimizations():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è Blackwell."""
    print("üß™ –¢–ï–°–¢ BLACKWELL –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ô")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    compute_cap = torch.cuda.get_device_capability(0)
    print(f"GPU: {gpu_name}")
    print(f"Compute Capability: {compute_cap}")
    
    # –í–∫–ª—é—á–∞–µ–º Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.enable_flash_sdp(True)
    
    print("‚úÖ Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
    
    # –¢–µ—Å—Ç bfloat16
    try:
        print("\\nüîç –¢–µ—Å—Ç bfloat16 –æ–ø–µ—Ä–∞—Ü–∏–π...")
        start = time.time()
        
        x = torch.randn(1024, 1024, device='cuda', dtype=torch.bfloat16)
        y = torch.randn(1024, 1024, device='cuda', dtype=torch.bfloat16)
        
        for _ in range(100):
            z = torch.matmul(x, y)
        
        torch.cuda.synchronize()
        elapsed = time.time() - start
        
        print(f"‚úÖ bfloat16 –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: {elapsed:.3f}s")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ bfloat16: {e}")
        return False
    
    # –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    try:
        print("\\nüîç –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Å Blackwell –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏...")
        start = time.time()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º eager attention (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å Blackwell)
        model = AutoModelForImageTextToText.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct",
            torch_dtype=torch.bfloat16,
            attn_implementation="eager",  # –ù–ï flash_attention_2
            device_map="auto",
            trust_remote_code=True
        )
        
        load_time = time.time() - start
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f}s")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º dtype –º–æ–¥–µ–ª–∏
        first_param = next(model.parameters())
        print(f"‚úÖ Dtype –º–æ–¥–µ–ª–∏: {first_param.dtype}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

if __name__ == "__main__":
    success = test_blackwell_optimizations()
    print(f"\\n{'‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´' if success else '‚ùå –ï–°–¢–¨ –ü–†–û–ë–õ–ï–ú–´'}")
'''
    
    try:
        with open("test_blackwell_optimizations.py", "w", encoding="utf-8") as f:
            f.write(test_script)
        print("‚úÖ –°–æ–∑–¥–∞–Ω test_blackwell_optimizations.py")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞: {e}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    print("üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï –î–õ–Ø RTX 5070 TI (BLACKWELL)")
    print("=" * 80)
    print("–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ PyTorch, Transformers, Flash Attention")
    print("=" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    if not check_gpu_compatibility():
        print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è...")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Ä—Å–∏–∏
    check_current_versions()
    
    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    response = input("\n‚ùì –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫? (y/N): ")
    if response.lower() not in ['y', 'yes', '–¥–∞']:
        print("‚ùå –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–º–µ–Ω–µ–Ω–æ")
        return False
    
    success = True
    
    # –£–¥–∞–ª—è–µ–º Flash Attention (–Ω–µ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å Blackwell)
    success &= remove_flash_attention()
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Blackwell
    success &= install_pytorch_blackwell()
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
    success &= install_transformers_optimized()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    success &= update_model_configs()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç
    create_test_script()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É
    if success:
        success &= verify_installation()
    
    print("\n" + "=" * 80)
    if success:
        print("üéâ –û–ë–ù–û–í–õ–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("‚úÖ PyTorch 2.7.0 —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Blackwell —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("‚úÖ Flash Attention —É–¥–∞–ª–µ–Ω (–Ω–µ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å RTX 5070 Ti)")
        print("‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –¥–ª—è Blackwell")
        print("\nüìã –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print("1. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Python/IDE")
        print("2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python test_blackwell_optimizations.py")
        print("3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ config_blackwell_optimized.yaml")
        print("4. –í –∫–æ–¥–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ attn_implementation='eager' –∏ torch.bfloat16")
    else:
        print("‚ùå –û–ë–ù–û–í–õ–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –° –û–®–ò–ë–ö–ê–ú–ò")
        print("‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É")
    
    print("=" * 80)
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)