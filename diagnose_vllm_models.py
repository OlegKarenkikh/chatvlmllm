#!/usr/bin/env python3
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å –º–æ–¥–µ–ª—è–º–∏ vLLM
"""

import json
import subprocess
import requests
import time
import os
from pathlib import Path

class VLLMDiagnostics:
    def __init__(self):
        self.cache_path = str(os.path.expanduser("~/.cache/huggingface/hub")).replace('\\', '/')
        
    def run_command(self, command):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã"""
        try:
            result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
            return True, result.stdout.strip()
        except subprocess.CalledProcessError as e:
            return False, e.stderr.strip() if e.stderr else str(e)
    
    def check_gpu_memory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU"""
        print("üéÆ –ü–†–û–í–ï–†–ö–ê GPU –ü–ê–ú–Ø–¢–ò")
        print("=" * 25)
        
        success, output = self.run_command("nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv,noheader,nounits")
        
        if success:
            lines = output.strip().split('\n')
            for i, line in enumerate(lines):
                parts = line.split(', ')
                if len(parts) == 3:
                    total, used, free = map(int, parts)
                    usage_percent = round((used / total) * 100, 1)
                    print(f"GPU {i}: {used}/{total} –ú–ë ({usage_percent}%)")
                    print(f"   –°–≤–æ–±–æ–¥–Ω–æ: {free} –ú–ë")
                    
                    if free < 2000:
                        print(f"   ‚ö†Ô∏è –ú–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏!")
                    elif free < 4000:
                        print(f"   üí° –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ª–µ–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π")
                    else:
                        print(f"   ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU: {output}")
    
    def check_model_requirements(self, model_name):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–∏"""
        model_path = Path(self.cache_path) / f"models--{model_name.replace('/', '--')}"
        
        if not model_path.exists():
            return {"status": "not_cached", "error": "–ú–æ–¥–µ–ª—å –Ω–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∞"}
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        snapshots_dir = model_path / "snapshots"
        if not snapshots_dir.exists():
            return {"status": "invalid_cache", "error": "–ù–µ—Ç –ø–∞–ø–∫–∏ snapshots"}
        
        snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
        if not snapshot_dirs:
            return {"status": "no_snapshots", "error": "–ù–µ—Ç —Å–Ω–∞–ø—à–æ—Ç–æ–≤"}
        
        latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
        config_path = latest_snapshot / "config.json"
        if not config_path.exists():
            return {"status": "no_config", "error": "–ù–µ—Ç config.json"}
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
            issues = []
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
            architectures = config.get('architectures', [])
            if any('got' in arch.lower() for arch in architectures):
                # GOT –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞–∫–µ—Ç—ã
                issues.append("GOT –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞–∫–µ—Ç—ã (verovio)")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è
            vocab_size = config.get('vocab_size', 0)
            if vocab_size > 100000:
                issues.append(f"–ë–æ–ª—å—à–æ–π —Å–ª–æ–≤–∞—Ä—å ({vocab_size} —Ç–æ–∫–µ–Ω–æ–≤)")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
            hidden_size = config.get('hidden_size', 0)
            if hidden_size > 4096:
                issues.append(f"–ë–æ–ª—å—à–æ–π hidden_size ({hidden_size})")
            
            return {
                "status": "ok",
                "config": config,
                "issues": issues,
                "architectures": architectures,
                "vocab_size": vocab_size,
                "hidden_size": hidden_size
            }
            
        except Exception as e:
            return {"status": "config_error", "error": str(e)}
    
    def test_model_launch(self, model_name, port, timeout=120):
        """–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏"""
        print(f"\nüß™ –¢–ï–°–¢ –ó–ê–ü–£–°–ö–ê: {model_name}")
        print("-" * 40)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        requirements = self.check_model_requirements(model_name)
        print(f"üìã –°—Ç–∞—Ç—É—Å –∫–µ—à–∞: {requirements['status']}")
        
        if requirements['status'] != 'ok':
            print(f"‚ùå {requirements.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            return False
        
        if requirements['issues']:
            print(f"‚ö†Ô∏è –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
            for issue in requirements['issues']:
                print(f"   ‚Ä¢ {issue}")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
        container_name = f"test-{model_name.replace('/', '-').replace('.', '-').lower()}"
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        self.run_command(f"docker stop {container_name}")
        self.run_command(f"docker rm {container_name}")
        
        docker_command = f"""
        docker run -d \
            --gpus all \
            --name {container_name} \
            -p {port}:{port} \
            -v {self.cache_path}:/root/.cache/huggingface/hub:ro \
            --shm-size=4g \
            vllm/vllm-openai:latest \
            --model {model_name} \
            --trust-remote-code \
            --max-model-len 1024 \
            --gpu-memory-utilization 0.6 \
            --host 0.0.0.0 \
            --port {port} \
            --disable-log-requests \
            --enforce-eager
        """.strip().replace('\n', ' ').replace('\\', '')
        
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞...")
        success, output = self.run_command(docker_command)
        
        if not success:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞: {output}")
            return False
        
        print(f"‚úÖ –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω, –æ–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏...")
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=5)
                if response.status_code == 200:
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∑–∞ {int(time.time() - start_time)} —Å–µ–∫—É–Ω–¥!")
                    
                    # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                    self.run_command(f"docker stop {container_name}")
                    self.run_command(f"docker rm {container_name}")
                    return True
                    
            except requests.exceptions.ConnectionError:
                pass
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            
            time.sleep(5)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤ –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –≥–æ—Ç–æ–≤–∞ –∑–∞ {timeout} —Å–µ–∫—É–Ω–¥")
        print(f"üìã –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏:")
        
        success, logs = self.run_command(f"docker logs {container_name} --tail 10")
        if success:
            print(logs)
        
        # –û—á–∏—Å—Ç–∫–∞
        self.run_command(f"docker stop {container_name}")
        self.run_command(f"docker rm {container_name}")
        return False
    
    def find_compatible_models(self):
        """–ü–æ–∏—Å–∫ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        print("üîç –ü–û–ò–°–ö –°–û–í–ú–ï–°–¢–ò–ú–´–• –ú–û–î–ï–õ–ï–ô")
        print("=" * 35)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
        try:
            with open('vllm_models_config.json', 'r', encoding='utf-8') as f:
                configs = json.load(f)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {e}")
            return []
        
        compatible_models = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∫ —Å–ª–æ–∂–Ω—ã–º)
        test_order = [
            "rednote-hilab/dots.ocr",  # –£–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            "Qwen/Qwen3-VL-2B-Instruct",  # –õ–µ–≥–∫–∞—è VLM
            "Qwen/Qwen2-VL-2B-Instruct",  # –õ–µ–≥–∫–∞—è VLM
            "microsoft/Phi-3.5-vision-instruct",  # Microsoft –º–æ–¥–µ–ª—å
            "Qwen/Qwen2.5-VL-7B-Instruct",  # –°—Ä–µ–¥–Ω—è—è VLM
            "Qwen/Qwen2-VL-7B-Instruct"  # –¢—è–∂–µ–ª–∞—è VLM
        ]
        
        port = 9000  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ—Ä—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤
        
        for model_name in test_order:
            if model_name in configs:
                print(f"\n{'='*50}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–æ–º
                self.check_gpu_memory()
                
                if self.test_model_launch(model_name, port):
                    compatible_models.append(model_name)
                    print(f"‚úÖ {model_name} - –°–û–í–ú–ï–°–¢–ò–ú–ê")
                else:
                    print(f"‚ùå {model_name} - –ù–ï –°–û–í–ú–ï–°–¢–ò–ú–ê")
                
                port += 1
                time.sleep(5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
        
        return compatible_models
    
    def create_working_config(self, compatible_models):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        if not compatible_models:
            print("‚ùå –ù–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π")
            return
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        try:
            with open('vllm_models_config.json', 'r', encoding='utf-8') as f:
                full_config = json.load(f)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π: {e}")
            return
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
        working_config = {}
        port = 8000
        
        for model_name in compatible_models:
            if model_name in full_config:
                config = full_config[model_name].copy()
                config['port'] = port
                config['container_name'] = f"{model_name.replace('/', '-').replace('.', '-').lower()}-vllm"
                working_config[model_name] = config
                port += 1
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        with open('vllm_working_models.json', 'w', encoding='utf-8') as f:
            json.dump(working_config, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ vllm_working_models.json")
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(compatible_models)} —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π:")
        
        for model_name in compatible_models:
            config = working_config[model_name]
            print(f"   ‚Ä¢ {model_name} (–ø–æ—Ä—Ç {config['port']})")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üîß –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ú–û–î–ï–õ–ï–ô VLLM")
    print("=" * 30)
    
    diagnostics = VLLMDiagnostics()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    diagnostics.check_gpu_memory()
    
    # –ü–æ–∏—Å–∫ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π
    compatible = diagnostics.find_compatible_models()
    
    if compatible:
        diagnostics.create_working_config(compatible)
        
        print(f"\nüéâ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π: {len(compatible)}")
        print(f"üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ vllm_working_models.json –¥–ª—è –∑–∞–ø—É—Å–∫–∞")
    else:
        print(f"\n‚ùå –°–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        print(f"üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π")

if __name__ == "__main__":
    main()